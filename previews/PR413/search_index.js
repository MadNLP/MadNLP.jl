var documenterSearchIndex = {"docs":
[{"location":"man/solver/","page":"IPM solver","title":"IPM solver","text":"MadNLP is an interior-point solver based on a filter line-search. We detail here the inner machinery happening at each MadNLP's iteration.","category":"page"},{"location":"man/solver/","page":"IPM solver","title":"IPM solver","text":"We recall that MadNLP is a primal-dual interior-point method and starts from an initial primal-dual variables (x_0 s_0 y_0).","category":"page"},{"location":"man/solver/#What-happens-at-iteration-k?","page":"IPM solver","title":"What happens at iteration k?","text":"","category":"section"},{"location":"man/solver/","page":"IPM solver","title":"IPM solver","text":"At iteration k, MadNLP aims at finding a new iterate (x_k+1 s_k+1 y_k+1) improving the current iterate (x_k s_k y_k), in the sense that the new iterate (i) improves the objective or (ii) decreases the infeasibility. The exact trade-off between (i) and (ii) is handled by the filter line-search.","category":"page"},{"location":"man/solver/","page":"IPM solver","title":"IPM solver","text":"The algorithm follows the steps:","category":"page"},{"location":"man/solver/","page":"IPM solver","title":"IPM solver","text":"Check problem convergence E_0(x_k s_k y_k)  varepsilon_tol\nIf necessary, update the barrier parameter mu_k\nEvaluate the Hessian of the Lagrangian W_k and the Jacobian A_k with the callbacks\nAssemble the KKT system and compute the search direction d_k by solving the resulting linear system.\nIf necessary, regularize the KKT system to guarantee that d_k is a descent direction\nRun the backtracking line-search and find a step alpha_k\nDefine the next iterate as x_k+1 = x_k + alpha_k d_k^x, s_k+1 = s_k + alpha_k d_k^s, y_k+1 = y_k + alpha_k d_k^y.","category":"page"},{"location":"man/solver/","page":"IPM solver","title":"IPM solver","text":"We detail each step in the following paragraphs.","category":"page"},{"location":"man/solver/","page":"IPM solver","title":"IPM solver","text":"note: Note\nIn general, Step 3 (Hessian and Jacobian evaluations) and Step 4 (solving the KKT system) are the two most numerically demanding steps.","category":"page"},{"location":"man/solver/#Step-1:-When-the-algorithm-stops?","page":"IPM solver","title":"Step 1: When the algorithm stops?","text":"","category":"section"},{"location":"man/solver/","page":"IPM solver","title":"IPM solver","text":"MadNLP stops once the solution satisfies a specified accuracy varepsilon_tol (by default 10^-8). MadNLP uses the same stopping criterion as Ipopt by defining","category":"page"},{"location":"man/solver/","page":"IPM solver","title":"IPM solver","text":"E_mu(x_k s_k y_k nu_k w_k) =\nmax left\nbeginaligned\n nabla f(x_k) + A_k^top y_k + nu_k + w_k _infty \n g(x_k) - s_k _infty \n X_knu_k - mu e _infty \n S_k w_k - mu e _infty\nendaligned\nright\n","category":"page"},{"location":"man/solver/","page":"IPM solver","title":"IPM solver","text":"and stopping the algorithm whenever","category":"page"},{"location":"man/solver/","page":"IPM solver","title":"IPM solver","text":"E_0(x_k s_k y_k nu_k w_k)  varepsilon_tol\n","category":"page"},{"location":"man/solver/#Step-2:-How-to-update-the-barrier-parameter-\\mu?","page":"IPM solver","title":"Step 2: How to update the barrier parameter mu?","text":"","category":"section"},{"location":"man/solver/","page":"IPM solver","title":"IPM solver","text":"TODO","category":"page"},{"location":"man/solver/#Step-4:-How-do-we-solve-the-KKT-system?","page":"IPM solver","title":"Step 4: How do we solve the KKT system?","text":"","category":"section"},{"location":"man/solver/","page":"IPM solver","title":"IPM solver","text":"Solving the KKT system happens in two substeps:","category":"page"},{"location":"man/solver/","page":"IPM solver","title":"IPM solver","text":"Assembling the KKT matrix K.\nSolve the system Kx = b with a linear solver.","category":"page"},{"location":"man/solver/","page":"IPM solver","title":"IPM solver","text":"In substep 1, MadNLP reads the Hessian and the Jacobian computed at Step 3 and build the associated KKT system in an AbstractKKTSystem. As a result, we get a matrix K_k encoding the KKT system at iteration k.","category":"page"},{"location":"man/solver/","page":"IPM solver","title":"IPM solver","text":"In substep 2, the matrix K_k is factorized by a compatible linear solver. Then a solution x is returned by applying a backsolve.","category":"page"},{"location":"man/solver/#Step-5:-How-to-regularize-the-KKT-system?","page":"IPM solver","title":"Step 5: How to regularize the KKT system?","text":"","category":"section"},{"location":"man/solver/","page":"IPM solver","title":"IPM solver","text":"TODO","category":"page"},{"location":"man/solver/#Step-6:-What-is-a-filter-line-search?","page":"IPM solver","title":"Step 6: What is a filter line-search?","text":"","category":"section"},{"location":"man/solver/","page":"IPM solver","title":"IPM solver","text":"TODO","category":"page"},{"location":"man/solver/#What-happens-if-the-line-search-failed?","page":"IPM solver","title":"What happens if the line-search failed?","text":"","category":"section"},{"location":"man/solver/","page":"IPM solver","title":"IPM solver","text":"If inside the line-search algorithm the step alpha_k becomes negligible (10^-8) then we consider the line-search has failed to find a next iterate along the current direction d_k. If that happens during several consecutive iterations, MadNLP enters into a feasible restoration phase. The goal of feasible restoration is to decrease the primal infeasibility, to move the current iterate closer to the feasible set.","category":"page"},{"location":"installation/#Installation","page":"Installation","title":"Installation","text":"","category":"section"},{"location":"installation/","page":"Installation","title":"Installation","text":"To install MadNLP, simply proceed to","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"pkg> add MadNLP\n","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"note: Note\nThe default installation comes is shipped only with two linear solvers (Umfpack and Lapack), which are not adapted to solve the KKT systems arising in large-scale nonlinear problems. We recommend using a specialized linear solver to speed-up the solution of the KKT systems.","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"In addition to Lapack and Umfpack, the user can install the following extensions to use a specialized linear solver.","category":"page"},{"location":"installation/#HSL-linear-solver","page":"Installation","title":"HSL linear solver","text":"","category":"section"},{"location":"installation/","page":"Installation","title":"Installation","text":"Obtain a license and download HSL_jll.jl from https://licences.stfc.ac.uk/products/Software/HSL/LibHSL. Install this download into your current environment using:","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"import Pkg\nPkg.develop(path = \"/full/path/to/HSL_jll.jl\")","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"If the user has already compiled the HSL solver library, one can simply override the path to the artifact by editing ~/.julia/artifacts/Overrides.toml","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"# replace HSL_jll artifact /usr/local/lib/libhsl.so\necece3e2c69a413a0e935cf52e03a3ad5492e137 = \"/usr/local\"","category":"page"},{"location":"installation/#Mumps-linear-solver","page":"Installation","title":"Mumps linear solver","text":"","category":"section"},{"location":"installation/","page":"Installation","title":"Installation","text":"Mumps is an open-source sparse linear solver, whose binaries are kindly provided as a Julia artifact. Installing Mumps simply amounts to","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"pkg> add MadNLPMumps","category":"page"},{"location":"installation/#Pardiso-linear-solver","page":"Installation","title":"Pardiso linear solver","text":"","category":"section"},{"location":"installation/","page":"Installation","title":"Installation","text":"To use Pardiso, the user needs to obtain the Pardiso shared libraries from https://panua.ch/, provide the absolute path to the shared library:","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"julia> ENV[\"MADNLP_PARDISO_LIBRARY_PATH\"] = \"/usr/lib/libpardiso600-GNU800-X86-64.so\"","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"and place the license file in the home directory. After obtaining the library and the license file, run","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"pkg> build MadNLPPardiso","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"The build process requires a C compiler.","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"CurrentModule = MadNLP","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"using LinearAlgebra\nusing SparseArrays\nusing NLPModels\nusing MadNLP\nusing MadNLPTests\nnlp = MadNLPTests.HS15Model()\n","category":"page"},{"location":"man/kkt/#KKT-systems","page":"KKT systems","title":"KKT systems","text":"","category":"section"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"KKT systems are linear system with a special KKT structure. MadNLP uses a special structure AbstractKKTSystem to represent internally the KKT system. The AbstractKKTSystem fulfills two goals:","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"Store the values of the Hessian of the Lagrangian and of the Jacobian.\nAssemble the corresponding KKT matrix K.","category":"page"},{"location":"man/kkt/#A-brief-look-at-the-math","page":"KKT systems","title":"A brief look at the math","text":"","category":"section"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"We recall that at each iteration the interior-point algorithm aims at solving the following relaxed KKT equations (mu playing the role of a homotopy parameter) with a Newton method:","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"F_mu(x s y v w) = 0","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"with","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"F_mu(x s y v w) =\nleft\nbeginaligned\n     nabla f(x) + A^top y + nu + w  (F_1) \n     - y - w   (F_2) \n     g(x) - s   (F_3) \n     X v - mu e_n  (F_4) \n     S w - mu e_m  (F_5)\nendaligned\nright","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"The Newton step associated to the KKT equations writes","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"overline\nbeginpmatrix\n W  0  A^top  - I  0 \n 0  0  -I  0  -I \n A  -I  0 0  0 \n V  0  0  X  0 \n 0  W  0  0  S\nendpmatrix^K_3\nbeginpmatrix\n    Delta x \n    Delta s \n    Delta y \n    Delta v \n    Delta w\nendpmatrix\n= -\nbeginpmatrix\n    F_1  F_2  F_3  F_4  F_5\nendpmatrix","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"The matrix K_3 is unsymmetric, but we can obtain an equivalent symmetric system by eliminating the two last rows:","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"overline\nbeginpmatrix\n W + Sigma_x  0  A^top \n 0  Sigma_s  -I \n A  -I  0\nendpmatrix^K_2\nbeginpmatrix\n    Delta x \n    Delta s \n    Delta y\nendpmatrix\n= -\nbeginpmatrix\n    F_1 + X^-1 F_4  F_2 + S^-1 F_5  F_3\nendpmatrix","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"with Sigma_x = X^-1 v and Sigma_s = S^-1 w. The matrix K_2, symmetric, has a structure more favorable for a direct linear solver.","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"In MadNLP, the matrix K_3 is encoded as an AbstractUnreducedKKTSystem and the matrix K_2 is encoded as an AbstractReducedKKTSystem.","category":"page"},{"location":"man/kkt/#Assembling-a-KKT-system,-step-by-step","page":"KKT systems","title":"Assembling a KKT system, step by step","text":"","category":"section"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"We note that both K_3 and K_2 depend on the Hessian of the Lagrangian W, the Jacobian A and the diagonal matrices Sigma_x = X^1V and Sigma_s = S^-1W. Hence, we have to update the KKT system at each iteration of the interior-point algorithm.","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"By default, MadNLP stores the KKT system as a SparseKKTSystem. The KKT system takes as input a SparseCallback wrapping a given NLPModel nlp. We instantiate the callback cb with the function create_callback:","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"cb = MadNLP.create_callback(\n    MadNLP.SparseCallback,\n    nlp,\n)\nind_cons = MadNLP.get_index_constraints(nlp)\n","category":"page"},{"location":"man/kkt/#Initializing-a-KKT-system","page":"KKT systems","title":"Initializing a KKT system","text":"","category":"section"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"The size of the KKT system depends directly on the problem's characteristics (number of variables, number of of equality and inequality constraints). A SparseKKTSystem stores the Hessian and the Jacobian in sparse (COO) format. The KKT matrix can be factorized using either a dense or a sparse linear solvers. Here we use the solver provided in Lapack:","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"linear_solver = LapackCPUSolver","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"We can instantiate a SparseKKTSystem using the function create_kkt_system:","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"kkt = MadNLP.create_kkt_system(\n    MadNLP.SparseKKTSystem,\n    cb,\n    ind_cons,\n    linear_solver,\n)\n","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"Once the KKT system built, one has to initialize it to use it inside the interior-point algorithm:","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"MadNLP.initialize!(kkt);\n","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"The user can query the KKT matrix inside kkt, simply as","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"kkt_matrix = MadNLP.get_kkt(kkt)","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"This returns a reference to the KKT matrix stores internally inside kkt. Each time the matrix is assembled inside kkt, kkt_matrix is updated automatically.","category":"page"},{"location":"man/kkt/#Updating-a-KKT-system","page":"KKT systems","title":"Updating a KKT system","text":"","category":"section"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"We suppose now we want to refresh the values stored in the KKT system.","category":"page"},{"location":"man/kkt/#Updating-the-values-of-the-Hessian","page":"KKT systems","title":"Updating the values of the Hessian","text":"","category":"section"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"The Hessian part of the KKT system can be queried as","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"hess_values = MadNLP.get_hessian(kkt)\n","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"For a SparseKKTSystem, hess_values is a Vector{Float64} storing the nonzero values of the Hessian. Then, one can update the vector hess_values by using NLPModels.jl:","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"n = NLPModels.get_nvar(nlp)\nm = NLPModels.get_ncon(nlp)\nx = NLPModels.get_x0(nlp) # primal variables\nl = zeros(m) # dual variables\n\nNLPModels.hess_coord!(nlp, x, l, hess_values)\n","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"Eventually, a post-processing step is applied to refresh all the values internally:","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"MadNLP.compress_hessian!(kkt)\n","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"note: Note\nBy default, the function compress_hessian! does nothing. But it can be required for very specific use-case, for instance building internally a Schur complement matrix.","category":"page"},{"location":"man/kkt/#Updating-the-values-of-the-Jacobian","page":"KKT systems","title":"Updating the values of the Jacobian","text":"","category":"section"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"We proceed exaclty the same way to update the values in the Jacobian. One queries the Jacobian values in the KKT system as","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"jac_values = MadNLP.get_jacobian(kkt)\n","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"We can refresh the values with NLPModels as","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"NLPModels.jac_coord!(nlp, x, jac_values)\n","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"And then applies a post-processing step as","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"MadNLP.compress_jacobian!(kkt)\n","category":"page"},{"location":"man/kkt/#Updating-the-values-of-the-diagonal-matrices","page":"KKT systems","title":"Updating the values of the diagonal matrices","text":"","category":"section"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"Once the Hessian and the Jacobian updated, the algorithm can apply primal and dual regularization terms on the diagonal of the KKT system, to improve the numerical behavior in the linear solver. This operation is implemented inside the regularize_diagonal! function:","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"pr_value = 1.0\ndu_value = 0.0\n\nMadNLP.regularize_diagonal!(kkt, pr_value, du_value)\n","category":"page"},{"location":"man/kkt/#Assembling-the-KKT-matrix","page":"KKT systems","title":"Assembling the KKT matrix","text":"","category":"section"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"Once the values updated, one can assemble the resulting KKT matrix. This translates to","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"MadNLP.build_kkt!(kkt)","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"By doing so, the values stored inside kkt will be transferred to the KKT matrix kkt_matrix (as returned by the function get_kkt):","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"kkt_matrix","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"Internally, a SparseKKTSystem stores the KKT system in a sparse COO format. When build_kkt! is called, the sparse COO matrix is transferred to SparseMatrixCSC if the linear solver is sparse, or alternatively to a Matrix if the linear solver is dense.","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"note: Note\nThe KKT system stores only the lower-triangular part of the KKT system, as it is symmetric.","category":"page"},{"location":"man/kkt/#Solution-of-the-KKT-system","page":"KKT systems","title":"Solution of the KKT system","text":"","category":"section"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"Now the KKT system is assembled in a matrix K (here stored in kkt_matrix), we want to solve a linear system K x = b, for instance to evaluate the next descent direction. To do so, we use the linear solver stored internally inside kkt (here an instance of LapackCPUSolver).","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"We start by factorizing the KKT matrix K:","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"MadNLP.factorize!(kkt.linear_solver)\n","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"By default, MadNLP uses a LBL factorization to decompose the symmetric indefinite KKT matrix.","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"Once the KKT matrix has been factorized, we can compute the solution of the linear system with a backsolve. The function takes as input a AbstractKKTVector, an object used to do algebraic manipulation with a AbstractKKTSystem. We start by instantiating two UnreducedKKTVector (encoding respectively the right-hand-side and the solution):","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"b = MadNLP.UnreducedKKTVector(kkt)\nfill!(MadNLP.full(b), 1.0)\nx = copy(b)\n","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"The right-hand-side encodes a vector of 1:","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"MadNLP.full(b)","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"We solve the system K x = b using the solve! function:","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"MadNLP.solve!(kkt, x)\nMadNLP.full(x)","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"We verify that the solution is correct by multiplying it on the left with the KKT system, using mul!:","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"mul!(b, kkt, x) # overwrite b!\nMadNLP.full(b)","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"We recover a vector filled with 1, which was the initial right-hand-side.","category":"page"},{"location":"lib/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"CurrentModule = MadNLP","category":"page"},{"location":"lib/linear_solvers/#Linear-solvers","page":"Linear Solvers","title":"Linear solvers","text":"","category":"section"},{"location":"lib/linear_solvers/#Direct-linear-solvers","page":"Linear Solvers","title":"Direct linear solvers","text":"","category":"section"},{"location":"lib/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"Each linear solver employed in MadNLP implements the following interface.","category":"page"},{"location":"lib/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"AbstractLinearSolver\nintroduce\nfactorize!\nsolve!\nis_inertia\ninertia","category":"page"},{"location":"lib/linear_solvers/#MadNLP.AbstractLinearSolver","page":"Linear Solvers","title":"MadNLP.AbstractLinearSolver","text":"AbstractLinearSolver\n\nAbstract type for linear solver targeting the resolution of the linear system Ax=b.\n\n\n\n\n\n","category":"type"},{"location":"lib/linear_solvers/#MadNLP.introduce","page":"Linear Solvers","title":"MadNLP.introduce","text":"introduce(::AbstractLinearSolver)\n\nPrint the name of the linear solver.\n\n\n\n\n\n","category":"function"},{"location":"lib/linear_solvers/#MadNLP.factorize!","page":"Linear Solvers","title":"MadNLP.factorize!","text":"factorize!(::AbstractLinearSolver)\n\nFactorize the matrix A and updates the factors inside the AbstractLinearSolver instance.\n\n\n\n\n\n","category":"function"},{"location":"lib/linear_solvers/#SolverCore.solve!","page":"Linear Solvers","title":"SolverCore.solve!","text":"solve!(::AbstractLinearSolver, x::AbstractVector)\n\nSolve the linear system Ax = b.\n\nThis function assumes the linear system has been factorized previously with factorize!.\n\n\n\n\n\n","category":"function"},{"location":"lib/linear_solvers/#MadNLP.is_inertia","page":"Linear Solvers","title":"MadNLP.is_inertia","text":"is_inertia(::AbstractLinearSolver)\n\nReturn true if the linear solver supports the computation of the inertia of the linear system.\n\n\n\n\n\n","category":"function"},{"location":"lib/linear_solvers/#MadNLP.inertia","page":"Linear Solvers","title":"MadNLP.inertia","text":"inertia(::AbstractLinearSolver)\n\nReturn the inertia (n, m, p) of the linear system as a tuple.\n\nNote\n\nThe inertia is defined as a tuple (n m p), with\n\nn: number of positive eigenvalues\nm: number of negative eigenvalues\np: number of zero eigenvalues\n\n\n\n\n\n","category":"function"},{"location":"lib/linear_solvers/#Iterative-refinement","page":"Linear Solvers","title":"Iterative refinement","text":"","category":"section"},{"location":"lib/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"MadNLP uses iterative refinement to improve the accuracy of the solution returned by the linear solver.","category":"page"},{"location":"lib/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"solve_refine!\n","category":"page"},{"location":"lib/linear_solvers/#MadNLP.solve_refine!","page":"Linear Solvers","title":"MadNLP.solve_refine!","text":"solve_refine!(x::VT, ::AbstractIterator, b::VT, w::VT) where {VT <: AbstractKKTVector}\n\nSolve the linear system Ax = b using iterative refinement. The object AbstractIterator stores an instance of a AbstractLinearSolver for the backsolve operations.\n\nNotes\n\nThis function assumes the matrix stored in the linear solver has been factorized previously.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/","page":"KKT systems","title":"KKT systems","text":"CurrentModule = MadNLP","category":"page"},{"location":"lib/kkt/#KKT-systems","page":"KKT systems","title":"KKT systems","text":"","category":"section"},{"location":"lib/kkt/","page":"KKT systems","title":"KKT systems","text":"MadNLP manipulates KKT systems using two abstractions: an AbstractKKTSystem storing the KKT system' matrix and an AbstractKKTVector storing the KKT system's right-hand-side.","category":"page"},{"location":"lib/kkt/#AbstractKKTSystem","page":"KKT systems","title":"AbstractKKTSystem","text":"","category":"section"},{"location":"lib/kkt/","page":"KKT systems","title":"KKT systems","text":"AbstractKKTSystem\n","category":"page"},{"location":"lib/kkt/#MadNLP.AbstractKKTSystem","page":"KKT systems","title":"MadNLP.AbstractKKTSystem","text":"AbstractKKTSystem{T, VT<:AbstractVector{T}, MT<:AbstractMatrix{T}, QN<:AbstractHessian{T}}\n\nAbstract type for KKT system.\n\n\n\n\n\n","category":"type"},{"location":"lib/kkt/","page":"KKT systems","title":"KKT systems","text":"MadNLP implements three different types of AbstractKKTSystem, depending how far we reduce the KKT system.","category":"page"},{"location":"lib/kkt/","page":"KKT systems","title":"KKT systems","text":"AbstractUnreducedKKTSystem\nAbstractReducedKKTSystem\nAbstractCondensedKKTSystem","category":"page"},{"location":"lib/kkt/#MadNLP.AbstractUnreducedKKTSystem","page":"KKT systems","title":"MadNLP.AbstractUnreducedKKTSystem","text":"AbstractUnreducedKKTSystem{T, VT, MT, QN} <: AbstractKKTSystem{T, VT, MT, QN}\n\nAugmented KKT system associated to the linearization of the KKT conditions at the current primal-dual iterate (x s y z ν w).\n\nThe associated matrix is\n\n[Wₓₓ  0   Aₑ'  Aᵢ' V½  0  ]  [Δx]\n[0    0   0   -I   0   W½ ]  [Δs]\n[Aₑ   0   0    0   0   0  ]  [Δy]\n[Aᵢ  -I   0    0   0   0  ]  [Δz]\n[V½   0   0    0  -X   0  ]  [Δτ]\n[0    W½  0    0   0  -S  ]  [Δρ]\n\nwith\n\nWₓₓ: Hessian of the Lagrangian.\nAₑ: Jacobian of the equality constraints\nAᵢ: Jacobian of the inequality constraints\nX = diag(x)\nS = diag(s)\nV = diag(ν)\nW = diag(w)\nΔτ = -W^-½Δν\nΔρ = -W^-½Δw\n\n\n\n\n\n","category":"type"},{"location":"lib/kkt/#MadNLP.AbstractReducedKKTSystem","page":"KKT systems","title":"MadNLP.AbstractReducedKKTSystem","text":"AbstractReducedKKTSystem{T, VT, MT, QN} <: AbstractKKTSystem{T, VT, MT, QN}\n\nThe reduced KKT system is a simplification of the original Augmented KKT system. Comparing to AbstractUnreducedKKTSystem), AbstractReducedKKTSystem removes the two last rows associated to the bounds' duals (ν w).\n\nAt a primal-dual iterate (x s y z), the matrix writes\n\n[Wₓₓ + Σₓ   0    Aₑ'   Aᵢ']  [Δx]\n[0          Σₛ    0    -I ]  [Δs]\n[Aₑ         0     0     0 ]  [Δy]\n[Aᵢ        -I     0     0 ]  [Δz]\n\nwith\n\nWₓₓ: Hessian of the Lagrangian.\nAₑ: Jacobian of the equality constraints\nAᵢ: Jacobian of the inequality constraints\nΣₓ = X¹ V\nΣₛ = S¹ W\n\n\n\n\n\n","category":"type"},{"location":"lib/kkt/#MadNLP.AbstractCondensedKKTSystem","page":"KKT systems","title":"MadNLP.AbstractCondensedKKTSystem","text":"AbstractCondensedKKTSystem{T, VT, MT, QN} <: AbstractKKTSystem{T, VT, MT, QN}\n\nThe condensed KKT system simplifies further the AbstractReducedKKTSystem by removing the rows associated to the slack variables s and the inequalities.\n\nAt the primal-dual iterate (x y), the matrix writes\n\n[Wₓₓ + Σₓ + Aᵢ' Σₛ Aᵢ    Aₑ']  [Δx]\n[         Aₑ              0 ]  [Δy]\n\nwith\n\nWₓₓ: Hessian of the Lagrangian.\nAₑ: Jacobian of the equality constraints\nAᵢ: Jacobian of the inequality constraints\nΣₓ = X¹ V\nΣₛ = S¹ W\n\n\n\n\n\n","category":"type"},{"location":"lib/kkt/","page":"KKT systems","title":"KKT systems","text":"Each AbstractKKTSystem follows the interface described below:","category":"page"},{"location":"lib/kkt/","page":"KKT systems","title":"KKT systems","text":"\ncreate_kkt_system\n\nnum_variables\nget_kkt\nget_jacobian\nget_hessian\n\ninitialize!\nbuild_kkt!\ncompress_hessian!\ncompress_jacobian!\njtprod!\nregularize_diagonal!\nis_inertia_correct\nnnz_jacobian","category":"page"},{"location":"lib/kkt/#MadNLP.create_kkt_system","page":"KKT systems","title":"MadNLP.create_kkt_system","text":"create_kkt_system(\n    ::Type{KKT},\n    cb::AbstractCallback,\n    ind_cons::NamedTuple,\n    linear_solver::Type{LinSol};\n    opt_linear_solver=default_options(linear_solver),\n    hessian_approximation=ExactHessian,\n) where {KKT<:AbstractKKTSystem, LinSol<:AbstractLinearSolver}\n\nInstantiate a new KKT system with type KKT, associated to the the nonlinear program encoded inside the callback cb. The NamedTuple ind_cons stores the indexes of all the variables and constraints in the callback cb. In addition, the user should pass the linear solver linear_solver that will be used to solve the KKT system after it has been assembled.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.num_variables","page":"KKT systems","title":"MadNLP.num_variables","text":"Number of primal variables (including slacks) associated to the KKT system.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.get_kkt","page":"KKT systems","title":"MadNLP.get_kkt","text":"get_kkt(kkt::AbstractKKTSystem)::AbstractMatrix\n\nReturn a pointer to the KKT matrix implemented in kkt. The pointer is passed afterward to a linear solver.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.get_jacobian","page":"KKT systems","title":"MadNLP.get_jacobian","text":"Get Jacobian matrix\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.get_hessian","page":"KKT systems","title":"MadNLP.get_hessian","text":"Get Hessian matrix\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.initialize!","page":"KKT systems","title":"MadNLP.initialize!","text":"initialize!(kkt::AbstractKKTSystem)\n\nInitialize KKT system with default values. Called when we initialize the MadNLPSolver storing the current KKT system kkt.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.build_kkt!","page":"KKT systems","title":"MadNLP.build_kkt!","text":"build_kkt!(kkt::AbstractKKTSystem)\n\nAssemble the KKT matrix before calling the factorization routine.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.compress_hessian!","page":"KKT systems","title":"MadNLP.compress_hessian!","text":"compress_hessian!(kkt::AbstractKKTSystem)\n\nCompress the Hessian inside kkt's internals. This function is called every time a new Hessian is evaluated.\n\nDefault implementation do nothing.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.compress_jacobian!","page":"KKT systems","title":"MadNLP.compress_jacobian!","text":"compress_jacobian!(kkt::AbstractKKTSystem)\n\nCompress the Jacobian inside kkt's internals. This function is called every time a new Jacobian is evaluated.\n\nBy default, the function updates in the Jacobian the coefficients associated to the slack variables.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.jtprod!","page":"KKT systems","title":"MadNLP.jtprod!","text":"jtprod!(y::AbstractVector, kkt::AbstractKKTSystem, x::AbstractVector)\n\nMultiply with transpose of Jacobian and store the result in y, such that y = A x (with A current Jacobian).\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.regularize_diagonal!","page":"KKT systems","title":"MadNLP.regularize_diagonal!","text":"regularize_diagonal!(kkt::AbstractKKTSystem, primal_values::Number, dual_values::Number)\n\nRegularize the values in the diagonal of the KKT system. Called internally inside the interior-point routine.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.is_inertia_correct","page":"KKT systems","title":"MadNLP.is_inertia_correct","text":"is_inertia_correct(kkt::AbstractKKTSystem, n::Int, m::Int, p::Int)\n\nCheck if the inertia (n m p) returned by the linear solver is adapted to the KKT system implemented in kkt.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.nnz_jacobian","page":"KKT systems","title":"MadNLP.nnz_jacobian","text":"Nonzero in Jacobian\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#Sparse-KKT-systems","page":"KKT systems","title":"Sparse KKT systems","text":"","category":"section"},{"location":"lib/kkt/","page":"KKT systems","title":"KKT systems","text":"By default, MadNLP stores a AbstractReducedKKTSystem in sparse format, as implemented by SparseKKTSystem:","category":"page"},{"location":"lib/kkt/","page":"KKT systems","title":"KKT systems","text":"SparseKKTSystem\n","category":"page"},{"location":"lib/kkt/#MadNLP.SparseKKTSystem","page":"KKT systems","title":"MadNLP.SparseKKTSystem","text":"SparseKKTSystem{T, VT, MT, QN} <: AbstractReducedKKTSystem{T, VT, MT, QN}\n\nImplement the AbstractReducedKKTSystem in sparse COO format.\n\n\n\n\n\n","category":"type"},{"location":"lib/kkt/","page":"KKT systems","title":"KKT systems","text":"Alternatively, the user has the choice to store the KKT system as a SparseUnreducedKKTSystem or as a SparseCondensedKKTSystem:","category":"page"},{"location":"lib/kkt/","page":"KKT systems","title":"KKT systems","text":"SparseUnreducedKKTSystem\nSparseCondensedKKTSystem","category":"page"},{"location":"lib/kkt/#MadNLP.SparseUnreducedKKTSystem","page":"KKT systems","title":"MadNLP.SparseUnreducedKKTSystem","text":"SparseUnreducedKKTSystem{T, VT, MT, QN} <: AbstractUnreducedKKTSystem{T, VT, MT, QN}\n\nImplement the AbstractUnreducedKKTSystem in sparse COO format.\n\n\n\n\n\n","category":"type"},{"location":"lib/kkt/#MadNLP.SparseCondensedKKTSystem","page":"KKT systems","title":"MadNLP.SparseCondensedKKTSystem","text":"SparseCondensedKKTSystem{T, VT, MT, QN} <: AbstractCondensedKKTSystem{T, VT, MT, QN}\n\nImplement the AbstractCondensedKKTSystem in sparse COO format.\n\n\n\n\n\n","category":"type"},{"location":"lib/kkt/#Dense-KKT-systems","page":"KKT systems","title":"Dense KKT systems","text":"","category":"section"},{"location":"lib/kkt/","page":"KKT systems","title":"KKT systems","text":"MadNLP provides also two structures to store the KKT system in a dense matrix. Although less efficient than their sparse counterparts, these two structures allow to store the KKT system efficiently when the problem is instantiated on the GPU.","category":"page"},{"location":"lib/kkt/","page":"KKT systems","title":"KKT systems","text":"DenseKKTSystem\nDenseCondensedKKTSystem\n","category":"page"},{"location":"lib/kkt/#MadNLP.DenseKKTSystem","page":"KKT systems","title":"MadNLP.DenseKKTSystem","text":"DenseKKTSystem{T, VT, MT, QN, VI} <: AbstractReducedKKTSystem{T, VT, MT, QN}\n\nImplement AbstractReducedKKTSystem with dense matrices.\n\nRequires a dense linear solver to be factorized (otherwise an error is returned).\n\n\n\n\n\n","category":"type"},{"location":"lib/kkt/#MadNLP.DenseCondensedKKTSystem","page":"KKT systems","title":"MadNLP.DenseCondensedKKTSystem","text":"DenseCondensedKKTSystem{T, VT, MT, QN} <: AbstractCondensedKKTSystem{T, VT, MT, QN}\n\nImplement AbstractCondensedKKTSystem with dense matrices.\n\nRequires a dense linear solver to factorize the associated KKT system (otherwise an error is returned).\n\n\n\n\n\n","category":"type"},{"location":"lib/kkt/#AbstractKKTVector","page":"KKT systems","title":"AbstractKKTVector","text":"","category":"section"},{"location":"lib/kkt/","page":"KKT systems","title":"KKT systems","text":"Each instance of AbstractKKTVector implements the following interface.","category":"page"},{"location":"lib/kkt/","page":"KKT systems","title":"KKT systems","text":"AbstractKKTVector\n\nnumber_primal\nnumber_dual\nfull\nprimal\ndual\nprimal_dual\ndual_lb\ndual_ub\n","category":"page"},{"location":"lib/kkt/#MadNLP.AbstractKKTVector","page":"KKT systems","title":"MadNLP.AbstractKKTVector","text":"AbstractKKTVector{T, VT}\n\nSupertype for KKT's right-hand-side vectors (x s y z ν w).\n\n\n\n\n\n","category":"type"},{"location":"lib/kkt/#MadNLP.number_primal","page":"KKT systems","title":"MadNLP.number_primal","text":"number_primal(X::AbstractKKTVector)\n\nGet total number of primal values (x s) in KKT vector X.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.number_dual","page":"KKT systems","title":"MadNLP.number_dual","text":"number_dual(X::AbstractKKTVector)\n\nGet total number of dual values (y z) in KKT vector X.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.full","page":"KKT systems","title":"MadNLP.full","text":"full(X::AbstractKKTVector)\n\nReturn the all the values stored inside the KKT vector X.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.primal","page":"KKT systems","title":"MadNLP.primal","text":"primal(X::AbstractKKTVector)\n\nReturn the primal values (x s) stored in the KKT vector X.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.dual","page":"KKT systems","title":"MadNLP.dual","text":"dual(X::AbstractKKTVector)\n\nReturn the dual values (y z) stored in the KKT vector X.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.primal_dual","page":"KKT systems","title":"MadNLP.primal_dual","text":"primal_dual(X::AbstractKKTVector)\n\nReturn both the primal and the dual values (x s y z) stored in the KKT vector X.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.dual_lb","page":"KKT systems","title":"MadNLP.dual_lb","text":"dual_lb(X::AbstractKKTVector)\n\nReturn the dual values ν associated to the lower-bound stored in the KKT vector X.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.dual_ub","page":"KKT systems","title":"MadNLP.dual_ub","text":"dual_ub(X::AbstractKKTVector)\n\nReturn the dual values w associated to the upper-bound stored in the KKT vector X.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/","page":"KKT systems","title":"KKT systems","text":"By default, MadNLP provides one implementation of an AbstractKKTVector.","category":"page"},{"location":"lib/kkt/","page":"KKT systems","title":"KKT systems","text":"UnreducedKKTVector","category":"page"},{"location":"lib/kkt/#MadNLP.UnreducedKKTVector","page":"KKT systems","title":"MadNLP.UnreducedKKTVector","text":"UnreducedKKTVector{T, VT<:AbstractVector{T}} <: AbstractKKTVector{T, VT}\n\nFull KKT vector (x s y z ν w), associated to a AbstractUnreducedKKTSystem.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/kktsystem/#Implementing-a-custom-KKT-system","page":"Custom KKT system","title":"Implementing a custom KKT system","text":"","category":"section"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"CurrentModule = MadNLP","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"using NLPModels\nusing MadNLP\nusing Random\n\ninclude(\"hs15.jl\")\ninclude(\"diag_kkt.jl\")\n","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"This tutorial explains how to implement a custom AbstractKKTSystem in MadNLP.","category":"page"},{"location":"tutorials/kktsystem/#Structure-exploiting-methods","page":"Custom KKT system","title":"Structure exploiting methods","text":"","category":"section"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"MadNLP gives the user the possibility to exploit the problem's structure at the linear algebra level, when solving the KKT system at every Newton iteration. By default, the KKT system is factorized using a sparse linear solver (MUMPS or HSL ma27/ma57). A sparse linear solver analyses the problem's algebraic structure when computing the symbolic factorization, with a heuristic that determines an optimal elimination tree. As an alternative, the problem's structure can be exploited directly, by specifying the order of the pivots to perform (e.g. using a block elimination algorithm). Doing so usually leads to significant speed-up in the algorithm.","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"We recall that the KKT system solved at each Newton iteration has the structure:","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"overline\nbeginpmatrix\n W        J^top  - I     I \n J        0       0       0 \n -Z_ell  0       X_ell  0 \n Z_u      0       0       X_u\nendpmatrix^K\nbeginpmatrix\n    Delta x \n    Delta y \n    Delta z_ell \n    Delta z_u\nendpmatrix\n=\nbeginpmatrix\n    r_1  r_2  r_3  r_4\nendpmatrix","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"with W a sparse matrix storing the Hessian of the Lagrangian, and J a sparse matrix storing the Jacobian of the constraints. We note the diagonal matrices Z_ell = diag(z_ell), Z_u = diag(z_u), X_ell = diag(x_ell - x), X_u = diag(x - x_u).","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"In MadNLP, every linear system with the structure K is implemented as an AbstractKKTSystem. By default, MadNLP represents a KKT system as a SparseKKTSystem:","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"nlp = HS15Model()\nresults = madnlp(nlp; kkt_system=MadNLP.SparseKKTSystem, linear_solver=LapackCPUSolver)\nnothing\n","category":"page"},{"location":"tutorials/kktsystem/#Solving-AbstractKKTSystem-in-MadNLP","page":"Custom KKT system","title":"Solving AbstractKKTSystem in MadNLP","text":"","category":"section"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"The AbstractKKTSystem object is an abstraction to solve the generic system K x = b. Depending on the implementation, the structure of the linear system is exploited in different manners. Solving a KKT system amounts to the four following operations:","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"Querying the current sensitivities to assemble the different blocks constituting the matrix K.\nAssembling a reduced sparse matrix condensing the sparse matrix K to an equivalent smaller system (and symmetric).\nCalling a linear solver to solve the condensed system.\nCalling a routine to unpack the condensed solution to get the original descent direction (Delta x Delta y Delta z_ell Delta_u).","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"Exploiting the problem's structure usually happens in steps (2) and (4). We skim through the four successive steps in more details.","category":"page"},{"location":"tutorials/kktsystem/#Getting-the-sensitivities","page":"Custom KKT system","title":"Getting the sensitivities","text":"","category":"section"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"The KKT system requires the following information:","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"the (approximated) Hessian of the Lagrangian W ;\nthe constraints' Jacobian J ;\nthe diagonal matrices Z_ell, Z_u and X_ell, X_u.","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"The Hessian and Jacobian are assumed sparse by default.","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"At every IPM iteration, MadNLP updates automatically the values in W, J and in the diagonal matrices Z_ell Z_u X_ell X_u. By default, the following attributes are expected in every instance kkt of an AbstractKKTSystem:","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"kkt.hess: stores the nonzeroes of the Hessian W;\nkkt.jac: stores the nonzeroes of the Jacobian J;\nkkt.l_diag: stores the diagonal entries in X_ell;\nkkt.u_diag: stores the diagonal entries in X_u;\nkkt.l_lower: stores the diagonal entries in Z_ell;\nkkt.u_lower: stores the diagonal entries in Z_u.","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"The attributes kkt.hess and kkt.jac are accessed respectively using the getters get_hessian and get_jacobian.","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"Every time MadNLP queries the Hessian and the Jacobian, it updates the nonzeroes values in kkt.hess and kkt.jac. Rightafter, MadNLP calls respectively the functions compress_hessian! and compress_jacobian! to update all the internal values in the KKT system kkt.","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"To recap, every time we re-evaluate the Jacobian and the Hessian, MadNLP calls automatically the functions:","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"hess = MadNLP.get_hessian(kkt)\nMadNLP.compress_hessian!(kkt)","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"to update the values in the Hessian, and for the Jacobian:","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"jac = MadNLP.get_jacobian(kkt)\nMadNLP.compress_jacobian!(kkt)","category":"page"},{"location":"tutorials/kktsystem/#Assembling-the-KKT-system","page":"Custom KKT system","title":"Assembling the KKT system","text":"","category":"section"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"Once the sensitivities have been updated, we can assemble the KKT matrix K and condense it to an equivalent system K_c before sending it to a linear solver for factorization. The assembling of the KKT system is done in the function build_kkt!.","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"The system is usually stored in the attribute kkt.aug_com. Its dimension depends on the condensation used. The matrix kkt.aug_com can be dense or sparse, depending on the condensation used. MadNLP uses the getter get_kkt to query the matrix kkt.aug_com stored in the KKT system kkt.","category":"page"},{"location":"tutorials/kktsystem/#Solving-the-system","page":"Custom KKT system","title":"Solving the system","text":"","category":"section"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"Once the matrix K_c is assembled, we pass it to the linear solver for factorization. The linear solver is stored internally in kkt: by default, it is stored in the attribute kkt.linear_solver. The factorization is handled internally in MadNLP.","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"Once factorized, it remains to solve the linear system using a backsolve. The backsolve has to be implemented by the user in the function solve!. It reduces the right-hand-side (RHS) down to a form adapted to the condensed matrix K_c and calls the linear solver to perform the backsolve. Then the condensed solution is unpacked to recover the full solution (Delta x Delta y Delta z_ell Delta z_u).","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"To recap, MadNLP assembles and solves the KKT linear system using the following operations:","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"# Assemble\nMadNLP.build_kkt!(kkt)\n# Factorize  the KKT system\nMadNLP.factorize!(kkt.linear_solver)\n# Backsolve\nMadNLP.solve!(kkt, w)\n","category":"page"},{"location":"tutorials/kktsystem/#Example:-Implementing-a-new-KKT-system","page":"Custom KKT system","title":"Example: Implementing a new KKT system","text":"","category":"section"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"As an example, we detail how to implement a custom KKT system in MadNLP. Note that we consider this usage as an advanced use of MadNLP. After this work of caution, let's dive into the details!","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"In this example, we want to approximate the Hessian of the Lagrangian W as a diagonal matrix D_w and solve the following KKT system at each IPM iteration:","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"beginpmatrix\n D_w  J^top  - I  I \n J  0   0  0 \n -Z_ell   0  X_ell  0 \n Z_u  W   0  X_u\nendpmatrix\nbeginpmatrix\n    Delta x \n    Delta y \n    Delta z_ell \n    Delta z_u\nendpmatrix\n=\nbeginpmatrix\n    r_1  r_2  r_3  r_4\nendpmatrix","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"This new system is not equivalent to the original system K, but it's much easier to solve at it does not involve the generic Hessian W. If the diagonal values of D_w are constant and are equal to alpha, the algorithm becomes equivalent to a gradient descent with step alpha^-1.","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"Using the relations Delta z_ell = X_ell^-1 (r_3 + Z_ell Delta x) and Delta z_u = X_u^-1 (r_3 - Z_u Delta x), we condense the matrix down to the reduced form:","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"beginpmatrix\n D_w + Sigma_x  J^top \n J  0  \nendpmatrix\nbeginpmatrix\n    Delta x  \n    Delta y\nendpmatrix\n=\nbeginpmatrix\n    r_1 + X_ell^-1 r_3 - X_u^-1 r_4 r_2\nendpmatrix","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"with the diagonal matrix Sigma_x = -X_ell^-1 Z_ell - X_u^-1 Z_u. The new system is symmetric indefinite, but much easier to solve than the original one.","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"The previous reduction is standard in NLP solvers: MadNLP implements the reduced KKT system operating in the space (Delta x Delta y) using the abstraction AbstractReducedKKTSystem. If D_w is replaced by the original Hessian matrix W, we recover exactly the SparseKKTSystem used by default in MadNLP.","category":"page"},{"location":"tutorials/kktsystem/#Creating-the-KKT-system","page":"Custom KKT system","title":"Creating the KKT system","text":"","category":"section"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"We create a new KKT system DiagonalHessianKKTSystem, inheriting from AbstractReducedKKTSystem. Using generic types, the structure DiagonalHessianKKTSystem is defined as:","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"struct DiagonalHessianKKTSystem{T, VT, MT, QN, LS, VI, VI32} <: MadNLP.AbstractReducedKKTSystem{T, VT, MT, QN}\n    # Nonzeroes values for Hessian and Jacobian\n    hess::VT\n    jac_callback::VT\n    jac::VT\n    # Diagonal matrices\n    reg::VT\n    pr_diag::VT\n    du_diag::VT\n    l_diag::VT\n    u_diag::VT\n    l_lower::VT\n    u_lower::VT\n    # Augmented system K\n    aug_raw::MadNLP.SparseMatrixCOO{T,Int32,VT, VI32}\n    aug_com::MT\n    aug_csc_map::Union{Nothing, VI}\n    # Diagonal of the Hessian\n    diag_hess::VT\n    # Jacobian\n    jac_raw::MadNLP.SparseMatrixCOO{T,Int32,VT, VI32}\n    jac_com::MT\n    jac_csc_map::Union{Nothing, VI}\n    # LinearSolver\n    linear_solver::LS\n    # Info\n    n_var::Int\n    n_ineq::Int\n    n_tot::Int\n    ind_ineq::VI\n    ind_lb::VI\n    ind_ub::VI\n    # Quasi-Newton approximation\n    quasi_newton::QN\nend\n","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"info: Info\nHere, we define a DiagonalHessianKKTSystem as a subtype of a AbstractReducedKKTSystem. Depending on the condensation, the following alternatives are available:AbstractUnreducedKKTSystem: no condensation is applied.\nAbstractCondensedKKTSystem: the reduced KKT system is condensed further by removing the blocks associated to the slack variables.","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"info: Info\nThe attributes pr_diag and du_diag store respectively the primal regularization (terms in the diagonal of the (1, 1) block) and the dual regularization (terms in the diagonal of the (2, 2) block). By default, the dual regularization is keep equal to 0, whereas the primal regularization is set equal to Sigma_x.","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"MadNLP instantiates a new KKT system with the function create_kkt_system, with the following signature:","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"function MadNLP.create_kkt_system(\n    ::Type{DiagonalHessianKKTSystem},\n    cb::MadNLP.SparseCallback{T,VT},\n    ind_cons,\n    linear_solver::Type;\n    opt_linear_solver=MadNLP.default_options(linear_solver),\n    hessian_approximation=MadNLP.ExactHessian,\n    qn_options=MadNLP.QuasiNewtonOptions(),\n) where {T,VT}","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"We pass as arguments:","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"the type of the KKT system to build (here, DiagonalHessianKKTSystem),\nthe structure used to evaluate the callbacks cb,\nthe index of the constraints,\na generic linear solver linear_solver.","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"This function instantiates all the data structures needed in DiagonalHessianKKTSystem. The most difficult part is to assemble the sparse matrices aug_raw and jac_raw, here stored in COO format. This is done in four steps:","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"Step 1. We import the sparsity pattern of the Jacobian :","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"    jac_sparsity_I = MadNLP.create_array(cb, Int32, cb.nnzj)\n    jac_sparsity_J = MadNLP.create_array(cb, Int32, cb.nnzj)\n    MadNLP._jac_sparsity_wrapper!(cb, jac_sparsity_I, jac_sparsity_J)","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"Step 2. We build the resulting KKT matrix aug_raw in COO format, knowing that D_w is diagonal:","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"    # System's dimension\n    n_hess = n_tot # Diagonal Hessian!\n    n_jac = length(jac_sparsity_I)\n    aug_vec_length = n_tot+m\n    aug_mat_length = n_tot+m+n_hess+n_jac+n_slack\n\n    # Build vectors to store COO coortinates\n    I = MadNLP.create_array(cb, Int32, aug_mat_length)\n    J = MadNLP.create_array(cb, Int32, aug_mat_length)\n    V = VT(undef, aug_mat_length)\n    fill!(V, 0.0)  # Need to initiate V to avoid NaN\n\n    offset = n_tot+n_jac+n_slack+n_hess+m\n\n    # Primal regularization block\n    I[1:n_tot] .= 1:n_tot\n    J[1:n_tot] .= 1:n_tot\n    # Hessian block\n    I[n_tot+1:n_tot+n_hess] .= 1:n_tot # diagonal Hessian!\n    J[n_tot+1:n_tot+n_hess] .= 1:n_tot # diagonal Hessian!\n    # Jacobian block\n    I[n_tot+n_hess+1:n_tot+n_hess+n_jac] .= (jac_sparsity_I.+n_tot)\n    J[n_tot+n_hess+1:n_tot+n_hess+n_jac] .= jac_sparsity_J\n    # Slack block\n    I[n_tot+n_hess+n_jac+1:n_tot+n_hess+n_jac+n_slack] .= ind_ineq .+ n_tot\n    J[n_tot+n_hess+n_jac+1:n_tot+n_hess+n_jac+n_slack] .= (n+1:n+n_slack)\n    # Dual regularization block\n    I[n_tot+n_hess+n_jac+n_slack+1:offset] .= (n_tot+1:n_tot+m)\n    J[n_tot+n_hess+n_jac+n_slack+1:offset] .= (n_tot+1:n_tot+m)\n\n    aug_raw = MadNLP.SparseMatrixCOO(aug_vec_length, aug_vec_length, I, J, V)","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"Step 3. We convert aug_raw from COO to CSC using the following utilities:","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"    aug_com, aug_csc_map = MadNLP.coo_to_csc(aug_raw)","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"Step 4. We pass the matrix in CSC format to the linear solver:","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"    _linear_solver = linear_solver(\n        aug_com; opt = opt_linear_solver\n    )\n","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"info: Info\nStoring the Hessian and Jacobian, even in sparse format, is expensive in term of memory. For that reason, MadNLP stores the Hessian and Jacobian only once in the KKT system.","category":"page"},{"location":"tutorials/kktsystem/#Getting-the-sensitivities-2","page":"Custom KKT system","title":"Getting the sensitivities","text":"","category":"section"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"MadNLP requires the following getters to update the sensitivities. As much as we can, we try to update the values inplace in the matrix aug_raw. We use the default implementation of compress_jacobian! in MadNLP:","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"function MadNLP.compress_jacobian!(kkt::DiagonalHessianKKTSystem)\n    ns = length(kkt.ind_ineq)\n    kkt.jac[end-ns+1:end] .= -1.0\n    MadNLP.transfer!(kkt.jac_com, kkt.jac_raw, kkt.jac_csc_map)\n    return\nend","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"The term -1.0 accounts for the slack variables used to reformulate the inequality constraints as equality constraints.","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"For compress_hessian!, we take into account that the diagonal matrix D_w is the diagonal of the Hessian:","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"function MadNLP.compress_hessian!(kkt::DiagonalHessianKKTSystem)\n    kkt.diag_hess .= 1.0\n    return\nend","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"MadNLP also needs the following basic functions to get the different matrices and the dimension of the linear system:","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"MadNLP.num_variables(kkt::DiagonalHessianKKTSystem) = length(kkt.diag_hess)\nMadNLP.get_kkt(kkt::DiagonalHessianKKTSystem) = kkt.aug_com\nMadNLP.get_jacobian(kkt::DiagonalHessianKKTSystem) = kkt.jac_callback\nfunction MadNLP.jtprod!(y::AbstractVector, kkt::DiagonalHessianKKTSystem, x::AbstractVector)\n    mul!(y, kkt.jac_com', x)\nend","category":"page"},{"location":"tutorials/kktsystem/#Assembling-the-KKT-system-2","page":"Custom KKT system","title":"Assembling the KKT system","text":"","category":"section"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"Once the sensitivities are updated, we assemble the new matrix K_c first in COO format in kkt.aug_raw, before converting the matrix to CSC format in kkt.jac_com using the utility MadNLP.transfer!:","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"function MadNLP.build_kkt!(kkt::DiagonalHessianKKTSystem)\n    MadNLP.transfer!(kkt.aug_com, kkt.aug_raw, kkt.aug_csc_map)\nend","category":"page"},{"location":"tutorials/kktsystem/#Solving-the-system-2","page":"Custom KKT system","title":"Solving the system","text":"","category":"section"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"It remains to implement the backsolve. For the reduced KKT formulation, the RHS r_1 + X_ell^-1 r_3 - X_u^-1 r_4 is built automatically using the function MadNLP.reduce_rhs!. The backsolve solves for (Delta x Delta y). The dual's descent direction Delta z_ell and Delta z_u are recovered afterwards using the function MadNLP.finish_aug_solve!:","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"function MadNLP.solve!(kkt::DiagonalHessianKKTSystem, w::MadNLP.AbstractKKTVector)\n    MadNLP.reduce_rhs!(w.xp_lr, dual_lb(w), kkt.l_diag, w.xp_ur, dual_ub(w), kkt.u_diag)\n    MadNLP.solve!(kkt.linear_solver, primal_dual(w))\n    MadNLP.finish_aug_solve!(kkt, w)\n    return w\nend","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"note: Note\nThe function solve! takes as second argument a vector w being a AbstractKKTVector. A AbstractKKTVector is a convenient data structure used in MadNLP to store and access the elements in the primal-dual vector (Delta x Delta y Delta z_ell Delta z_u).","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"warning: Warning\nWhen calling solve!, the values in the vector w are updated inplace. The vector w should be initialized with the RHS (r_1 r_2 r_3 r_4) before calling the function solve!. The function modifies the values directly in the vector w to return the solution (Delta x Delta y Delta z_ell Delta z_u).","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"Last, MadNLP implements an iterative refinement method to get accurate descent directions in the final iterations. The iterative refinement algorithm implements Richardson's method, which requires multiplying the KKT matrix K on the right by any vector w = (w_x w_y w_z_l w_z_u). This is provided in MadNLP by overloading the function LinearAlgebra.mul!:","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"function LinearAlgebra.mul!(\n    w::MadNLP.AbstractKKTVector{T},\n    kkt::DiagonalHessianKKTSystem,\n    x::MadNLP.AbstractKKTVector{T},\n    alpha = one(T),\n    beta = zero(T),\n) where {T}\n\n    mul!(primal(w), Diagonal(kkt.diag_hess), primal(x), alpha, beta)\n    mul!(primal(w), kkt.jac_com', dual(x), alpha, one(T))\n    mul!(dual(w), kkt.jac_com,  primal(x), alpha, beta)\n\n    # Reduce KKT vector\n    MadNLP._kktmul!(w,x,kkt.reg,kkt.du_diag,kkt.l_lower,kkt.u_lower,kkt.l_diag,kkt.u_diag, alpha, beta)\n    return w\nend","category":"page"},{"location":"tutorials/kktsystem/#Demonstration","page":"Custom KKT system","title":"Demonstration","text":"","category":"section"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"We now have all the elements we need to solve the problem with the new KKT linear system DiagonalHessianKKTSystem. We just have to pass the KKT system to MadNLP using the option kkt_system:","category":"page"},{"location":"tutorials/kktsystem/","page":"Custom KKT system","title":"Custom KKT system","text":"nlp = HS15Model()\nresults = madnlp(nlp; kkt_system=DiagonalHessianKKTSystem, linear_solver=LapackCPUSolver)\nnothing\n","category":"page"},{"location":"tutorials/multiprecision/#Running-MadNLP-in-arbitrary-precision","page":"Multi-precision","title":"Running MadNLP in arbitrary precision","text":"","category":"section"},{"location":"tutorials/multiprecision/","page":"Multi-precision","title":"Multi-precision","text":"CurrentModule = MadNLP","category":"page"},{"location":"tutorials/multiprecision/","page":"Multi-precision","title":"Multi-precision","text":"using NLPModels\nusing MadNLP\n","category":"page"},{"location":"tutorials/multiprecision/","page":"Multi-precision","title":"Multi-precision","text":"MadNLP is written in pure Julia, and as such support solving optimization problems in arbitrary precision. By default, MadNLP adapts its precision according to the NLPModel passed in input. Most models use Float64 (in fact, almost all optimization modelers are implemented using double precision), but for certain applications it can be useful to use arbitrary precision to get more accurate solution.","category":"page"},{"location":"tutorials/multiprecision/","page":"Multi-precision","title":"Multi-precision","text":"info: Info\nThere exists different packages to instantiate a optimization model in arbitrary precision in Julia. Most of them leverage the flexibility offered by NLPModels.jl. In particular, we recommend:CUTEst.jl: supports Float32, Float64 and Float128.\nExaModels: supports AbstractFloat.","category":"page"},{"location":"tutorials/multiprecision/#Defining-a-problem-in-arbitrary-precision","page":"Multi-precision","title":"Defining a problem in arbitrary precision","text":"","category":"section"},{"location":"tutorials/multiprecision/","page":"Multi-precision","title":"Multi-precision","text":"As a demonstration, we implement the model airport from CUTEst using ExaModels. The code writes:","category":"page"},{"location":"tutorials/multiprecision/","page":"Multi-precision","title":"Multi-precision","text":"using ExaModels\n\nfunction airport_model(T)\n    N = 42\n    # Data\n    r = T[0.09 , 0.3, 0.09, 0.45, 0.5, 0.04, 0.1, 0.02, 0.02, 0.07, 0.4, 0.045, 0.05, 0.056, 0.36, 0.08, 0.07, 0.36, 0.67, 0.38, 0.37, 0.05, 0.4, 0.66, 0.05, 0.07, 0.08, 0.3, 0.31, 0.49, 0.09, 0.46, 0.12, 0.07, 0.07, 0.09, 0.05, 0.13, 0.16, 0.46, 0.25, 0.1]\n    cx = T[-6.3, -7.8, -9.0, -7.2, -5.7, -1.9, -3.5, -0.5, 1.4, 4.0, 2.1, 5.5, 5.7, 5.7, 3.8, 5.3, 4.7, 3.3, 0.0, -1.0, -0.4, 4.2, 3.2, 1.7, 3.3, 2.0, 0.7, 0.1, -0.1, -3.5, -4.0, -2.7, -0.5, -2.9, -1.2, -0.4, -0.1, -1.0, -1.7, -2.1, -1.8, 0.0]\n    cy = T[8.0, 5.1, 2.0, 2.6, 5.5, 7.1, 5.9, 6.6, 6.1, 5.6, 4.9, 4.7, 4.3, 3.6, 4.1, 3.0, 2.4, 3.0, 4.7, 3.4, 2.3, 1.5, 0.5, -1.7, -2.0, -3.1, -3.5, -2.4, -1.3, 0.0, -1.7, -2.1, -0.4, -2.9, -3.4, -4.3, -5.2, -6.5, -7.5, -6.4, -5.1, 0.0]\n    # Wrap all data in a single iterator for ExaModels\n    data = [(i, cx[i], cy[i], r[i]) for i in 1:N]\n    IJ = [(i, j) for i in 1:N-1 for j in i+1:N]\n    # Write model using ExaModels\n    core = ExaModels.ExaCore(T)\n    x = ExaModels.variable(core, 1:N, lvar = -10.0, uvar=10.0)\n    y = ExaModels.variable(core, 1:N, lvar = -10.0, uvar=10.0)\n    ExaModels.objective(\n        core,\n        ((x[i] - x[j])^2 + (y[i] - y[j])^2) for (i, j) in IJ\n    )\n    ExaModels.constraint(core, (x[i]-dcx)^2 + (y[i] - dcy)^2 - dr for (i, dcx, dcy, dr) in data; lcon=-Inf)\n    return ExaModels.ExaModel(core)\nend","category":"page"},{"location":"tutorials/multiprecision/","page":"Multi-precision","title":"Multi-precision","text":"The function airport_model takes as input the type used to define the model in ExaModels. For example, ExaCore(Float64) instantiates a model with Float64, whereas ExaCore(Float32) instantiates a model using Float32. Thus, instantiating the instance airport using Float32 simply amounts to","category":"page"},{"location":"tutorials/multiprecision/","page":"Multi-precision","title":"Multi-precision","text":"nlp = airport_model(Float32)\n","category":"page"},{"location":"tutorials/multiprecision/","page":"Multi-precision","title":"Multi-precision","text":"We verify that the model is correctly instantiated using Float32:","category":"page"},{"location":"tutorials/multiprecision/","page":"Multi-precision","title":"Multi-precision","text":"x0 = NLPModels.get_x0(nlp)\nprintln(typeof(x0))","category":"page"},{"location":"tutorials/multiprecision/#Solving-a-problem-in-Float32","page":"Multi-precision","title":"Solving a problem in Float32","text":"","category":"section"},{"location":"tutorials/multiprecision/","page":"Multi-precision","title":"Multi-precision","text":"Now that we have defined our model in Float32, we solve it using MadNLP. As nlp is using Float32, MadNLP will automatically adjust its internal types to Float32 during the instantiation. By default, the convergence tolerance is also adjusted to the input type, such that tol = sqrt(eps(T)). Hence, in our case the tolerance is set automatically to","category":"page"},{"location":"tutorials/multiprecision/","page":"Multi-precision","title":"Multi-precision","text":"tol = sqrt(eps(Float32))","category":"page"},{"location":"tutorials/multiprecision/","page":"Multi-precision","title":"Multi-precision","text":"We solve the problem using Lapack as linear solver:","category":"page"},{"location":"tutorials/multiprecision/","page":"Multi-precision","title":"Multi-precision","text":"results = madnlp(nlp; linear_solver=LapackCPUSolver)","category":"page"},{"location":"tutorials/multiprecision/","page":"Multi-precision","title":"Multi-precision","text":"note: Note\nNote that the distribution of Lapack shipped with Julia supports Float32, so here we do not have to worry whether the type is supported by the linear solver. Almost all linear solvers shipped with MadNLP supports Float32.","category":"page"},{"location":"tutorials/multiprecision/","page":"Multi-precision","title":"Multi-precision","text":"The final solution is","category":"page"},{"location":"tutorials/multiprecision/","page":"Multi-precision","title":"Multi-precision","text":"results.solution\n","category":"page"},{"location":"tutorials/multiprecision/","page":"Multi-precision","title":"Multi-precision","text":"and the objective is","category":"page"},{"location":"tutorials/multiprecision/","page":"Multi-precision","title":"Multi-precision","text":"results.objective\n","category":"page"},{"location":"tutorials/multiprecision/","page":"Multi-precision","title":"Multi-precision","text":"For completeness, we compare with the solution returned when we solve the same problem using Float64:","category":"page"},{"location":"tutorials/multiprecision/","page":"Multi-precision","title":"Multi-precision","text":"nlp_64 = airport_model(Float64)\nresults_64 = madnlp(nlp_64; linear_solver=LapackCPUSolver)","category":"page"},{"location":"tutorials/multiprecision/","page":"Multi-precision","title":"Multi-precision","text":"The final objective is now","category":"page"},{"location":"tutorials/multiprecision/","page":"Multi-precision","title":"Multi-precision","text":"results_64.objective\n","category":"page"},{"location":"tutorials/multiprecision/","page":"Multi-precision","title":"Multi-precision","text":"As expected when solving an optimization problem with Float32, the relative difference between the two solutions is far from being negligible:","category":"page"},{"location":"tutorials/multiprecision/","page":"Multi-precision","title":"Multi-precision","text":"rel_diff = abs(results.objective - results_64.objective) / results_64.objective","category":"page"},{"location":"tutorials/multiprecision/#Solving-a-problem-in-Float128","page":"Multi-precision","title":"Solving a problem in Float128","text":"","category":"section"},{"location":"tutorials/multiprecision/","page":"Multi-precision","title":"Multi-precision","text":"Now, we go in the opposite direction and solve a problem using Float128 to get a better accuracy. We start by loading the library Quadmath to work with quadruple precision:","category":"page"},{"location":"tutorials/multiprecision/","page":"Multi-precision","title":"Multi-precision","text":"using Quadmath","category":"page"},{"location":"tutorials/multiprecision/","page":"Multi-precision","title":"Multi-precision","text":"We can instantiate our problem using Float128 directly as:","category":"page"},{"location":"tutorials/multiprecision/","page":"Multi-precision","title":"Multi-precision","text":"nlp_128 = airport_model(Float128)","category":"page"},{"location":"tutorials/multiprecision/","page":"Multi-precision","title":"Multi-precision","text":"warning: Warning\nUnfortunately, a few linear solvers support Float128 out of the box. Currently, the only solver suporting quadruple in MadNLP is LDLSolver, which implements an LDL factorization in pure Julia. The solver LDLSolver is not adapted to solve large-scale nonconvex nonlinear programs, but works if the problem is small enough (as it is the case here).","category":"page"},{"location":"tutorials/multiprecision/","page":"Multi-precision","title":"Multi-precision","text":"Replacing the solver by LDLSolver, solving the problem with MadNLP just amounts to","category":"page"},{"location":"tutorials/multiprecision/","page":"Multi-precision","title":"Multi-precision","text":"results_128 = madnlp(nlp_128; linear_solver=LDLSolver)\n","category":"page"},{"location":"tutorials/multiprecision/","page":"Multi-precision","title":"Multi-precision","text":"Note that the final tolerance is much lower than before. We get the solution in quadruple precision","category":"page"},{"location":"tutorials/multiprecision/","page":"Multi-precision","title":"Multi-precision","text":"results_128.solution","category":"page"},{"location":"tutorials/multiprecision/","page":"Multi-precision","title":"Multi-precision","text":"as well as the final objective:","category":"page"},{"location":"tutorials/multiprecision/","page":"Multi-precision","title":"Multi-precision","text":"results_128.objective","category":"page"},{"location":"tutorials/lbfgs/#Limited-memory-BFGS","page":"LBFGS","title":"Limited-memory BFGS","text":"","category":"section"},{"location":"tutorials/lbfgs/","page":"LBFGS","title":"LBFGS","text":"CurrentModule = MadNLP","category":"page"},{"location":"tutorials/lbfgs/","page":"LBFGS","title":"LBFGS","text":"using NLPModels\nusing MadNLP\nusing Random\nusing ExaModels\n","category":"page"},{"location":"tutorials/lbfgs/","page":"LBFGS","title":"LBFGS","text":"Sometimes, the second-order derivatives are just too expensive to evaluate. In that case, it is often a good idea to approximate the Hessian matrix. The BFGS algorithm uses the first-order derivatives (gradient and tranposed-Jacobian vector product) to approximate the Hessian of the Lagrangian. LBFGS is a variant of BFGS that computes a low-rank approximation of the Hessian matrix from the past iterates. That way, LBFGS does not have to store a large dense matrix in memory, rendering the algorithm appropriate in the large-scale regime.","category":"page"},{"location":"tutorials/lbfgs/","page":"LBFGS","title":"LBFGS","text":"MadNLP implements several quasi-Newton approximation for the Hessian matrix. In this page, we focus on the limited-memory version of the BFGS algorithm, commonly known as LBFGS. We refer to this article for a detailed description of the method.","category":"page"},{"location":"tutorials/lbfgs/#How-to-set-up-LBFGS-in-MadNLP?","page":"LBFGS","title":"How to set up LBFGS in MadNLP?","text":"","category":"section"},{"location":"tutorials/lbfgs/","page":"LBFGS","title":"LBFGS","text":"We look at the elec optimization problem from the COPS benchmark:","category":"page"},{"location":"tutorials/lbfgs/","page":"LBFGS","title":"LBFGS","text":"function elec_model(np)\n    Random.seed!(1)\n    # Set the starting point to a quasi-uniform distribution of electrons on a unit sphere\n    theta = (2pi) .* rand(np)\n    phi = pi .* rand(np)\n\n    core = ExaModels.ExaCore(Float64)\n    x = ExaModels.variable(core, 1:np; start = [cos(theta[i])*sin(phi[i]) for i=1:np])\n    y = ExaModels.variable(core, 1:np; start = [sin(theta[i])*sin(phi[i]) for i=1:np])\n    z = ExaModels.variable(core, 1:np; start = [cos(phi[i]) for i=1:np])\n    # Coulomb potential\n    itr = [(i,j) for i in 1:np-1 for j in i+1:np]\n    ExaModels.objective(core, 1.0 / sqrt((x[i] - x[j])^2 + (y[i] - y[j])^2 + (z[i] - z[j])^2) for (i,j) in itr)\n    # Unit-ball\n    ExaModels.constraint(core, x[i]^2 + y[i]^2 + z[i]^2 - 1 for i=1:np)\n\n    return ExaModels.ExaModel(core)\nend\n","category":"page"},{"location":"tutorials/lbfgs/","page":"LBFGS","title":"LBFGS","text":"The problem computes the positions of the electrons in an atom. The potential of each electron depends on the positions of all the other electrons: all the variables in the problem are coupled, resulting in a dense Hessian matrix. Hence, the problem is good candidate for a quasi-Newton algorithm.","category":"page"},{"location":"tutorials/lbfgs/","page":"LBFGS","title":"LBFGS","text":"We start by solving the problem with the default options in MadNLP, using a dense linear solver from LAPACK:","category":"page"},{"location":"tutorials/lbfgs/","page":"LBFGS","title":"LBFGS","text":"nh = 10\nnlp = elec_model(nh)\nresults_hess = madnlp(\n    nlp;\n    linear_solver=LapackCPUSolver,\n)\nnothing\n","category":"page"},{"location":"tutorials/lbfgs/","page":"LBFGS","title":"LBFGS","text":"We observe that MadNLP converges in 21 iterations.","category":"page"},{"location":"tutorials/lbfgs/","page":"LBFGS","title":"LBFGS","text":"To replace the second-order derivatives by an LBFGS approximation, you should pass the option hessian_approximation=CompactLBFGS to MadNLP.","category":"page"},{"location":"tutorials/lbfgs/","page":"LBFGS","title":"LBFGS","text":"results_qn = madnlp(\n    nlp;\n    linear_solver=LapackCPUSolver,\n    hessian_approximation=MadNLP.CompactLBFGS,\n)\nnothing\n","category":"page"},{"location":"tutorials/lbfgs/","page":"LBFGS","title":"LBFGS","text":"We observe that MadNLP converges in 93 iterations. As expected, the number of Hessian evaluations is 0.","category":"page"},{"location":"tutorials/lbfgs/#How-to-tune-the-options-in-LBGS?","page":"LBFGS","title":"How to tune the options in LBGS?","text":"","category":"section"},{"location":"tutorials/lbfgs/","page":"LBFGS","title":"LBFGS","text":"You can tune the LBFGS options by using the option quasi_newton_options. The option takes as input a QuasiNewtonOptions structure, with the following attributes (we put on the right their default values):","category":"page"},{"location":"tutorials/lbfgs/","page":"LBFGS","title":"LBFGS","text":"init_strategy::BFGSInitStrategy = SCALAR1\nmax_history::Int = 6\ninit_value::Float64 = 1.0\nsigma_min::Float64 = 1e-8\nsigma_max::Float64 = 1e+8","category":"page"},{"location":"tutorials/lbfgs/","page":"LBFGS","title":"LBFGS","text":"The most important parameter is max_history, which encodes the number of vectors used in the low-rank approximation. For instance, we can increase the history to use the 20 past iterates using:","category":"page"},{"location":"tutorials/lbfgs/","page":"LBFGS","title":"LBFGS","text":"qn_options = MadNLP.QuasiNewtonOptions(; max_history=20)\nresults_qn = madnlp(\n    nlp;\n    linear_solver=LapackCPUSolver,\n    hessian_approximation=MadNLP.CompactLBFGS,\n    quasi_newton_options=qn_options,\n)\nnothing\n","category":"page"},{"location":"tutorials/lbfgs/","page":"LBFGS","title":"LBFGS","text":"We observe that the total number of iterations has been reduced from 93 to 60.","category":"page"},{"location":"tutorials/lbfgs/#How-does-LBFGS-is-implemented-in-MadNLP?","page":"LBFGS","title":"How does LBFGS is implemented in MadNLP?","text":"","category":"section"},{"location":"tutorials/lbfgs/","page":"LBFGS","title":"LBFGS","text":"MadNLP implements the compact LBFGS algorithm described in this article. At each iteration, the Hessian W_k is approximated by a low rank positive definite matrix B_k, defined as","category":"page"},{"location":"tutorials/lbfgs/","page":"LBFGS","title":"LBFGS","text":"B_k = xi_k I + U_k V_k^top\n","category":"page"},{"location":"tutorials/lbfgs/","page":"LBFGS","title":"LBFGS","text":"with xi  0 a scaling factor, U_k and V_k two n times 2p matrices. The number p denotes the number of vectors used when computing the limited memory updates (the parameter max_history in MadNLP): the larger, the more accurate is the low-rank approximation.","category":"page"},{"location":"tutorials/lbfgs/","page":"LBFGS","title":"LBFGS","text":"Replacing the Hessian of the Lagrangian W_k by the low-rank matrix B_k, the KKT system solved in MadNLP rewrites as","category":"page"},{"location":"tutorials/lbfgs/","page":"LBFGS","title":"LBFGS","text":"beginbmatrix\nxi I + U V^top + Sigma_x  0  A^top \n0  Sigma_s  -I \nA  -I  0\nendbmatrix\nbeginbmatrix\nDelta x  Delta s  Delta y\nendbmatrix\n=\nbeginbmatrix\nr_1  r_2  r_3\nendbmatrix\n","category":"page"},{"location":"tutorials/lbfgs/","page":"LBFGS","title":"LBFGS","text":"The KKT system has a low-rank structure we can exploit using the Sherman-Morrison formula. The method is detailed e.g. in Section 3.8 of that paper.","category":"page"},{"location":"tutorials/lbfgs/","page":"LBFGS","title":"LBFGS","text":"info: Info\nAs MadNLP is designed to solve generic constrained optimization problems, it does not approximate the inverse of the Hessian matrix, as done in other implementations of the LBFGS algorithm specialized on solving nonlinear problems with bound constraints. If your problem has no generic nonlinear constraints, we recommend for optimal performance using LBFGSB or the LBFGS implemented in JSOSolvers.jl.","category":"page"},{"location":"lib/ipm/","page":"IPM solver","title":"IPM solver","text":"CurrentModule = MadNLP","category":"page"},{"location":"lib/ipm/#MadNLP-solver","page":"IPM solver","title":"MadNLP solver","text":"","category":"section"},{"location":"lib/ipm/","page":"IPM solver","title":"IPM solver","text":"MadNLP takes as input a nonlinear program encoded as a AbstractNLPModel and solve it using interior-point. The main entry point is the function madnlp:","category":"page"},{"location":"lib/ipm/","page":"IPM solver","title":"IPM solver","text":"madnlp\nMadNLPExecutionStats","category":"page"},{"location":"lib/ipm/#MadNLP.madnlp","page":"IPM solver","title":"MadNLP.madnlp","text":"madnlp(model::AbstractNLPModel; options...)\n\nBuild a MadNLPSolver and solve it using the interior-point method. Return the solution as a MadNLPExecutionStats.\n\n\n\n\n\n","category":"function"},{"location":"lib/ipm/#MadNLP.MadNLPExecutionStats","page":"IPM solver","title":"MadNLP.MadNLPExecutionStats","text":"MadNLPExecutionStats{T, VT} <: AbstractExecutionStats\n\nStore the results returned by MadNLP once the interior-point algorithm has terminated.\n\n\n\n\n\n","category":"type"},{"location":"lib/ipm/","page":"IPM solver","title":"IPM solver","text":"In detail, the function madnlp builds a MadNLPSolver storing all the required structures in the solution algorithm. Once the MadNLPSolver instantiated, the function solve! is applied to solve the nonlinear program with MadNLP's interior-point algorithm.","category":"page"},{"location":"lib/ipm/","page":"IPM solver","title":"IPM solver","text":"MadNLPSolver\n","category":"page"},{"location":"lib/ipm/#MadNLP.MadNLPSolver","page":"IPM solver","title":"MadNLP.MadNLPSolver","text":"MadNLPSolver(nlp::AbstractNLPModel{T, VT}; options...) where {T, VT}\n\nInstantiate a new MadNLPSolver associated to the nonlinear program nlp::AbstractNLPModel. The options are passed as optional arguments.\n\nThe constructor allocates all the memory required in the interior-point algorithm, so the main algorithm remains allocation free.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/warmstart/#Warmstarting-MadNLP","page":"Warm-start","title":"Warmstarting MadNLP","text":"","category":"section"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"CurrentModule = MadNLP","category":"page"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"using NLPModels\nusing MadNLP\ninclude(\"hs15.jl\")\n","category":"page"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"We use a parameterized version of the instance HS15 used in the introduction. This updated version of HS15Model stores the parameters of the model in an attribute nlp.params:","category":"page"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"\nnlp = HS15Model()\nprintln(nlp.params)","category":"page"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"By default the parameters are set to [100.0, 1.0]. In a first solve, we find a solution associated to these parameters. We want to warmstart MadNLP from the solution found in the first solve, after a small update in the problem's parameters.","category":"page"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"info: Info\nIt is known that the interior-point method has a poor support of warmstarting, on the contrary to active-set methods. However, if the parameter changes remain reasonable and do not lead to significant changes in the active set, warmstarting the interior-point algorithm can significantly reduces the total number of barrier iterations in the second solve.","category":"page"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"warning: Warning\nThe warm-start described in this tutorial remains basic. Its main application is updating the solution of a parametric problem after an update in the parameters. The warm-start always assumes that the structure of the problem remains the same between two consecutive solves. MadNLP cannot be warm-started if variables or constraints are added to the problem.","category":"page"},{"location":"tutorials/warmstart/#Naive-solution:-starting-from-the-previous-solution","page":"Warm-start","title":"Naive solution: starting from the previous solution","text":"","category":"section"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"By default, MadNLP starts its interior-point algorithm at the primal variable stored in nlp.meta.x0. We can access this attribute using the function get_x0:","category":"page"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"x0 = NLPModels.get_x0(nlp)\n","category":"page"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"Here, we observe that the initial solution is [0, 0].","category":"page"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"We solve the problem using the function madnlp:","category":"page"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"results = madnlp(nlp)\nnothing","category":"page"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"MadNLP converges in 19 barrier iterations.  The solution is:","category":"page"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"println(\"Objective: \", results.objective)\nprintln(\"Solution:  \", results.solution)","category":"page"},{"location":"tutorials/warmstart/#Solution-1:-updating-the-starting-solution","page":"Warm-start","title":"Solution 1: updating the starting solution","text":"","category":"section"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"We have found a solution to the problem. Now, what happens if we update the parameters inside nlp?","category":"page"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"nlp.params .= [101.0, 1.1]","category":"page"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"As MadNLP starts the algorithm at nlp.meta.x0, we pass the previous solution to the initial vector:","category":"page"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"copyto!(NLPModels.get_x0(nlp), results.solution)","category":"page"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"Solving the problem again with MadNLP, we observe that MadNLP converges in only 6 iterations:","category":"page"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"results_new = madnlp(nlp)\nnothing\n","category":"page"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"By decreasing the initial barrier parameter, we can reduce the total number of iterations to 5:","category":"page"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"results_new = madnlp(nlp; mu_init=1e-7)\nnothing\n","category":"page"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"The final solution is slightly different from the previous one, as we have updated the parameters inside the model nlp:","category":"page"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"results_new.solution\n","category":"page"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"info: Info\nSimilarly as with the primal solution, we can pass the initial dual solution to MadNLP using the function get_y0. We can overwrite the value of y0 in nlp using:copyto!(NLPModels.get_y0(nlp), results.multipliers)In our particular example, setting the dual multipliers has only a minor influence on the convergence of the algorithm.","category":"page"},{"location":"tutorials/warmstart/#Advanced-solution:-keeping-the-solver-in-memory","page":"Warm-start","title":"Advanced solution: keeping the solver in memory","text":"","category":"section"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"The previous solution works, but is wasteful in resource: each time we call the function madnlp we create a new instance of MadNLPSolver, leading to a significant number of memory allocations. A workaround is to keep the solver in memory to have more fine-grained control on the warm-start.","category":"page"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"We start by creating a new model nlp and we instantiate a new instance of MadNLPSolver attached to this model:","category":"page"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"nlp = HS15Model()\nsolver = MadNLP.MadNLPSolver(nlp)","category":"page"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"Note that","category":"page"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"nlp === solver.nlp","category":"page"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"Hence, updating the parameter values in nlp will automatically update the parameters in the solver.","category":"page"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"We solve the problem using the function solve!:","category":"page"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"results = MadNLP.solve!(solver)","category":"page"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"Before warmstarting MadNLP, we proceed as before and update the parameters and the primal solution in nlp:","category":"page"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"nlp.params .= [101.0, 1.1]\ncopyto!(NLPModels.get_x0(nlp), results.solution)","category":"page"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"MadNLP stores in memory the dual solutions computed during the first solve. One can access to the (scaled) multipliers as","category":"page"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"solver.y","category":"page"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"and to the multipliers of the bound constraints with","category":"page"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"[solver.zl.values solver.zu.values]","category":"page"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"warning: Warning\nIf we call the function solve! a second-time, MadNLP will use the following rule:The initial primal solution is copied from NLPModels.get_x0(nlp)\nThe initial dual solution is directly taken from the values specified in solver.y, solver.zl and solver.zu. (MadNLP is not using the values stored in nlp.meta.y0 in the second solve).","category":"page"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"As before, it is advised to decrease the initial barrier parameter: if the initial point is close enough to the solution, this reduces drastically the total number of iterations. We solve the problem again using:","category":"page"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"MadNLP.solve!(solver; mu_init=1e-7)\nnothing","category":"page"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"Three observations are in order:","category":"page"},{"location":"tutorials/warmstart/","page":"Warm-start","title":"Warm-start","text":"The iteration count starts directly from the previous count (as stored in solver.cnt.k).\nMadNLP converges in only 4 iterations.\nThe factorization stored in solver is directly re-used, leading to significant savings. As a consequence, the warm-start does not work if the structure of the problem changes between the first and the second solve (e.g, if variables or constraints are added to the constraints).","category":"page"},{"location":"quickstart/#Quickstart","page":"Quickstart","title":"Quickstart","text":"","category":"section"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"CurrentModule = MadNLP","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"using NLPModels\n","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"This page presents a quickstart guide to solve a nonlinear problem with MadNLP.","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"As a demonstration, we show how to implement the HS15 nonlinear problem from the Hock & Schittkowski collection, first by using a nonlinear modeler and then by specifying the derivatives manually.","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"The HS15 problem is defined as:","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"beginaligned\nmin_x_1 x_2     100 times (x_2 - x_1^2)^2 +(1 - x_1)^2 \ntextsubject to quad   x_1  times x_2 geq 1 \n         x_1 + x_2^2 geq 0 \n         x_1 leq 05\nendaligned\n","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"Despite its small dimension, its resolution remains challenging as the problem is nonlinear nonconvex. Note that HS15 encompasses one bound constraint (x_1 leq 05) and two generic constraints.","category":"page"},{"location":"quickstart/#Using-a-nonlinear-modeler:-JuMP.jl","page":"Quickstart","title":"Using a nonlinear modeler: JuMP.jl","text":"","category":"section"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"The easiest way to implement a nonlinear problem is to use a nonlinear modeler as JuMP. In JuMP, the user just has to pass the structure of the problem, the computation of the first- and second-order derivatives being handled automatically.","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"Using JuMP's syntax, the HS15 problem translates to","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"using JuMP\nmodel = Model()\n@variable(model, x1 <= 0.5)\n@variable(model, x2)\n\n@objective(model, Min, 100.0 * (x2 - x1^2)^2 + (1.0 - x1)^2)\n@constraint(model, x1 * x2 >= 1.0)\n@constraint(model, x1 + x2^2 >= 0.0)\n\nprintln(model)\n","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"Then, solving HS15 with MadNLP directly translates to","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"using MadNLP\nJuMP.set_optimizer(model, MadNLP.Optimizer)\nJuMP.optimize!(model)\n","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"Under the hood, JuMP builds a nonlinear model with a sparse AD backend to evaluate the first and second-order derivatives of the objective and the constraints. Internally, MadNLP takes as input the callbacks generated by JuMP and wraps them inside a MadNLP.MOIModel.","category":"page"},{"location":"quickstart/#Specifying-the-derivatives-by-hand:-NLPModels.jl","page":"Quickstart","title":"Specifying the derivatives by hand: NLPModels.jl","text":"","category":"section"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"Alternatively, we can compute the derivatives manually and define directly a NLPModel associated to our problem. This second option, although more complicated, gives us more flexibility and comes without boilerplate.","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"We define a new NLPModel structure simply as:","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"struct HS15Model <: NLPModels.AbstractNLPModel{Float64,Vector{Float64}}\n    meta::NLPModels.NLPModelMeta{Float64, Vector{Float64}}\n    counters::NLPModels.Counters\nend\n\nfunction HS15Model(x0)\n    return HS15Model(\n        NLPModels.NLPModelMeta(\n            2,     #nvar\n            ncon = 2,\n            nnzj = 4,\n            nnzh = 3,\n            x0 = x0,\n            y0 = zeros(2),\n            lvar = [-Inf, -Inf],\n            uvar = [0.5, Inf],\n            lcon = [1.0, 0.0],\n            ucon = [Inf, Inf],\n            minimize = true\n        ),\n        NLPModels.Counters()\n    )\nend","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"This structure takes as input the initial position x0 and generates a AbstractNLPModel. NLPModelMeta stores the information about the structure of the problem (variables and constraints' lower and upper bounds, number of variables, number of constraints, ...). Counters is just a utility storing the number of time each callbacks is being called.","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"The objective function takes as input a HS15Model instance and a vector with dimension 2 storing the current values for x_1 and x_2:","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"function NLPModels.obj(nlp::HS15Model, x::AbstractVector)\n    return 100.0 * (x[2] - x[1]^2)^2 + (1.0 - x[1])^2\nend","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"The corresponding gradient writes (note that we update the values of the gradient g inplace):","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"function NLPModels.grad!(nlp::HS15Model, x::AbstractVector, g::AbstractVector)\n    z = x[2] - x[1]^2\n    g[1] = -400.0 * z * x[1] - 2.0 * (1.0 - x[1])\n    g[2] = 200.0 * z\n    return g\nend","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"Similarly, we define the constraints","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"function NLPModels.cons!(nlp::HS15Model, x::AbstractVector, c::AbstractVector)\n    c[1] = x[1] * x[2]\n    c[2] = x[1] + x[2]^2\n    return c\nend","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"and the associated Jacobian","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"function NLPModels.jac_structure!(nlp::HS15Model, I::AbstractVector{T}, J::AbstractVector{T}) where T\n    copyto!(I, [1, 1, 2, 2])\n    copyto!(J, [1, 2, 1, 2])\nend\n\nfunction NLPModels.jac_coord!(nlp::HS15Model, x::AbstractVector, J::AbstractVector)\n    J[1] = x[2]    # (1, 1)\n    J[2] = x[1]    # (1, 2)\n    J[3] = 1.0     # (2, 1)\n    J[4] = 2*x[2]  # (2, 2)\n    return J\nend","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"note: Note\nAs the Jacobian is sparse, we have to provide its sparsity structure.","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"It remains to implement the Hessian of the Lagrangian for a HS15Model. The Lagrangian of the problem writes","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"L(x_1 x_2 y_1 y_2) = 100 times (x_2 - x_1^2)^2 +(1 - x_1)^2\n+ y_1 times (x_1 times x_2) + y_2 times (x_1 + x_2^2)","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"and we aim at evaluating its second-order derivative nabla^2_xxL(x_1 x_2 y_1 y_2).","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"We first have to define the sparsity structure of the Hessian, which is assumed to be sparse. The Hessian is a symmetric matrix, and by convention we pass only the lower-triangular part of the matrix to the solver. Hence, we define the sparsity structure as","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"function NLPModels.hess_structure!(nlp::HS15Model, I::AbstractVector{T}, J::AbstractVector{T}) where T\n    copyto!(I, [1, 2, 2])\n    copyto!(J, [1, 1, 2])\nend","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"Now that the sparsity structure is defined, the associated Hessian writes down:","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"function NLPModels.hess_coord!(nlp::HS15Model, x, y, H::AbstractVector; obj_weight=1.0)\n    # Objective\n    H[1] = obj_weight * (-400.0 * x[2] + 1200.0 * x[1]^2 + 2.0)\n    H[2] = obj_weight * (-400.0 * x[1])\n    H[3] = obj_weight * 200.0\n    # First constraint\n    H[2] += y[1] * 1.0\n    # Second constraint\n    H[3] += y[2] * 2.0\n    return H\nend\n","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"Once the problem specified in NLPModels's syntax, we can create a new MadNLP instance and solve it:","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"x0 = zeros(2) # initial position\nnlp = HS15Model(x0)\nsolver = MadNLP.MadNLPSolver(nlp; print_level=MadNLP.INFO)\nresults = MadNLP.solve!(solver)","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"MadNLP converges in 19 iterations to a (local) optimal solution. MadNLP returns a MadNLPExecutionStats storing all the results. We can query the primal and the dual solutions respectively by","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"results.solution","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"and","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"results.multipliers","category":"page"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"CurrentModule = MadNLP","category":"page"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"using SparseArrays\nusing NLPModels\nusing MadNLP\nusing MadNLPTests\n# Build nonlinear model\nnlp = MadNLPTests.HS15Model()\n# Build KKT\ncb = MadNLP.create_callback(\n    MadNLP.SparseCallback,\n    nlp,\n)\nind_cons = MadNLP.get_index_constraints(nlp)\nlinear_solver = LapackCPUSolver\nkkt = MadNLP.create_kkt_system(\n    MadNLP.SparseKKTSystem,\n    cb,\n    ind_cons,\n    linear_solver,\n)\n\nn = NLPModels.get_nvar(nlp)\nm = NLPModels.get_ncon(nlp)\nx = NLPModels.get_x0(nlp)\nl = zeros(m)\nhess_values = MadNLP.get_hessian(kkt)\nNLPModels.hess_coord!(nlp, x, l, hess_values)\n\njac_values = MadNLP.get_jacobian(kkt)\nNLPModels.jac_coord!(nlp, x, jac_values)\nMadNLP.compress_jacobian!(kkt)\n\nMadNLP.build_kkt!(kkt)\n","category":"page"},{"location":"man/linear_solvers/#Linear-solvers","page":"Linear Solvers","title":"Linear solvers","text":"","category":"section"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"We suppose that the KKT system has been assembled previously into a given AbstractKKTSystem. Then, it remains to compute the Newton step by solving the KKT system for a given right-hand-side (given as a AbstractKKTVector). That's exactly the role of the linear solver.","category":"page"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"If we do not assume any structure, the KKT system writes in generic form","category":"page"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"K x = b","category":"page"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"with K the KKT matrix and b the current right-hand-side. MadNLP provides a suite of specialized linear solvers to solve the linear system.","category":"page"},{"location":"man/linear_solvers/#Inertia-detection","page":"Linear Solvers","title":"Inertia detection","text":"","category":"section"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"If the matrix K has negative eigenvalues, we have no guarantee that the solution of the KKT system is a descent direction with regards to the original nonlinear problem. That's the reason why most of the linear solvers compute the inertia of the linear system when factorizing the matrix K. The inertia counts the number of positive, negative and zero eigenvalues in the matrix. If the inertia does not meet a given criteria, then the matrix K is regularized by adding a multiple of the identity to it: K_r = K + alpha I.","category":"page"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"note: Note\nWe recall that the inertia of a matrix K is given as a triplet (nmp), with n the number of positive eigenvalues, m the number of negative eigenvalues and p the number of zero eigenvalues.","category":"page"},{"location":"man/linear_solvers/#Factorization-algorithm","page":"Linear Solvers","title":"Factorization algorithm","text":"","category":"section"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"In nonlinear programming, it is common to employ a LBL factorization to decompose the symmetric indefinite matrix K, as this algorithm returns the inertia of the matrix directly as a result of the factorization.","category":"page"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"note: Note\nWhen MadNLP runs in inertia-free mode, the algorithm does not require to compute the inertia when factorizing the matrix K. In that case, MadNLP can use a classical LU or QR factorization to solve the linear system Kx = b.","category":"page"},{"location":"man/linear_solvers/#Solving-a-KKT-system-with-MadNLP","page":"Linear Solvers","title":"Solving a KKT system with MadNLP","text":"","category":"section"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"We suppose available a AbstractKKTSystem kkt, properly assembled following the procedure presented previously. We can query the assembled matrix K as","category":"page"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"K = MadNLP.get_kkt(kkt)\n","category":"page"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"Then, if we want to pass the KKT matrix K to Lapack, this translates to","category":"page"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"linear_solver = LapackCPUSolver(K)\n","category":"page"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"The instance linear_solver does not copy the matrix K and instead keep a reference to it.","category":"page"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"linear_solver.A === K","category":"page"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"That way every time we re-assemble the matrix K in kkt, the values are directly updated inside linear_solver.","category":"page"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"To compute the factorization inside linear_solver, one simply as to call:","category":"page"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"MadNLP.factorize!(linear_solver)\n","category":"page"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"Once the factorization computed, computing the backsolve for a right-hand-side b amounts to","category":"page"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"nk = size(kkt, 1)\nb = rand(nk)\nMadNLP.solve!(linear_solver, b)","category":"page"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"The values of b being modified inplace to store the solution x of the linear system Kx =b.","category":"page"},{"location":"options/#MadNLP-Options","page":"Options","title":"MadNLP Options","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"Pages = [\n    \"options.md\",\n]\nDepth=3","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"","category":"page"},{"location":"options/#Primary-options","page":"Options","title":"Primary options","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"These options are used to set the values for other options. The default values are inferred from the NLP model.","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"tol::Float64\n  Termination tolerance. The default value is 1e-8 for double precision. The solver terminates if the scaled primal, dual, complementary infeasibility is less than tol. Valid range is (0infty).\ncallback::Type \t Valid values are: MadNLP.{DenseCallback,SparseCallback}.\nkkt_system::Type  The type of KKT system. Valid values are MadNLP.{SpasreKKTSystem,SparseUnreducedKKTSystem,SparseCondensedKKTSystem,DenseKKTSystem,DenseCondensedKKTSystem}.\nlinear_solver::Type\nLinear solver used for solving primal-dual system. Valid values are: {MadNLP.UmfpackSolver,MadNLP.LDLSolver,MadNLP.CHOLMODSolver, MadNLP.MumpsSolver, MadNLP.PardisoSolver, MadNLP.PardisoMKLSolver, MadNLP.Ma27Solver, MadNLP.Ma57Solver, MadNLP.Ma77Solver, MadNLP.Ma86Solver, MadNLP.Ma97Solver, MadNLP.LapackCPUSolver, MadNLPGPU.LapackGPUSolver,MadNLPGPU.RFSolver,MadNLPGPU.GLUSolver,MadNLPGPU.CuCholeskySolver,MadNLPGPU.CUDSSSolver} (some may require using extension packages). The selected solver should be properly built in the build procedure. See README.md file.","category":"page"},{"location":"options/#General-options","page":"Options","title":"General options","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"iterator::Type = RichardsonIterator\n  Iterator used for iterative refinement. Valid values are: {MadNLPRichardson,MadNLPKrylov}.\nRichardson uses Richardson iteration\nKrylov uses restarted Generalized Minimal Residual method implemented in IterativeSolvers.jl.\nblas_num_threads::Int = 1\n  Number of threads used for BLAS routines. Valid range is 1infty).\ndisable_garbage_collector::Bool = false\n  If true, Julia garbage collector is temporarily disabled while solving the problem, and then enabled back once the solution is complete.\nrethrow_error::Bool = true\n  If false, any internal error thrown by MadNLP and interruption exception (triggered by the user via ^C) is caught, and not rethrown. If an error is caught, the solver terminates with an error message.","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"","category":"page"},{"location":"options/#Output-options","page":"Options","title":"Output options","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"print_level::LogLevels = INFO\n  stdout print level. Any message with level less than print_level is not printed on stdout. Valid values are: MadNLP.{TRACE, DEBUG, INFO, NOTICE, WARN, ERROR}.\noutput_file::String = INFO\n  If not \"\", the output log is teed to the file at the path specified in output_file.\nfile_print_level::LogLevels = TRACE\n  File print level; any message with level less than file_print_level is not printed on the file specified in output_file. Valid values are: MadNLP.{TRACE, DEBUG, INFO, NOTICE, WARN, ERROR}.","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"","category":"page"},{"location":"options/#Termination-options","page":"Options","title":"Termination options","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"max_iter::Int = 3000\n  Maximum number of interior point iterations. The solver terminates with exit symbol :Maximum_Iterations_Exceeded if the interior point iteration count exceeds max_iter.\nacceptable_tol::Float64 = 1e-6\n  Acceptable tolerance. The solver terminates if the scaled primal, dual, complementary infeasibility is less than acceptable_tol, for acceptable_iter consecutive interior point iteration steps.\nacceptable_iter::Int = 15\n  Acceptable iteration tolerance. Valid rage is 1infty).\ndiverging_iterates_tol::Float64 = 1e20\n  Diverging iteration tolerance. The solver terminates with exit symbol :Diverging_Iterates if the NLP error is greater than diverging_iterates_tol.\nmax_wall_time::Float64 = 1e6\n  Maximum wall time for interior point solver. The solver terminates with exit symbol :Maximum_WallTime_Exceeded if the total solver wall time exceeds max_wall_time.\ns_max::Float64 = 100.","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"","category":"page"},{"location":"options/#NLP-options","page":"Options","title":"NLP options","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"kappa_d::Float64 = 1e-5\nfixed_variable_treatment::FixedVariableTreatments = MakeParameter\n  Valid values are: MadNLP.{RelaxBound,MakeParameter}.\nequality_treatment::FixedVariableTreatments = MakeParameter\n  Valid values are: MadNLP.{RelaxEquality,EnforceEquality}.\njacobian_constant::Bool = false\n  If true, constraint Jacobian is only evaluated once and reused.\nhessian_constant::Bool = false\n  If true, Lagrangian Hessian is only evaluated once and reused.\nbound_push::Float64 = 1e-2\nbound_fac::Float64 = 1e-2\nhessian_approximation::Type = ExactHessian\nquasi_newton_options::QuasiNewtonOptions = QuasiNewtonOptions()\ninertia_correction_method::InertiaCorrectionMethods = INERTIA_AUTO\n  Valid values are: MadNLP.{INERTIA_AUTO,INERTIA_BASED, INERTIA_FREE}.\nINERTIA_BASED uses the strategy in Ipopt.\nINERTIA_FREE uses the strategy in Chiang (2016).\nINERTIA_AUTO uses INERTIA_BASED if inertia information is available and uses INERTIA_FREE otherwise.\ninertia_free_tol::Float64 = 0.","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"","category":"page"},{"location":"options/#Initialization-Options","page":"Options","title":"Initialization Options","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"dual_initialized::Bool = false\ndual_initialization_method::Type = kkt_system <: MadNLP.SparseCondensedKKTSystem ? DualInitializeSetZero : DualInitializeLeastSquares","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"constr_mult_init_max::Float64 = 1e3\nnlp_scaling::Bool = true: \n  If true, MadNLP scales the nonlinear problem during the resolution.\nnlp_scaling_max_gradient::Float64 = 100.","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"","category":"page"},{"location":"options/#Hessian-perturbation-options","page":"Options","title":"Hessian perturbation options","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"min_hessian_perturbation::Float64 = 1e-20\nfirst_hessian_perturbation::Float64 = 1e-4\nmax_hessian_perturbation::Float64 = 1e20\nperturb_inc_fact_first::Float64 = 1e2\nperturb_inc_fact::Float64 = 8.\nperturb_dec_fact::Float64 = 1/3\njacobian_regularization_exponent::Float64 = 1/4\njacobian_regularization_value::Float64 = 1e-8","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"","category":"page"},{"location":"options/#Restoration-options","page":"Options","title":"Restoration options","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"soft_resto_pderror_reduction_factor::Float64 = 0.9999\nrequired_infeasibility_reduction::Float64 = 0.9","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"","category":"page"},{"location":"options/#Line-search-options","page":"Options","title":"Line-search options","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"obj_max_inc::Float64 = 5.\nkappha_soc::Float64 = 0.99\nmax_soc::Int = 4\nalpha_min_frac::Float64 = 0.05\ns_theta::Float64 = 1.1\ns_phi::Float64 = 2.3\neta_phi::Float64 = 1e-4\nkappa_soc::Float64 = 0.99\ngamma_theta::Float64 = 1e-5\ngamma_phi::Float64 = 1e-5\ndelta::Float64 = 1\nkappa_sigma::Float64 = 1e10\nbarrier_tol_factor::Float64 = 10.\nrho::Float64 = 1000.","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"","category":"page"},{"location":"options/#Barrier-options","page":"Options","title":"Barrier options","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"mu_init::Float64 = 1e-1\nmu_min::Float64 = 1e-9\nmu_superlinear_decrease_power::Float64 = 1.5\ntau_min::Float64 = 0.99\nmu_linear_decrease_factor::Float64 = .2","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"","category":"page"},{"location":"options/#Linear-Solver-Options","page":"Options","title":"Linear Solver Options","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"Linear solver options are specific to the linear solver chosen at linear_solver option. Irrelevant options are ignored and a warning message is printed.","category":"page"},{"location":"options/#Ma27-(requires-MadNLPHSL)","page":"Options","title":"Ma27 (requires MadNLPHSL)","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"ma27_pivtol::Float64 = 1e-8\nma27_pivtolmax::Float64 = 1e-4\nma27_liw_init_factor::Float64 = 5.\nma27_la_init_factor::Float64 = 5.\nma27_meminc_factor::Float64 = 2.","category":"page"},{"location":"options/#Ma57-(requires-MadNLPHSL)","page":"Options","title":"Ma57 (requires MadNLPHSL)","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"ma57_pivtol::Float64 = 1e-8\nma57_pivtolmax::Float64 = 1e-4\nma57_pre_alloc::Float64 = 1.05\nma57_pivot_order::Int = 5\nma57_automatic_scaling::Bool = false\nma57_block_size::Int = 16\nma57_node_amalgamation::Int = 16\nma57_small_pivot_flag::Int = 0","category":"page"},{"location":"options/#Ma77-(requires-MadNLPHSL)","page":"Options","title":"Ma77 (requires MadNLPHSL)","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"ma77_buffer_lpage::Int = 4096\nma77_buffer_npage::Int = 1600\nma77_file_size::Int = 2097152\nma77_maxstore::Int = 0\nma77_nemin::Int = 8\nma77_order::Ma77.Ordering = Ma77.METIS\nma77_print_level::Int = -1\nma77_small::Float64 = 1e-20\nma77_static::Float64 = 0.\nma77_u::Float64 = 1e-8\nma77_umax::Float64 = 1e-4","category":"page"},{"location":"options/#Ma86-(requires-MadNLPHSL)","page":"Options","title":"Ma86 (requires MadNLPHSL)","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"ma86_num_threads::Int = 1\nma86_print_level::Float64 = -1\nma86_nemin::Int = 32\nma86_order::Ma86.Ordering = Ma86.METIS\nma86_scaling::Ma86.Scaling = Ma86.SCALING_NONE\nma86_small::Float64 = 1e-20\nma86_static::Float64 = 0.\nma86_u::Float64 = 1e-8\nma86_umax::Float64 = 1e-4","category":"page"},{"location":"options/#Ma97-(requires-MadNLPHSL)","page":"Options","title":"Ma97 (requires MadNLPHSL)","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"ma97_num_threads::Int = 1\nma97_print_level::Int = -1\nma97_nemin::Int = 8\nma97_order::Ma97.Ordering = Ma97.METIS\nma97_scaling::Ma97.Scaling = Ma97.SCALING_NONE\nma97_small::Float64 = 1e-20\nma97_u::Float64 = 1e-8\nma97_umax::Float64 = 1e-4","category":"page"},{"location":"options/#Mumps-(requires-MadNLPMumps)","page":"Options","title":"Mumps (requires MadNLPMumps)","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"mumps_dep_tol::Float64 = 0.\nmumps_mem_percent::Int = 1000\nmumps_permuting_scaling::Int = 7\nmumps_pivot_order::Int = 7\nmumps_pivtol::Float64 = 1e-6\nmumps_pivtolmax::Float64 = .1\nmumps_scaling::Int = 77","category":"page"},{"location":"options/#Umfpack-(requires-MadNLPUmfpack)","page":"Options","title":"Umfpack (requires MadNLPUmfpack)","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"umfpack_pivtol::Float64 = 1e-4\numfpack_pivtolmax::Float64 = 1e-1\numfpack_sym_pivtol::Float64 = 1e-3\numfpack_block_size::Float64 = 16\numfpack_strategy::Float64 = 2.","category":"page"},{"location":"options/#Pardiso-(requires-MadNLPPardiso)","page":"Options","title":"Pardiso (requires MadNLPPardiso)","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"pardiso_matching_strategy::Pardiso.MatchingStrategy = COMPLETE2x2\npardiso_max_inner_refinement_steps::Int = 1\npardiso_msglvl::Int = 0\npardiso_order::Int = 2","category":"page"},{"location":"options/#PardisoMKL","page":"Options","title":"PardisoMKL","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"pardisomkl_num_threads::Int = 1\npardiso_matching_strategy::PardisoMKL.MatchingStrategy = COMPLETE2x2\npardisomkl_max_iterative_refinement_steps::Int = 1\npardisomkl_msglvl::Int = 0\npardisomkl_order::Int = 2","category":"page"},{"location":"options/#LapackGPU-(requires-MadNLPGPU)","page":"Options","title":"LapackGPU (requires MadNLPGPU)","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"lapackgpu_algorithm::LapackGPU.Algorithms = BUNCHKAUFMAN","category":"page"},{"location":"options/#LapackCPU","page":"Options","title":"LapackCPU","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"lapackcpu_algorithm::LapackCPU.Algorithms = BUNCHKAUFMAN","category":"page"},{"location":"options/#Iterator-Options","page":"Options","title":"Iterator Options","text":"","category":"section"},{"location":"options/#Richardson","page":"Options","title":"Richardson","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"richardson_max_iter::Int = 10 \n  Maximum number of Richardson iteration steps. Valid range is 1infty).\nrichardson_tol::Float64 = 1e-10 \n  Convergence tolerance of Richardson iteration. Valid range is (0infty).\nrichardson_acceptable_tol::Float64 = 1e-5 \n  Acceptable convergence tolerance of Richardson iteration. If the Richardson iteration counter exceeds richardson_max_iter without satisfying the convergence criteria set with richardson_tol, the Richardson solver checks whether the acceptable convergence criteria set with richardson_acceptable_tol is satisfied; if the acceptable convergence criteria is satisfied, the computed step is used; otherwise, the augmented system is treated to be singular. Valid range is (0infty).","category":"page"},{"location":"options/#Krylov-(requires-MadNLPIterative)","page":"Options","title":"Krylov (requires MadNLPIterative)","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"krylov_max_iter::Int = 10 \n  Maximum number of Krylov iteration steps. Valid range is 1infty).\nkrylov_tol::Float64 = 1e-10 \n  Convergence tolerance of Krylov iteration. Valid range is (0infty).\nkrylov_acceptable_tol::Float64 = 1e-5 \n  Acceptable convergence tolerance of Krylov iteration. If the Krylov iteration counter exceeds krylov_max_iter without satisfying the convergence criteria set with krylov_tol, the Krylov solver checks whether the acceptable convergence criteria set with krylov_acceptable_tol is satisfied; if the acceptable convergence criteria is satisfied, the computed step is used; otherwise, the augmented system is treated to be singular. Valid range is (0infty).\nkrylov_restart::Int = 5 \n  Maximum Krylov iteration before restarting. Valid range is 1infty).","category":"page"},{"location":"options/#Reference","page":"Options","title":"Reference","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"[Bunch, 1977]: J R Bunch and L Kaufman, Some stable methods for calculating inertia and solving symmetric linear systems, Mathematics of Computation 31:137 (1977), 163-179.","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"[Greif, 2014]: Greif, Chen, Erin Moulding, and Dominique Orban. \"Bounds on eigenvalues of matrices arising from interior-point methods.\" SIAM Journal on Optimization 24.1 (2014): 49-83.","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"[Wächter, 2006]: Wächter, Andreas, and Lorenz T. Biegler. \"On the implementation of an interior-point filter line-search algorithm for large-scale nonlinear programming.\" Mathematical programming 106.1 (2006): 25-57.","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"[Chiang, 2016]: Chiang, Nai-Yuan, and Victor M. Zavala. \"An inertia-free filter line-search algorithm for large-scale nonlinear programming.\" Computational Optimization and Applications 64.2 (2016): 327-354.","category":"page"},{"location":"lib/callbacks/","page":"Callback wrappers","title":"Callback wrappers","text":"CurrentModule = MadNLP","category":"page"},{"location":"lib/callbacks/#Callbacks","page":"Callback wrappers","title":"Callbacks","text":"","category":"section"},{"location":"lib/callbacks/","page":"Callback wrappers","title":"Callback wrappers","text":"In MadNLP, a nonlinear program is implemented with a given AbstractNLPModel. The model may have a form unsuitable for the interior-point algorithm. For that reason, MadNLP wraps the AbstractNLPModel internally using custom data structures, encoded as a AbstractCallback. Depending on the setting, choose to wrap the AbstractNLPModel as a DenseCallback or alternatively, as a SparseCallback.","category":"page"},{"location":"lib/callbacks/","page":"Callback wrappers","title":"Callback wrappers","text":"AbstractCallback\nDenseCallback\nSparseCallback\n","category":"page"},{"location":"lib/callbacks/#MadNLP.AbstractCallback","page":"Callback wrappers","title":"MadNLP.AbstractCallback","text":"AbstractCallback{T, VT}\n\nWrap the AbstractNLPModel passed by the user in a form amenable to MadNLP.\n\nAn AbstractCallback handles the scaling of the problem and the reformulations of the equality constraints and fixed variables.\n\n\n\n\n\n","category":"type"},{"location":"lib/callbacks/#MadNLP.DenseCallback","page":"Callback wrappers","title":"MadNLP.DenseCallback","text":"DenseCallback{T, VT} < AbstractCallback{T, VT}\n\nWrap an AbstractNLPModel using dense structures.\n\n\n\n\n\n","category":"type"},{"location":"lib/callbacks/#MadNLP.SparseCallback","page":"Callback wrappers","title":"MadNLP.SparseCallback","text":"SparseCallback{T, VT} < AbstractCallback{T, VT}\n\nWrap an AbstractNLPModel using sparse structures.\n\n\n\n\n\n","category":"type"},{"location":"lib/callbacks/","page":"Callback wrappers","title":"Callback wrappers","text":"The function create_callback allows to instantiate a AbstractCallback from a given NLPModel:","category":"page"},{"location":"lib/callbacks/","page":"Callback wrappers","title":"Callback wrappers","text":"create_callback\n","category":"page"},{"location":"lib/callbacks/#MadNLP.create_callback","page":"Callback wrappers","title":"MadNLP.create_callback","text":"create_callback(\n    ::Type{Callback},\n    nlp::AbstractNLPModel{T, VT};\n    fixed_variable_treatment=MakeParameter,\n    equality_treatment=EnforceEquality,\n) where {T, VT}\n\nWrap the nonlinear program nlp using the callback wrapper with type Callback. The option fixed_variable_treatment decides if the fixed variables are relaxed (RelaxBound) or removed (MakeParameter). The option equality_treatment decides if the the equality constraints are keep as is (EnforceEquality) or relaxed (RelaxEquality).\n\n\n\n\n\n","category":"function"},{"location":"lib/callbacks/","page":"Callback wrappers","title":"Callback wrappers","text":"Internally, a AbstractCallback reformulates the inequality constraints as equality constraints by introducing additional slack variables. The fixed variables are reformulated as parameters (using MakeParameter) or are relaxed (using RelaxBound). The equality constraints can be keep as is with EnforceEquality (default option) or relaxed as inequality constraints with RelaxEquality. In that later case, MadNLP solves a relaxation of the original problem.","category":"page"},{"location":"lib/callbacks/","page":"Callback wrappers","title":"Callback wrappers","text":"AbstractFixedVariableTreatment\nMakeParameter\nRelaxBound\n\nAbstractEqualityTreatment\nEnforceEquality\nRelaxEquality","category":"page"},{"location":"lib/callbacks/#MadNLP.AbstractFixedVariableTreatment","page":"Callback wrappers","title":"MadNLP.AbstractFixedVariableTreatment","text":"AbstractFixedVariableTreatment\n\nAbstract type to define the reformulation of the fixed variables inside MadNLP.\n\n\n\n\n\n","category":"type"},{"location":"lib/callbacks/#MadNLP.MakeParameter","page":"Callback wrappers","title":"MadNLP.MakeParameter","text":"MakeParameter{VT, VI} <: AbstractFixedVariableTreatment\n\nRemove the fixed variables from the optimization variables and define them as problem's parameters.\n\n\n\n\n\n","category":"type"},{"location":"lib/callbacks/#MadNLP.RelaxBound","page":"Callback wrappers","title":"MadNLP.RelaxBound","text":"RelaxBound <: AbstractFixedVariableTreatment\n\nRelax the fixed variables x = x_fixed as bounded variables x_fixed - ϵ  x  x_fixed + ϵ, with ϵ a small-enough parameter.\n\n\n\n\n\n","category":"type"},{"location":"lib/callbacks/#MadNLP.AbstractEqualityTreatment","page":"Callback wrappers","title":"MadNLP.AbstractEqualityTreatment","text":"AbstractEqualityTreatment\n\nAbstract type to define the reformulation of the equality constraints inside MadNLP.\n\n\n\n\n\n","category":"type"},{"location":"lib/callbacks/#MadNLP.EnforceEquality","page":"Callback wrappers","title":"MadNLP.EnforceEquality","text":"EnforceEquality <: AbstractEqualityTreatment\n\nKeep the equality constraints intact.\n\nThe solution returned by MadNLP will respect the equality constraints.\n\n\n\n\n\n","category":"type"},{"location":"lib/callbacks/#MadNLP.RelaxEquality","page":"Callback wrappers","title":"MadNLP.RelaxEquality","text":"RelaxEquality <: AbstractEqualityTreatment\n\nRelax the equality constraints g(x) = 0 with two inequality constraints, as -ϵ  g(x)  ϵ. The parameter ϵ is usually small.\n\nThe solution returned by MadNLP will satisfy the equality constraints only up to a tolerance ϵ.\n\n\n\n\n\n","category":"type"},{"location":"lib/callbacks/","page":"Callback wrappers","title":"Callback wrappers","text":"MadNLP has to keep in memory all the indexes associated to the equality and inequality constraints. Similarly, MadNLP has to keep track of the indexes of the bounded variables and the fixed variables. MadNLP provides a utility get_index_constraints to import all the indexes required by MadNLP. Each index vector is encoded as a Vector{Int}.","category":"page"},{"location":"lib/callbacks/","page":"Callback wrappers","title":"Callback wrappers","text":"get_index_constraints\n","category":"page"},{"location":"lib/callbacks/#MadNLP.get_index_constraints","page":"Callback wrappers","title":"MadNLP.get_index_constraints","text":"get_index_constraints(nlp::AbstractNLPModel)\n\nAnalyze the bounds of the variables and the constraints in the AbstractNLPModel nlp. Return a named-tuple witht the following keys:return (\n\nind_eq: indices of equality constraints.\nind_ineq: indices of inequality constraints.\nind_fixed: indices of fixed variables.\nind_lb: indices of variables with a lower-bound.\nind_ub: indices of variables with an upper-bound.\nind_llb: indices of variables with only a lower-bound.\nind_uub: indices of variables with only an upper-bound.\n\n\n\n\n\n","category":"function"},{"location":"","page":"Home","title":"Home","text":"(Image: Logo)","category":"page"},{"location":"","page":"Home","title":"Home","text":"MadNLP is an open-source nonlinear programming solver, purely implemented in Julia. MadNLP implements a filter line-search interior-point algorithm, as used in Ipopt. MadNLP seeks to streamline the development of modeling and algorithmic paradigms in order to exploit structures and to make efficient use of high-performance computers.","category":"page"},{"location":"#Design","page":"Home","title":"Design","text":"","category":"section"},{"location":"#MadNLP's-problem-structure","page":"Home","title":"MadNLP's problem structure","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"MadNLP targets the resolution of constrained nonlinear problems, formulating as","category":"page"},{"location":"","page":"Home","title":"Home","text":"  beginaligned\n    min_x   f(x) \n    textsubject to quad  g_ell leq g(x) leq g_u \n                             x_ell leq x leq x_u\n  endaligned","category":"page"},{"location":"","page":"Home","title":"Home","text":"where x in mathbbR^n is the decision variable, f mathbbR^n to mathbbR and g mathbbR^n to mathbbR^m two nonlinear functions. MadNLP makes the distinction between the bound constraints x_ell leq x leq x_u and the generic constraints g_ell leq g(x) leq g_u. No other structure is assumed a priori.","category":"page"},{"location":"","page":"Home","title":"Home","text":"MadNLP is built on top of NLPModels.jl, a generic package to represent optimization models in Julia. In addition, MadNLP is interfaced with JuMP and Plasmo.","category":"page"},{"location":"","page":"Home","title":"Home","text":"note: Note\nMadNLP requires the evaluation of both the first-order and the second-order derivatives of the nonlinear problem.","category":"page"},{"location":"#MadNLP's-algorithm","page":"Home","title":"MadNLP's algorithm","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"MadNLP implements a primal-dual filter line-search interior-point algorithm, closely related to Ipopt. The nonlinear problem is reformulated in standard form by introducing a slack variables s in mathbbR^m to rewrite all the inequality constraints as equality constraints:","category":"page"},{"location":"","page":"Home","title":"Home","text":"  beginaligned\n    min_x s   f(x) \n    textsubject to quad  g(x) - s = 0  \n                             g_ell leq s leq g_u \n                             x_ell leq x leq x_u\n  endaligned","category":"page"},{"location":"","page":"Home","title":"Home","text":"The algorithm starts from an initial primal-dual iterate (x_0 s_0 y_0). Then, at each iteration the current iterate is updated by solving the KKT linear system:","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginbmatrix\n    W_k + Sigma_x  0  A_k^top \n    0  Sigma_s  - I \n    A_k  -I  0\nendbmatrix\nbeginbmatrix\n    Delta x  Delta s  Delta y\nendbmatrix\n=\n-\nbeginbmatrix\n    nabla f(x_k) + A_k^top y_k - mu U^-1 e_n \n    y_k - mu S^-1 e_n \n    g(x_k) - s_k\nendbmatrix\n","category":"page"},{"location":"","page":"Home","title":"Home","text":"with mu being the current barrier parameter.","category":"page"},{"location":"","page":"Home","title":"Home","text":"We call the linear system the augmented KKT system at iteration k.","category":"page"},{"location":"#MadNLP's-performance","page":"Home","title":"MadNLP's performance","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"In any interior-point algorithm, the two computational bottlenecks are","category":"page"},{"location":"","page":"Home","title":"Home","text":"The evaluation of the first- and second-order derivatives.\nThe factorization of the augmented KKT system.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The first point is problem dependent, and often related to the automatic differentiation backend being employed. The second point is more problematic, as the augmented KKT system is usually symmetric indefinite and ill-conditionned. For that reason we recommend using efficient sparse-linear solvers (HSL, Mumps, Pardiso) if they are available to the user.","category":"page"},{"location":"#Citing-MadNLP.jl","page":"Home","title":"Citing MadNLP.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you use MadNLP.jl in your research, we would greatly appreciate your citing it.","category":"page"},{"location":"","page":"Home","title":"Home","text":"@article{shin2023accelerating,\n  title={Accelerating optimal power flow with {GPU}s: {SIMD} abstraction of nonlinear programs and condensed-space interior-point methods},\n  author={Shin, Sungho and Pacaud, Fran{\\c{c}}ois and Anitescu, Mihai},\n  journal={arXiv preprint arXiv:2307.16830},\n  year={2023}\n}\n@article{shin2020graph,\n  title={Graph-Based Modeling and Decomposition of Energy Infrastructures},\n  author={Shin, Sungho and Coffrin, Carleton and Sundar, Kaarthik and Zavala, Victor M},\n  journal={arXiv preprint arXiv:2010.02404},\n  year={2020}\n}","category":"page"}]
}
