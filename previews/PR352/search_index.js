var documenterSearchIndex = {"docs":
[{"location":"man/solver/","page":"IPM solver","title":"IPM solver","text":"MadNLP is an interior-point solver based on a filter line-search. We detail here the inner machinery happening at each MadNLP's iteration.","category":"page"},{"location":"man/solver/","page":"IPM solver","title":"IPM solver","text":"We recall that MadNLP is a primal-dual interior-point method and starts from an initial primal-dual variables (x_0 s_0 y_0).","category":"page"},{"location":"man/solver/#What-happens-at-iteration-k?","page":"IPM solver","title":"What happens at iteration k?","text":"","category":"section"},{"location":"man/solver/","page":"IPM solver","title":"IPM solver","text":"At iteration k, MadNLP aims at finding a new iterate (x_k+1 s_k+1 y_k+1) improving the current iterate (x_k s_k y_k), in the sense that the new iterate (i) improves the objective or (ii) decreases the infeasibility. The exact trade-off between (i) and (ii) is handled by the filter line-search.","category":"page"},{"location":"man/solver/","page":"IPM solver","title":"IPM solver","text":"The algorithm follows the steps:","category":"page"},{"location":"man/solver/","page":"IPM solver","title":"IPM solver","text":"Check problem convergence E_0(x_k s_k y_k)  varepsilon_tol\nIf necessary, update the barrier parameter mu_k\nEvaluate the Hessian of the Lagrangian W_k and the Jacobian A_k with the callbacks\nAssemble the KKT system and compute the search direction d_k by solving the resulting linear system.\nIf necessary, regularize the KKT system to guarantee that d_k is a descent direction\nRun the backtracking line-search and find a step alpha_k\nDefine the next iterate as x_k+1 = x_k + alpha_k d_k^x, s_k+1 = s_k + alpha_k d_k^s, y_k+1 = y_k + alpha_k d_k^y.","category":"page"},{"location":"man/solver/","page":"IPM solver","title":"IPM solver","text":"We detail each step in the following paragraphs.","category":"page"},{"location":"man/solver/","page":"IPM solver","title":"IPM solver","text":"note: Note\nIn general, Step 3 (Hessian and Jacobian evaluations) and Step 4 (solving the KKT system) are the two most numerically demanding steps.","category":"page"},{"location":"man/solver/#Step-1:-When-the-algorithm-stops?","page":"IPM solver","title":"Step 1: When the algorithm stops?","text":"","category":"section"},{"location":"man/solver/","page":"IPM solver","title":"IPM solver","text":"MadNLP stops once the solution satisfies a specified accuracy varepsilon_tol (by default 10^-8). MadNLP uses the same stopping criterion as Ipopt by defining","category":"page"},{"location":"man/solver/","page":"IPM solver","title":"IPM solver","text":"E_mu(x_k s_k y_k nu_k w_k) =\nmax left\nbeginaligned\n nabla f(x_k) + A_k^top y_k + nu_k + w_k _infty \n g(x_k) - s_k _infty \n X_knu_k - mu e _infty \n S_k w_k - mu e _infty\nendaligned\nright\n","category":"page"},{"location":"man/solver/","page":"IPM solver","title":"IPM solver","text":"and stopping the algorithm whenever","category":"page"},{"location":"man/solver/","page":"IPM solver","title":"IPM solver","text":"E_0(x_k s_k y_k nu_k w_k)  varepsilon_tol\n","category":"page"},{"location":"man/solver/#Step-2:-How-to-update-the-barrier-parameter-\\mu?","page":"IPM solver","title":"Step 2: How to update the barrier parameter mu?","text":"","category":"section"},{"location":"man/solver/","page":"IPM solver","title":"IPM solver","text":"TODO","category":"page"},{"location":"man/solver/#Step-4:-How-do-we-solve-the-KKT-system?","page":"IPM solver","title":"Step 4: How do we solve the KKT system?","text":"","category":"section"},{"location":"man/solver/","page":"IPM solver","title":"IPM solver","text":"Solving the KKT system happens in two substeps:","category":"page"},{"location":"man/solver/","page":"IPM solver","title":"IPM solver","text":"Assembling the KKT matrix K.\nSolve the system Kx = b with a linear solver.","category":"page"},{"location":"man/solver/","page":"IPM solver","title":"IPM solver","text":"In substep 1, MadNLP reads the Hessian and the Jacobian computed at Step 3 and build the associated KKT system in an AbstractKKTSystem. As a result, we get a matrix K_k encoding the KKT system at iteration k.","category":"page"},{"location":"man/solver/","page":"IPM solver","title":"IPM solver","text":"In substep 2, the matrix K_k is factorized by a compatible linear solver. Then a solution x is returned by applying a backsolve.","category":"page"},{"location":"man/solver/#Step-5:-How-to-regularize-the-KKT-system?","page":"IPM solver","title":"Step 5: How to regularize the KKT system?","text":"","category":"section"},{"location":"man/solver/","page":"IPM solver","title":"IPM solver","text":"TODO","category":"page"},{"location":"man/solver/#Step-6:-What-is-a-filter-line-search?","page":"IPM solver","title":"Step 6: What is a filter line-search?","text":"","category":"section"},{"location":"man/solver/","page":"IPM solver","title":"IPM solver","text":"TODO","category":"page"},{"location":"man/solver/#What-happens-if-the-line-search-failed?","page":"IPM solver","title":"What happens if the line-search failed?","text":"","category":"section"},{"location":"man/solver/","page":"IPM solver","title":"IPM solver","text":"If inside the line-search algorithm the step alpha_k becomes negligible (10^-8) then we consider the line-search has failed to find a next iterate along the current direction d_k. If that happens during several consecutive iterations, MadNLP enters into a feasible restoration phase. The goal of feasible restoration is to decrease the primal infeasibility, to move the current iterate closer to the feasible set.","category":"page"},{"location":"installation/#Installation","page":"Installation","title":"Installation","text":"","category":"section"},{"location":"installation/","page":"Installation","title":"Installation","text":"To install MadNLP, simply proceed to","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"pkg> add MadNLP\n","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"note: Note\nThe default installation comes is shipped only with two linear solvers (Umfpack and Lapack), which are not adapted to solve the KKT systems arising in large-scale nonlinear problems. We recommend using a specialized linear solver to speed-up the solution of the KKT systems.","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"In addition to Lapack and Umfpack, the user can install the following extensions to use a specialized linear solver.","category":"page"},{"location":"installation/#HSL-linear-solver","page":"Installation","title":"HSL linear solver","text":"","category":"section"},{"location":"installation/","page":"Installation","title":"Installation","text":"Obtain a license and download HSL_jll.jl from https://licences.stfc.ac.uk/product/julia-hsl. There are two versions available: LBT and OpenBLAS. LBT is the recommended option for Julia >= v1.9. Install this download into your current environment using:","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"import Pkg\nPkg.develop(path = \"/full/path/to/HSL_jll.jl\")","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"If the user has already compiled the HSL solver library, one can simply override the path to the artifact by editing ~/.julia/artifacts/Overrides.toml","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"# replace HSL_jll artifact /usr/local/lib/libhsl.so\necece3e2c69a413a0e935cf52e03a3ad5492e137 = \"/usr/local\"","category":"page"},{"location":"installation/#Mumps-linear-solver","page":"Installation","title":"Mumps linear solver","text":"","category":"section"},{"location":"installation/","page":"Installation","title":"Installation","text":"Mumps is an open-source sparse linear solver, whose binaries are kindly provided as a Julia artifact. Installing Mumps simply amounts to","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"pkg> add MadNLPMumps","category":"page"},{"location":"installation/#Pardiso-linear-solver","page":"Installation","title":"Pardiso linear solver","text":"","category":"section"},{"location":"installation/","page":"Installation","title":"Installation","text":"To use Pardiso, the user needs to obtain the Pardiso shared libraries from https://panua.ch/, provide the absolute path to the shared library:","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"julia> ENV[\"MADNLP_PARDISO_LIBRARY_PATH\"] = \"/usr/lib/libpardiso600-GNU800-X86-64.so\"","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"and place the license file in the home directory. After obtaining the library and the license file, run","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"pkg> build MadNLPPardiso","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"The build process requires a C compiler.","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"CurrentModule = MadNLP","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"using LinearAlgebra\nusing SparseArrays\nusing NLPModels\nusing MadNLP\nusing MadNLPTests\nnlp = MadNLPTests.HS15Model()\n","category":"page"},{"location":"man/kkt/#KKT-systems","page":"KKT systems","title":"KKT systems","text":"","category":"section"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"KKT systems are linear system with a special KKT structure. MadNLP uses a special structure AbstractKKTSystem to represent internally the KKT system. The AbstractKKTSystem fulfills two goals:","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"Store the values of the Hessian of the Lagrangian and of the Jacobian.\nAssemble the corresponding KKT matrix K.","category":"page"},{"location":"man/kkt/#A-brief-look-at-the-math","page":"KKT systems","title":"A brief look at the math","text":"","category":"section"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"We recall that at each iteration the interior-point algorithm aims at solving the following relaxed KKT equations (mu playing the role of a homotopy parameter) with a Newton method:","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"F_mu(x s y v w) = 0","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"with","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"F_mu(x s y v w) =\nleft\nbeginaligned\n     nabla f(x) + A^top y + nu + w  (F_1) \n     - y - w   (F_2) \n     g(x) - s   (F_3) \n     X v - mu e_n  (F_4) \n     S w - mu e_m  (F_5)\nendaligned\nright","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"The Newton step associated to the KKT equations writes","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"overline\nbeginpmatrix\n W  0  A^top  - I  0 \n 0  0  -I  0  -I \n A  -I  0 0  0 \n V  0  0  X  0 \n 0  W  0  0  S\nendpmatrix^K_1\nbeginpmatrix\n    Delta x \n    Delta s \n    Delta y \n    Delta v \n    Delta w\nendpmatrix\n= -\nbeginpmatrix\n    F_1  F_2  F_3  F_4  F_5\nendpmatrix","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"The matrix K_1 is unsymmetric, but we can obtain an equivalent symmetric system by eliminating the two last rows:","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"overline\nbeginpmatrix\n W + Sigma_x  0  A^top \n 0  Sigma_s  -I \n A  -I  0\nendpmatrix^K_2\nbeginpmatrix\n    Delta x \n    Delta s \n    Delta y\nendpmatrix\n= -\nbeginpmatrix\n    F_1 + X^-1 F_4  F_2 + S^-1 F_5  F_3\nendpmatrix","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"with Sigma_x = X^-1 v and Sigma_s = S^-1 w. The matrix K_2, symmetric, has a structure more favorable for a direct linear solver.","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"In MadNLP, the matrix K_1 is encoded as an AbstractUnreducedKKTSystem and the matrix K_2 is encoded as an AbstractReducedKKTSystem.","category":"page"},{"location":"man/kkt/#Assembling-a-KKT-system,-step-by-step","page":"KKT systems","title":"Assembling a KKT system, step by step","text":"","category":"section"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"We note that both K_1 and K_2 depend on the Hessian of the Lagrangian W, the Jacobian A and the diagonal matrices Sigma_x = X^1V and Sigma_s = S^-1W. Hence, we have to update the KKT system at each iteration of the interior-point algorithm.","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"By default, MadNLP stores the KKT system as a SparseKKTSystem. The KKT system takes as input a SparseCallback wrapping a given NLPModel nlp. We instantiate the callback cb with the function create_callback:","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"cb = MadNLP.create_callback(\n    MadNLP.SparseCallback,\n    nlp,\n)\nind_cons = MadNLP.get_index_constraints(nlp)\n","category":"page"},{"location":"man/kkt/#Initializing-a-KKT-system","page":"KKT systems","title":"Initializing a KKT system","text":"","category":"section"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"The size of the KKT system depends directly on the problem's characteristics (number of variables, number of of equality and inequality constraints). A SparseKKTSystem stores the Hessian and the Jacobian in sparse (COO) format. The KKT matrix can be factorized using either a dense or a sparse linear solvers. Here we use the solver provided in Lapack:","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"linear_solver = LapackCPUSolver","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"We can instantiate a SparseKKTSystem using the function create_kkt_system:","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"kkt = MadNLP.create_kkt_system(\n    MadNLP.SparseKKTSystem,\n    cb,\n    ind_cons,\n    linear_solver,\n)\n","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"Once the KKT system built, one has to initialize it to use it inside the interior-point algorithm:","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"MadNLP.initialize!(kkt);\n","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"The user can query the KKT matrix inside kkt, simply as","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"kkt_matrix = MadNLP.get_kkt(kkt)","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"This returns a reference to the KKT matrix stores internally inside kkt. Each time the matrix is assembled inside kkt, kkt_matrix is updated automatically.","category":"page"},{"location":"man/kkt/#Updating-a-KKT-system","page":"KKT systems","title":"Updating a KKT system","text":"","category":"section"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"We suppose now we want to refresh the values stored in the KKT system.","category":"page"},{"location":"man/kkt/#Updating-the-values-of-the-Hessian","page":"KKT systems","title":"Updating the values of the Hessian","text":"","category":"section"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"The Hessian part of the KKT system can be queried as","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"hess_values = MadNLP.get_hessian(kkt)\n","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"For a SparseKKTSystem, hess_values is a Vector{Float64} storing the nonzero values of the Hessian. Then, one can update the vector hess_values by using NLPModels.jl:","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"n = NLPModels.get_nvar(nlp)\nm = NLPModels.get_ncon(nlp)\nx = NLPModels.get_x0(nlp) # primal variables\nl = zeros(m) # dual variables\n\nNLPModels.hess_coord!(nlp, x, l, hess_values)\n","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"Eventually, a post-processing step is applied to refresh all the values internally:","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"MadNLP.compress_hessian!(kkt)\n","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"note: Note\nBy default, the function compress_hessian! does nothing. But it can be required for very specific use-case, for instance building internally a Schur complement matrix.","category":"page"},{"location":"man/kkt/#Updating-the-values-of-the-Jacobian","page":"KKT systems","title":"Updating the values of the Jacobian","text":"","category":"section"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"We proceed exaclty the same way to update the values in the Jacobian. One queries the Jacobian values in the KKT system as","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"jac_values = MadNLP.get_jacobian(kkt)\n","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"We can refresh the values with NLPModels as","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"NLPModels.jac_coord!(nlp, x, jac_values)\n","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"And then applies a post-processing step as","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"MadNLP.compress_jacobian!(kkt)\n","category":"page"},{"location":"man/kkt/#Updating-the-values-of-the-diagonal-matrices","page":"KKT systems","title":"Updating the values of the diagonal matrices","text":"","category":"section"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"Once the Hessian and the Jacobian updated, the algorithm can apply primal and dual regularization terms on the diagonal of the KKT system, to improve the numerical behavior in the linear solver. This operation is implemented inside the regularize_diagonal! function:","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"pr_value = 1.0\ndu_value = 0.0\n\nMadNLP.regularize_diagonal!(kkt, pr_value, du_value)\n","category":"page"},{"location":"man/kkt/#Assembling-the-KKT-matrix","page":"KKT systems","title":"Assembling the KKT matrix","text":"","category":"section"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"Once the values updated, one can assemble the resulting KKT matrix. This translates to","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"MadNLP.build_kkt!(kkt)","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"By doing so, the values stored inside kkt will be transferred to the KKT matrix kkt_matrix (as returned by the function get_kkt):","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"kkt_matrix","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"Internally, a SparseKKTSystem stores the KKT system in a sparse COO format. When build_kkt! is called, the sparse COO matrix is transferred to SparseMatrixCSC if the linear solver is sparse, or alternatively to a Matrix if the linear solver is dense.","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"note: Note\nThe KKT system stores only the lower-triangular part of the KKT system, as it is symmetric.","category":"page"},{"location":"man/kkt/#Solution-of-the-KKT-system","page":"KKT systems","title":"Solution of the KKT system","text":"","category":"section"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"Now the KKT system is assembled in a matrix K (here stored in kkt_matrix), we want to solve a linear system K x = b, for instance to evaluate the next descent direction. To do so, we use the linear solver stored internally inside kkt (here an instance of LapackCPUSolver).","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"We start by factorizing the KKT matrix K:","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"MadNLP.factorize!(kkt.linear_solver)\n","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"By default, MadNLP uses a LBL factorization to decompose the symmetric indefinite KKT matrix.","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"Once the KKT matrix has been factorized, we can compute the solution of the linear system with a backsolve. The function takes as input a AbstractKKTVector, an object used to do algebraic manipulation with a AbstractKKTSystem. We start by instantiating two UnreducedKKTVector (encoding respectively the right-hand-side and the solution):","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"b = MadNLP.UnreducedKKTVector(kkt)\nfill!(MadNLP.full(b), 1.0)\nx = copy(b)\n","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"The right-hand-side encodes a vector of 1:","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"MadNLP.full(b)","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"We solve the system K x = b using the solve! function:","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"MadNLP.solve!(kkt, x)\nMadNLP.full(x)","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"We verify that the solution is correct by multiplying it on the left with the KKT system, using mul!:","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"mul!(b, kkt, x) # overwrite b!\nMadNLP.full(b)","category":"page"},{"location":"man/kkt/","page":"KKT systems","title":"KKT systems","text":"We recover a vector filled with 1, which was the initial right-hand-side.","category":"page"},{"location":"lib/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"CurrentModule = MadNLP","category":"page"},{"location":"lib/linear_solvers/#Linear-solvers","page":"Linear Solvers","title":"Linear solvers","text":"","category":"section"},{"location":"lib/linear_solvers/#Direct-linear-solvers","page":"Linear Solvers","title":"Direct linear solvers","text":"","category":"section"},{"location":"lib/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"Each linear solver employed in MadNLP implements the following interface.","category":"page"},{"location":"lib/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"AbstractLinearSolver\nintroduce\nfactorize!\nsolve!\nis_inertia\ninertia","category":"page"},{"location":"lib/linear_solvers/#MadNLP.AbstractLinearSolver","page":"Linear Solvers","title":"MadNLP.AbstractLinearSolver","text":"AbstractLinearSolver\n\nAbstract type for linear solver targeting the resolution of the linear system Ax=b.\n\n\n\n\n\n","category":"type"},{"location":"lib/linear_solvers/#MadNLP.introduce","page":"Linear Solvers","title":"MadNLP.introduce","text":"introduce(::AbstractLinearSolver)\n\nPrint the name of the linear solver.\n\n\n\n\n\n","category":"function"},{"location":"lib/linear_solvers/#MadNLP.factorize!","page":"Linear Solvers","title":"MadNLP.factorize!","text":"factorize!(::AbstractLinearSolver)\n\nFactorize the matrix A and updates the factors inside the AbstractLinearSolver instance.\n\n\n\n\n\n","category":"function"},{"location":"lib/linear_solvers/#SolverCore.solve!","page":"Linear Solvers","title":"SolverCore.solve!","text":"solve!(::AbstractLinearSolver, x::AbstractVector)\n\nSolve the linear system Ax = b.\n\nThis function assumes the linear system has been factorized previously with factorize!.\n\n\n\n\n\n","category":"function"},{"location":"lib/linear_solvers/#MadNLP.is_inertia","page":"Linear Solvers","title":"MadNLP.is_inertia","text":"is_inertia(::AbstractLinearSolver)\n\nReturn true if the linear solver supports the computation of the inertia of the linear system.\n\n\n\n\n\n","category":"function"},{"location":"lib/linear_solvers/#MadNLP.inertia","page":"Linear Solvers","title":"MadNLP.inertia","text":"inertia(::AbstractLinearSolver)\n\nReturn the inertia (n, m, p) of the linear system as a tuple.\n\nNote\n\nThe inertia is defined as a tuple (n m p), with\n\nn: number of positive eigenvalues\nm: number of negative eigenvalues\np: number of zero eigenvalues\n\n\n\n\n\n","category":"function"},{"location":"lib/linear_solvers/#Iterative-refinement","page":"Linear Solvers","title":"Iterative refinement","text":"","category":"section"},{"location":"lib/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"MadNLP uses iterative refinement to improve the accuracy of the solution returned by the linear solver.","category":"page"},{"location":"lib/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"solve_refine!\n","category":"page"},{"location":"lib/linear_solvers/#MadNLP.solve_refine!","page":"Linear Solvers","title":"MadNLP.solve_refine!","text":"solve_refine!(x::VT, ::AbstractIterator, b::VT, w::VT) where {VT <: AbstractKKTVector}\n\nSolve the linear system Ax = b using iterative refinement. The object AbstractIterator stores an instance of a AbstractLinearSolver for the backsolve operations.\n\nNotes\n\nThis function assumes the matrix stored in the linear solver has been factorized previously.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/","page":"KKT systems","title":"KKT systems","text":"CurrentModule = MadNLP","category":"page"},{"location":"lib/kkt/#KKT-systems","page":"KKT systems","title":"KKT systems","text":"","category":"section"},{"location":"lib/kkt/","page":"KKT systems","title":"KKT systems","text":"MadNLP manipulates KKT systems using two abstractions: an AbstractKKTSystem storing the KKT system' matrix and an AbstractKKTVector storing the KKT system's right-hand-side.","category":"page"},{"location":"lib/kkt/#AbstractKKTSystem","page":"KKT systems","title":"AbstractKKTSystem","text":"","category":"section"},{"location":"lib/kkt/","page":"KKT systems","title":"KKT systems","text":"AbstractKKTSystem\n","category":"page"},{"location":"lib/kkt/#MadNLP.AbstractKKTSystem","page":"KKT systems","title":"MadNLP.AbstractKKTSystem","text":"AbstractKKTSystem{T, VT<:AbstractVector{T}, MT<:AbstractMatrix{T}, QN<:AbstractHessian{T}}\n\nAbstract type for KKT system.\n\n\n\n\n\n","category":"type"},{"location":"lib/kkt/","page":"KKT systems","title":"KKT systems","text":"MadNLP implements three different types of AbstractKKTSystem, depending how far we reduce the KKT system.","category":"page"},{"location":"lib/kkt/","page":"KKT systems","title":"KKT systems","text":"AbstractUnreducedKKTSystem\nAbstractReducedKKTSystem\nAbstractCondensedKKTSystem","category":"page"},{"location":"lib/kkt/#MadNLP.AbstractUnreducedKKTSystem","page":"KKT systems","title":"MadNLP.AbstractUnreducedKKTSystem","text":"AbstractUnreducedKKTSystem{T, VT, MT, QN} <: AbstractKKTSystem{T, VT, MT, QN}\n\nAugmented KKT system associated to the linearization of the KKT conditions at the current primal-dual iterate (x s y z ν w).\n\nThe associated matrix is\n\n[Wₓₓ  0   Aₑ'  Aᵢ' V½  0  ]  [Δx]\n[0    0   0   -I   0   W½ ]  [Δs]\n[Aₑ   0   0    0   0   0  ]  [Δy]\n[Aᵢ  -I   0    0   0   0  ]  [Δz]\n[V½   0   0    0  -X   0  ]  [Δτ]\n[0    W½  0    0   0  -S  ]  [Δρ]\n\nwith\n\nWₓₓ: Hessian of the Lagrangian.\nAₑ: Jacobian of the equality constraints\nAᵢ: Jacobian of the inequality constraints\nX = diag(x)\nS = diag(s)\nV = diag(ν)\nW = diag(w)\nΔτ = -W^-½Δν\nΔρ = -W^-½Δw\n\n\n\n\n\n","category":"type"},{"location":"lib/kkt/#MadNLP.AbstractReducedKKTSystem","page":"KKT systems","title":"MadNLP.AbstractReducedKKTSystem","text":"AbstractReducedKKTSystem{T, VT, MT, QN} <: AbstractKKTSystem{T, VT, MT, QN}\n\nThe reduced KKT system is a simplification of the original Augmented KKT system. Comparing to AbstractUnreducedKKTSystem), AbstractReducedKKTSystem removes the two last rows associated to the bounds' duals (ν w).\n\nAt a primal-dual iterate (x s y z), the matrix writes\n\n[Wₓₓ + Σₓ   0    Aₑ'   Aᵢ']  [Δx]\n[0          Σₛ    0    -I ]  [Δs]\n[Aₑ         0     0     0 ]  [Δy]\n[Aᵢ        -I     0     0 ]  [Δz]\n\nwith\n\nWₓₓ: Hessian of the Lagrangian.\nAₑ: Jacobian of the equality constraints\nAᵢ: Jacobian of the inequality constraints\nΣₓ = X¹ V\nΣₛ = S¹ W\n\n\n\n\n\n","category":"type"},{"location":"lib/kkt/#MadNLP.AbstractCondensedKKTSystem","page":"KKT systems","title":"MadNLP.AbstractCondensedKKTSystem","text":"AbstractCondensedKKTSystem{T, VT, MT, QN} <: AbstractKKTSystem{T, VT, MT, QN}\n\nThe condensed KKT system simplifies further the AbstractReducedKKTSystem by removing the rows associated to the slack variables s and the inequalities.\n\nAt the primal-dual iterate (x y), the matrix writes\n\n[Wₓₓ + Σₓ + Aᵢ' Σₛ Aᵢ    Aₑ']  [Δx]\n[         Aₑ              0 ]  [Δy]\n\nwith\n\nWₓₓ: Hessian of the Lagrangian.\nAₑ: Jacobian of the equality constraints\nAᵢ: Jacobian of the inequality constraints\nΣₓ = X¹ V\nΣₛ = S¹ W\n\n\n\n\n\n","category":"type"},{"location":"lib/kkt/","page":"KKT systems","title":"KKT systems","text":"Each AbstractKKTSystem follows the interface described below:","category":"page"},{"location":"lib/kkt/","page":"KKT systems","title":"KKT systems","text":"\ncreate_kkt_system\n\nnum_variables\nget_kkt\nget_jacobian\nget_hessian\n\ninitialize!\nbuild_kkt!\ncompress_hessian!\ncompress_jacobian!\njtprod!\nregularize_diagonal!\nis_inertia_correct\nnnz_jacobian","category":"page"},{"location":"lib/kkt/#MadNLP.create_kkt_system","page":"KKT systems","title":"MadNLP.create_kkt_system","text":"create_kkt_system(\n    ::Type{KKT},\n    cb::AbstractCallback,\n    ind_cons::NamedTuple,\n    linear_solver::Type{LinSol};\n    opt_linear_solver=default_options(linear_solver),\n    hessian_approximation=ExactHessian,\n) where {KKT<:AbstractKKTSystem, LinSol<:AbstractLinearSolver}\n\nInstantiate a new KKT system with type KKT, associated to the the nonlinear program encoded inside the callback cb. The NamedTuple ind_cons stores the indexes of all the variables and constraints in the callback cb. In addition, the user should pass the linear solver linear_solver that will be used to solve the KKT system after it has been assembled.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.num_variables","page":"KKT systems","title":"MadNLP.num_variables","text":"Number of primal variables (including slacks) associated to the KKT system.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.get_kkt","page":"KKT systems","title":"MadNLP.get_kkt","text":"get_kkt(kkt::AbstractKKTSystem)::AbstractMatrix\n\nReturn a pointer to the KKT matrix implemented in kkt. The pointer is passed afterward to a linear solver.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.get_jacobian","page":"KKT systems","title":"MadNLP.get_jacobian","text":"Get Jacobian matrix\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.get_hessian","page":"KKT systems","title":"MadNLP.get_hessian","text":"Get Hessian matrix\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.initialize!","page":"KKT systems","title":"MadNLP.initialize!","text":"initialize!(kkt::AbstractKKTSystem)\n\nInitialize KKT system with default values. Called when we initialize the MadNLPSolver storing the current KKT system kkt.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.build_kkt!","page":"KKT systems","title":"MadNLP.build_kkt!","text":"build_kkt!(kkt::AbstractKKTSystem)\n\nAssemble the KKT matrix before calling the factorization routine.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.compress_hessian!","page":"KKT systems","title":"MadNLP.compress_hessian!","text":"compress_hessian!(kkt::AbstractKKTSystem)\n\nCompress the Hessian inside kkt's internals. This function is called every time a new Hessian is evaluated.\n\nDefault implementation do nothing.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.compress_jacobian!","page":"KKT systems","title":"MadNLP.compress_jacobian!","text":"compress_jacobian!(kkt::AbstractKKTSystem)\n\nCompress the Jacobian inside kkt's internals. This function is called every time a new Jacobian is evaluated.\n\nBy default, the function updates in the Jacobian the coefficients associated to the slack variables.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.jtprod!","page":"KKT systems","title":"MadNLP.jtprod!","text":"jtprod!(y::AbstractVector, kkt::AbstractKKTSystem, x::AbstractVector)\n\nMultiply with transpose of Jacobian and store the result in y, such that y = A x (with A current Jacobian).\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.regularize_diagonal!","page":"KKT systems","title":"MadNLP.regularize_diagonal!","text":"regularize_diagonal!(kkt::AbstractKKTSystem, primal_values::Number, dual_values::Number)\n\nRegularize the values in the diagonal of the KKT system. Called internally inside the interior-point routine.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.is_inertia_correct","page":"KKT systems","title":"MadNLP.is_inertia_correct","text":"is_inertia_correct(kkt::AbstractKKTSystem, n::Int, m::Int, p::Int)\n\nCheck if the inertia (n m p) returned by the linear solver is adapted to the KKT system implemented in kkt.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.nnz_jacobian","page":"KKT systems","title":"MadNLP.nnz_jacobian","text":"Nonzero in Jacobian\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#Sparse-KKT-systems","page":"KKT systems","title":"Sparse KKT systems","text":"","category":"section"},{"location":"lib/kkt/","page":"KKT systems","title":"KKT systems","text":"By default, MadNLP stores a AbstractReducedKKTSystem in sparse format, as implemented by SparseKKTSystem:","category":"page"},{"location":"lib/kkt/","page":"KKT systems","title":"KKT systems","text":"SparseKKTSystem\n","category":"page"},{"location":"lib/kkt/#MadNLP.SparseKKTSystem","page":"KKT systems","title":"MadNLP.SparseKKTSystem","text":"SparseKKTSystem{T, VT, MT, QN} <: AbstractReducedKKTSystem{T, VT, MT, QN}\n\nImplement the AbstractReducedKKTSystem in sparse COO format.\n\n\n\n\n\n","category":"type"},{"location":"lib/kkt/","page":"KKT systems","title":"KKT systems","text":"Alternatively, the user has the choice to store the KKT system as a SparseUnreducedKKTSystem or as a SparseCondensedKKTSystem:","category":"page"},{"location":"lib/kkt/","page":"KKT systems","title":"KKT systems","text":"SparseUnreducedKKTSystem\nSparseCondensedKKTSystem","category":"page"},{"location":"lib/kkt/#MadNLP.SparseUnreducedKKTSystem","page":"KKT systems","title":"MadNLP.SparseUnreducedKKTSystem","text":"SparseUnreducedKKTSystem{T, VT, MT, QN} <: AbstractUnreducedKKTSystem{T, VT, MT, QN}\n\nImplement the AbstractUnreducedKKTSystem in sparse COO format.\n\n\n\n\n\n","category":"type"},{"location":"lib/kkt/#MadNLP.SparseCondensedKKTSystem","page":"KKT systems","title":"MadNLP.SparseCondensedKKTSystem","text":"SparseCondensedKKTSystem{T, VT, MT, QN} <: AbstractCondensedKKTSystem{T, VT, MT, QN}\n\nImplement the AbstractCondensedKKTSystem in sparse COO format.\n\n\n\n\n\n","category":"type"},{"location":"lib/kkt/#Dense-KKT-systems","page":"KKT systems","title":"Dense KKT systems","text":"","category":"section"},{"location":"lib/kkt/","page":"KKT systems","title":"KKT systems","text":"MadNLP provides also two structures to store the KKT system in a dense matrix. Although less efficient than their sparse counterparts, these two structures allow to store the KKT system efficiently when the problem is instantiated on the GPU.","category":"page"},{"location":"lib/kkt/","page":"KKT systems","title":"KKT systems","text":"DenseKKTSystem\nDenseCondensedKKTSystem\n","category":"page"},{"location":"lib/kkt/#MadNLP.DenseKKTSystem","page":"KKT systems","title":"MadNLP.DenseKKTSystem","text":"DenseKKTSystem{T, VT, MT, QN, VI} <: AbstractReducedKKTSystem{T, VT, MT, QN}\n\nImplement AbstractReducedKKTSystem with dense matrices.\n\nRequires a dense linear solver to be factorized (otherwise an error is returned).\n\n\n\n\n\n","category":"type"},{"location":"lib/kkt/#MadNLP.DenseCondensedKKTSystem","page":"KKT systems","title":"MadNLP.DenseCondensedKKTSystem","text":"DenseCondensedKKTSystem{T, VT, MT, QN} <: AbstractCondensedKKTSystem{T, VT, MT, QN}\n\nImplement AbstractCondensedKKTSystem with dense matrices.\n\nRequires a dense linear solver to factorize the associated KKT system (otherwise an error is returned).\n\n\n\n\n\n","category":"type"},{"location":"lib/kkt/#AbstractKKTVector","page":"KKT systems","title":"AbstractKKTVector","text":"","category":"section"},{"location":"lib/kkt/","page":"KKT systems","title":"KKT systems","text":"Each instance of AbstractKKTVector implements the following interface.","category":"page"},{"location":"lib/kkt/","page":"KKT systems","title":"KKT systems","text":"AbstractKKTVector\n\nnumber_primal\nnumber_dual\nfull\nprimal\ndual\nprimal_dual\ndual_lb\ndual_ub\n","category":"page"},{"location":"lib/kkt/#MadNLP.AbstractKKTVector","page":"KKT systems","title":"MadNLP.AbstractKKTVector","text":"AbstractKKTVector{T, VT}\n\nSupertype for KKT's right-hand-side vectors (x s y z ν w).\n\n\n\n\n\n","category":"type"},{"location":"lib/kkt/#MadNLP.number_primal","page":"KKT systems","title":"MadNLP.number_primal","text":"number_primal(X::AbstractKKTVector)\n\nGet total number of primal values (x s) in KKT vector X.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.number_dual","page":"KKT systems","title":"MadNLP.number_dual","text":"number_dual(X::AbstractKKTVector)\n\nGet total number of dual values (y z) in KKT vector X.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.full","page":"KKT systems","title":"MadNLP.full","text":"full(X::AbstractKKTVector)\n\nReturn the all the values stored inside the KKT vector X.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.primal","page":"KKT systems","title":"MadNLP.primal","text":"primal(X::AbstractKKTVector)\n\nReturn the primal values (x s) stored in the KKT vector X.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.dual","page":"KKT systems","title":"MadNLP.dual","text":"dual(X::AbstractKKTVector)\n\nReturn the dual values (y z) stored in the KKT vector X.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.primal_dual","page":"KKT systems","title":"MadNLP.primal_dual","text":"primal_dual(X::AbstractKKTVector)\n\nReturn both the primal and the dual values (x s y z) stored in the KKT vector X.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.dual_lb","page":"KKT systems","title":"MadNLP.dual_lb","text":"dual_lb(X::AbstractKKTVector)\n\nReturn the dual values ν associated to the lower-bound stored in the KKT vector X.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.dual_ub","page":"KKT systems","title":"MadNLP.dual_ub","text":"dual_ub(X::AbstractKKTVector)\n\nReturn the dual values w associated to the upper-bound stored in the KKT vector X.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/","page":"KKT systems","title":"KKT systems","text":"By default, MadNLP provides one implementation of an AbstractKKTVector.","category":"page"},{"location":"lib/kkt/","page":"KKT systems","title":"KKT systems","text":"UnreducedKKTVector","category":"page"},{"location":"lib/kkt/#MadNLP.UnreducedKKTVector","page":"KKT systems","title":"MadNLP.UnreducedKKTVector","text":"UnreducedKKTVector{T, VT<:AbstractVector{T}} <: AbstractKKTVector{T, VT}\n\nFull KKT vector (x s y z ν w), associated to a AbstractUnreducedKKTSystem.\n\n\n\n\n\n","category":"type"},{"location":"lib/ipm/","page":"IPM solver","title":"IPM solver","text":"CurrentModule = MadNLP","category":"page"},{"location":"lib/ipm/#MadNLP-solver","page":"IPM solver","title":"MadNLP solver","text":"","category":"section"},{"location":"lib/ipm/","page":"IPM solver","title":"IPM solver","text":"MadNLP takes as input a nonlinear program encoded as a AbstractNLPModel and solve it using interior-point. The main entry point is the function madnlp:","category":"page"},{"location":"lib/ipm/","page":"IPM solver","title":"IPM solver","text":"madnlp\nMadNLPExecutionStats","category":"page"},{"location":"lib/ipm/#MadNLP.madnlp","page":"IPM solver","title":"MadNLP.madnlp","text":"madnlp(model::AbstractNLPModel; options...)\n\nBuild a MadNLPSolver and solve it using the interior-point method. Return the solution as a MadNLPExecutionStats.\n\n\n\n\n\n","category":"function"},{"location":"lib/ipm/#MadNLP.MadNLPExecutionStats","page":"IPM solver","title":"MadNLP.MadNLPExecutionStats","text":"MadNLPExecutionStats{T, VT} <: AbstractExecutionStats\n\nStore the results returned by MadNLP once the interior-point algorithm has terminated.\n\n\n\n\n\n","category":"type"},{"location":"lib/ipm/","page":"IPM solver","title":"IPM solver","text":"In detail, the function madnlp builds a MadNLPSolver storing all the required structures in the solution algorithm. Once the MadNLPSolver instantiated, the function solve! is applied to solve the nonlinear program with MadNLP's interior-point algorithm.","category":"page"},{"location":"lib/ipm/","page":"IPM solver","title":"IPM solver","text":"MadNLPSolver\n","category":"page"},{"location":"lib/ipm/#MadNLP.MadNLPSolver","page":"IPM solver","title":"MadNLP.MadNLPSolver","text":"MadNLPSolver(nlp::AbstractNLPModel{T, VT}; options...) where {T, VT}\n\nInstantiate a new MadNLPSolver associated to the nonlinear program nlp::AbstractNLPModel. The options are passed as optional arguments.\n\nThe constructor allocates all the memory required in the interior-point algorithm, so the main algorithm remains allocation free.\n\n\n\n\n\n","category":"type"},{"location":"quickstart/#Quickstart","page":"Quickstart","title":"Quickstart","text":"","category":"section"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"CurrentModule = MadNLP","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"using NLPModels\n","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"This page presents a quickstart guide to solve a nonlinear problem with MadNLP.","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"As a demonstration, we show how to implement the HS15 nonlinear problem from the Hock & Schittkowski collection, first by using a nonlinear modeler and then by specifying the derivatives manually.","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"The HS15 problem is defined as:","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"beginaligned\nmin_x_1 x_2     100 times (x_2 - x_1^2)^2 +(1 - x_1)^2 \ntextsubject to quad   x_1  times x_2 geq 1 \n         x_1 + x_2^2 geq 0 \n         x_1 leq 05\nendaligned\n","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"Despite its small dimension, its resolution remains challenging as the problem is nonlinear nonconvex. Note that HS15 encompasses one bound constraint (x_1 leq 05) and two generic constraints.","category":"page"},{"location":"quickstart/#Using-a-nonlinear-modeler:-JuMP.jl","page":"Quickstart","title":"Using a nonlinear modeler: JuMP.jl","text":"","category":"section"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"The easiest way to implement a nonlinear problem is to use a nonlinear modeler as JuMP. In JuMP, the user just has to pass the structure of the problem, the computation of the first- and second-order derivatives being handled automatically.","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"Using JuMP's syntax, the HS15 problem translates to","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"using JuMP\nmodel = Model()\n@variable(model, x1 <= 0.5)\n@variable(model, x2)\n\n@objective(model, Min, 100.0 * (x2 - x1^2)^2 + (1.0 - x1)^2)\n@constraint(model, x1 * x2 >= 1.0)\n@constraint(model, x1 + x2^2 >= 0.0)\n\nprintln(model)\n","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"Then, solving HS15 with MadNLP directly translates to","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"using MadNLP\nJuMP.set_optimizer(model, MadNLP.Optimizer)\nJuMP.optimize!(model)\n","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"Under the hood, JuMP builds a nonlinear model with a sparse AD backend to evaluate the first and second-order derivatives of the objective and the constraints. Internally, MadNLP takes as input the callbacks generated by JuMP and wraps them inside a MadNLP.MOIModel.","category":"page"},{"location":"quickstart/#Specifying-the-derivatives-by-hand:-NLPModels.jl","page":"Quickstart","title":"Specifying the derivatives by hand: NLPModels.jl","text":"","category":"section"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"Alternatively, we can compute the derivatives manually and define directly a NLPModel associated to our problem. This second option, although more complicated, gives us more flexibility and comes without boilerplate.","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"We define a new NLPModel structure simply as:","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"struct HS15Model <: NLPModels.AbstractNLPModel{Float64,Vector{Float64}}\n    meta::NLPModels.NLPModelMeta{Float64, Vector{Float64}}\n    counters::NLPModels.Counters\nend\n\nfunction HS15Model(x0)\n    return HS15Model(\n        NLPModels.NLPModelMeta(\n            2,     #nvar\n            ncon = 2,\n            nnzj = 4,\n            nnzh = 3,\n            x0 = x0,\n            y0 = zeros(2),\n            lvar = [-Inf, -Inf],\n            uvar = [0.5, Inf],\n            lcon = [1.0, 0.0],\n            ucon = [Inf, Inf],\n            minimize = true\n        ),\n        NLPModels.Counters()\n    )\nend","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"This structure takes as input the initial position x0 and generates a AbstractNLPModel. NLPModelMeta stores the information about the structure of the problem (variables and constraints' lower and upper bounds, number of variables, number of constraints, ...). Counters is just a utility storing the number of time each callbacks is being called.","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"The objective function takes as input a HS15Model instance and a vector with dimension 2 storing the current values for x_1 and x_2:","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"function NLPModels.obj(nlp::HS15Model, x::AbstractVector)\n    return 100.0 * (x[2] - x[1]^2)^2 + (1.0 - x[1])^2\nend","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"The corresponding gradient writes (note that we update the values of the gradient g inplace):","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"function NLPModels.grad!(nlp::HS15Model, x::AbstractVector, g::AbstractVector)\n    z = x[2] - x[1]^2\n    g[1] = -400.0 * z * x[1] - 2.0 * (1.0 - x[1])\n    g[2] = 200.0 * z\n    return g\nend","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"Similarly, we define the constraints","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"function NLPModels.cons!(nlp::HS15Model, x::AbstractVector, c::AbstractVector)\n    c[1] = x[1] * x[2]\n    c[2] = x[1] + x[2]^2\n    return c\nend","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"and the associated Jacobian","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"function NLPModels.jac_structure!(nlp::HS15Model, I::AbstractVector{T}, J::AbstractVector{T}) where T\n    copyto!(I, [1, 1, 2, 2])\n    copyto!(J, [1, 2, 1, 2])\nend\n\nfunction NLPModels.jac_coord!(nlp::HS15Model, x::AbstractVector, J::AbstractVector)\n    J[1] = x[2]    # (1, 1)\n    J[2] = x[1]    # (1, 2)\n    J[3] = 1.0     # (2, 1)\n    J[4] = 2*x[2]  # (2, 2)\n    return J\nend","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"note: Note\nAs the Jacobian is sparse, we have to provide its sparsity structure.","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"It remains to implement the Hessian of the Lagrangian for a HS15Model. The Lagrangian of the problem writes","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"L(x_1 x_2 y_1 y_2) = 100 times (x_2 - x_1^2)^2 +(1 - x_1)^2\n+ y_1 times (x_1 times x_2) + y_2 times (x_1 + x_2^2)","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"and we aim at evaluating its second-order derivative nabla^2_xxL(x_1 x_2 y_1 y_2).","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"We first have to define the sparsity structure of the Hessian, which is assumed to be sparse. The Hessian is a symmetric matrix, and by convention we pass only the lower-triangular part of the matrix to the solver. Hence, we define the sparsity structure as","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"function NLPModels.hess_structure!(nlp::HS15Model, I::AbstractVector{T}, J::AbstractVector{T}) where T\n    copyto!(I, [1, 2, 2])\n    copyto!(J, [1, 1, 2])\nend","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"Now that the sparsity structure is defined, the associated Hessian writes down:","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"function NLPModels.hess_coord!(nlp::HS15Model, x, y, H::AbstractVector; obj_weight=1.0)\n    # Objective\n    H[1] = obj_weight * (-400.0 * x[2] + 1200.0 * x[1]^2 + 2.0)\n    H[2] = obj_weight * (-400.0 * x[1])\n    H[3] = obj_weight * 200.0\n    # First constraint\n    H[2] += y[1] * 1.0\n    # Second constraint\n    H[3] += y[2] * 2.0\n    return H\nend\n","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"Once the problem specified in NLPModels's syntax, we can create a new MadNLP instance and solve it:","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"x0 = zeros(2) # initial position\nnlp = HS15Model(x0)\nsolver = MadNLP.MadNLPSolver(nlp; print_level=MadNLP.INFO)\nresults = MadNLP.solve!(solver)","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"MadNLP converges in 19 iterations to a (local) optimal solution. MadNLP returns a MadNLPExecutionStats storing all the results. We can query the primal and the dual solutions respectively by","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"results.solution","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"and","category":"page"},{"location":"quickstart/","page":"Quickstart","title":"Quickstart","text":"results.multipliers","category":"page"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"CurrentModule = MadNLP","category":"page"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"using SparseArrays\nusing NLPModels\nusing MadNLP\nusing MadNLPTests\n# Build nonlinear model\nnlp = MadNLPTests.HS15Model()\n# Build KKT\ncb = MadNLP.create_callback(\n    MadNLP.SparseCallback,\n    nlp,\n)\nind_cons = MadNLP.get_index_constraints(nlp)\nlinear_solver = LapackCPUSolver\nkkt = MadNLP.create_kkt_system(\n    MadNLP.SparseKKTSystem,\n    cb,\n    ind_cons,\n    linear_solver,\n)\n\nn = NLPModels.get_nvar(nlp)\nm = NLPModels.get_ncon(nlp)\nx = NLPModels.get_x0(nlp)\nl = zeros(m)\nhess_values = MadNLP.get_hessian(kkt)\nNLPModels.hess_coord!(nlp, x, l, hess_values)\n\njac_values = MadNLP.get_jacobian(kkt)\nNLPModels.jac_coord!(nlp, x, jac_values)\nMadNLP.compress_jacobian!(kkt)\n\nMadNLP.build_kkt!(kkt)\n","category":"page"},{"location":"man/linear_solvers/#Linear-solvers","page":"Linear Solvers","title":"Linear solvers","text":"","category":"section"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"We suppose that the KKT system has been assembled previously into a given AbstractKKTSystem. Then, it remains to compute the Newton step by solving the KKT system for a given right-hand-side (given as a AbstractKKTVector). That's exactly the role of the linear solver.","category":"page"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"If we do not assume any structure, the KKT system writes in generic form","category":"page"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"K x = b","category":"page"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"with K the KKT matrix and b the current right-hand-side. MadNLP provides a suite of specialized linear solvers to solve the linear system.","category":"page"},{"location":"man/linear_solvers/#Inertia-detection","page":"Linear Solvers","title":"Inertia detection","text":"","category":"section"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"If the matrix K has negative eigenvalues, we have no guarantee that the solution of the KKT system is a descent direction with regards to the original nonlinear problem. That's the reason why most of the linear solvers compute the inertia of the linear system when factorizing the matrix K. The inertia counts the number of positive, negative and zero eigenvalues in the matrix. If the inertia does not meet a given criteria, then the matrix K is regularized by adding a multiple of the identity to it: K_r = K + alpha I.","category":"page"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"note: Note\nWe recall that the inertia of a matrix K is given as a triplet (nmp), with n the number of positive eigenvalues, m the number of negative eigenvalues and p the number of zero eigenvalues.","category":"page"},{"location":"man/linear_solvers/#Factorization-algorithm","page":"Linear Solvers","title":"Factorization algorithm","text":"","category":"section"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"In nonlinear programming, it is common to employ a LBL factorization to decompose the symmetric indefinite matrix K, as this algorithm returns the inertia of the matrix directly as a result of the factorization.","category":"page"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"note: Note\nWhen MadNLP runs in inertia-free mode, the algorithm does not require to compute the inertia when factorizing the matrix K. In that case, MadNLP can use a classical LU or QR factorization to solve the linear system Kx = b.","category":"page"},{"location":"man/linear_solvers/#Solving-a-KKT-system-with-MadNLP","page":"Linear Solvers","title":"Solving a KKT system with MadNLP","text":"","category":"section"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"We suppose available a AbstractKKTSystem kkt, properly assembled following the procedure presented previously. We can query the assembled matrix K as","category":"page"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"K = MadNLP.get_kkt(kkt)\n","category":"page"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"Then, if we want to pass the KKT matrix K to Lapack, this translates to","category":"page"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"linear_solver = LapackCPUSolver(K)\n","category":"page"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"The instance linear_solver does not copy the matrix K and instead keep a reference to it.","category":"page"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"linear_solver.A === K","category":"page"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"That way every time we re-assemble the matrix K in kkt, the values are directly updated inside linear_solver.","category":"page"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"To compute the factorization inside linear_solver, one simply as to call:","category":"page"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"MadNLP.factorize!(linear_solver)\n","category":"page"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"Once the factorization computed, computing the backsolve for a right-hand-side b amounts to","category":"page"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"nk = size(kkt, 1)\nb = rand(nk)\nMadNLP.solve!(linear_solver, b)","category":"page"},{"location":"man/linear_solvers/","page":"Linear Solvers","title":"Linear Solvers","text":"The values of b being modified inplace to store the solution x of the linear system Kx =b.","category":"page"},{"location":"options/#MadNLP-Options","page":"Options","title":"MadNLP Options","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"Pages = [\n    \"options.md\",\n]\nDepth=3","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"","category":"page"},{"location":"options/#Primary-options","page":"Options","title":"Primary options","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"These options are used to set the values for other options. The default values are inferred from the NLP model.","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"tol::Float64\n  Termination tolerance. The default value is 1e-8 for double precision. The solver terminates if the scaled primal, dual, complementary infeasibility is less than tol. Valid range is (0infty).\ncallback::Type \t Valid values are: MadNLP.{DenseCallback,SparseCallback}.\nkkt_system::Type  The type of KKT system. Valid values are MadNLP.{SpasreKKTSystem,SparseUnreducedKKTSystem,SparseCondensedKKTSystem,DenseKKTSystem,DenseCondensedKKTSystem}.\nlinear_solver::Type\nLinear solver used for solving primal-dual system. Valid values are: {MadNLP.UmfpackSolver,MadNLP.LDLSolver,MadNLP.CHOLMODSolver, MadNLP.MumpsSolver, MadNLP.PardisoSolver, MadNLP.PardisoMKLSolver, MadNLP.Ma27Solver, MadNLP.Ma57Solver, MadNLP.Ma77Solver, MadNLP.Ma86Solver, MadNLP.Ma97Solver, MadNLP.LapackCPUSolver, MadNLPGPU.LapackGPUSolver,MadNLPGPU.RFSolver,MadNLPGPU.GLUSolver,MadNLPGPU.CuCholeskySolver,MadNLPGPU.CUDSSSolver} (some may require using extension packages). The selected solver should be properly built in the build procedure. See README.md file.","category":"page"},{"location":"options/#General-options","page":"Options","title":"General options","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"iterator::Type = RichardsonIterator\n  Iterator used for iterative refinement. Valid values are: {MadNLPRichardson,MadNLPKrylov}.\nRichardson uses Richardson iteration\nKrylov uses restarted Generalized Minimal Residual method implemented in IterativeSolvers.jl.\nblas_num_threads::Int = 1\n  Number of threads used for BLAS routines. Valid range is 1infty).\ndisable_garbage_collector::Bool = false\n  If true, Julia garbage collector is temporarily disabled while solving the problem, and then enabled back once the solution is complete.\nrethrow_error::Bool = true\n  If false, any internal error thrown by MadNLP and interruption exception (triggered by the user via ^C) is caught, and not rethrown. If an error is caught, the solver terminates with an error message.","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"","category":"page"},{"location":"options/#Output-options","page":"Options","title":"Output options","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"print_level::LogLevels = INFO\n  stdout print level. Any message with level less than print_level is not printed on stdout. Valid values are: MadNLP.{TRACE, DEBUG, INFO, NOTICE, WARN, ERROR}.\noutput_file::String = INFO\n  If not \"\", the output log is teed to the file at the path specified in output_file.\nfile_print_level::LogLevels = TRACE\n  File print level; any message with level less than file_print_level is not printed on the file specified in output_file. Valid values are: MadNLP.{TRACE, DEBUG, INFO, NOTICE, WARN, ERROR}.","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"","category":"page"},{"location":"options/#Termination-options","page":"Options","title":"Termination options","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"max_iter::Int = 3000\n  Maximum number of interior point iterations. The solver terminates with exit symbol :Maximum_Iterations_Exceeded if the interior point iteration count exceeds max_iter.\nacceptable_tol::Float64 = 1e-6\n  Acceptable tolerance. The solver terminates if the scaled primal, dual, complementary infeasibility is less than acceptable_tol, for acceptable_iter consecutive interior point iteration steps.\nacceptable_iter::Int = 15\n  Acceptable iteration tolerance. Valid rage is 1infty).\ndiverging_iterates_tol::Float64 = 1e20\n  Diverging iteration tolerance. The solver terminates with exit symbol :Diverging_Iterates if the NLP error is greater than diverging_iterates_tol.\nmax_wall_time::Float64 = 1e6\n  Maximum wall time for interior point solver. The solver terminates with exit symbol :Maximum_WallTime_Exceeded if the total solver wall time exceeds max_wall_time.\ns_max::Float64 = 100.","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"","category":"page"},{"location":"options/#NLP-options","page":"Options","title":"NLP options","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"kappa_d::Float64 = 1e-5\nfixed_variable_treatment::FixedVariableTreatments = MakeParameter\n  Valid values are: MadNLP.{RelaxBound,MakeParameter}.\nequality_treatment::FixedVariableTreatments = MakeParameter\n  Valid values are: MadNLP.{RelaxEquality,EnforceEquality}.\njacobian_constant::Bool = false\n  If true, constraint Jacobian is only evaluated once and reused.\nhessian_constant::Bool = false\n  If true, Lagrangian Hessian is only evaluated once and reused.\nbound_push::Float64 = 1e-2\nbound_fac::Float64 = 1e-2\nhessian_approximation::Type = ExactHessian\nquasi_newton_options::QuasiNewtonOptions = QuasiNewtonOptions()\ninertia_correction_method::InertiaCorrectionMethods = INERTIA_AUTO\n  Valid values are: MadNLP.{INERTIA_AUTO,INERTIA_BASED, INERTIA_FREE}.\nINERTIA_BASED uses the strategy in Ipopt.\nINERTIA_FREE uses the strategy in Chiang (2016).\nINERTIA_AUTO uses INERTIA_BASED if inertia information is available and uses INERTIA_FREE otherwise.\ninertia_free_tol::Float64 = 0.","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"","category":"page"},{"location":"options/#Initialization-Options","page":"Options","title":"Initialization Options","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"dual_initialized::Bool = false\ndual_initialization_method::Type = kkt_system <: MadNLP.SparseCondensedKKTSystem ? DualInitializeSetZero : DualInitializeLeastSquares","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"constr_mult_init_max::Float64 = 1e3\nnlp_scaling::Bool = true: \n  If true, MadNLP scales the nonlinear problem during the resolution.\nnlp_scaling_max_gradient::Float64 = 100.","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"","category":"page"},{"location":"options/#Hessian-perturbation-options","page":"Options","title":"Hessian perturbation options","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"min_hessian_perturbation::Float64 = 1e-20\nfirst_hessian_perturbation::Float64 = 1e-4\nmax_hessian_perturbation::Float64 = 1e20\nperturb_inc_fact_first::Float64 = 1e2\nperturb_inc_fact::Float64 = 8.\nperturb_dec_fact::Float64 = 1/3\njacobian_regularization_exponent::Float64 = 1/4\njacobian_regularization_value::Float64 = 1e-8","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"","category":"page"},{"location":"options/#Restoration-options","page":"Options","title":"Restoration options","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"soft_resto_pderror_reduction_factor::Float64 = 0.9999\nrequired_infeasibility_reduction::Float64 = 0.9","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"","category":"page"},{"location":"options/#Line-search-options","page":"Options","title":"Line-search options","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"obj_max_inc::Float64 = 5.\nkappha_soc::Float64 = 0.99\nmax_soc::Int = 4\nalpha_min_frac::Float64 = 0.05\ns_theta::Float64 = 1.1\ns_phi::Float64 = 2.3\neta_phi::Float64 = 1e-4\nkappa_soc::Float64 = 0.99\ngamma_theta::Float64 = 1e-5\ngamma_phi::Float64 = 1e-5\ndelta::Float64 = 1\nkappa_sigma::Float64 = 1e10\nbarrier_tol_factor::Float64 = 10.\nrho::Float64 = 1000.","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"","category":"page"},{"location":"options/#Barrier-options","page":"Options","title":"Barrier options","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"mu_init::Float64 = 1e-1\nmu_min::Float64 = 1e-9\nmu_superlinear_decrease_power::Float64 = 1.5\ntau_min::Float64 = 0.99\nmu_linear_decrease_factor::Float64 = .2","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"","category":"page"},{"location":"options/#Linear-Solver-Options","page":"Options","title":"Linear Solver Options","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"Linear solver options are specific to the linear solver chosen at linear_solver option. Irrelevant options are ignored and a warning message is printed.","category":"page"},{"location":"options/#Ma27-(requires-MadNLPHSL)","page":"Options","title":"Ma27 (requires MadNLPHSL)","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"ma27_pivtol::Float64 = 1e-8\nma27_pivtolmax::Float64 = 1e-4\nma27_liw_init_factor::Float64 = 5.\nma27_la_init_factor::Float64 = 5.\nma27_meminc_factor::Float64 = 2.","category":"page"},{"location":"options/#Ma57-(requires-MadNLPHSL)","page":"Options","title":"Ma57 (requires MadNLPHSL)","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"ma57_pivtol::Float64 = 1e-8\nma57_pivtolmax::Float64 = 1e-4\nma57_pre_alloc::Float64 = 1.05\nma57_pivot_order::Int = 5\nma57_automatic_scaling::Bool = false\nma57_block_size::Int = 16\nma57_node_amalgamation::Int = 16\nma57_small_pivot_flag::Int = 0","category":"page"},{"location":"options/#Ma77-(requires-MadNLPHSL)","page":"Options","title":"Ma77 (requires MadNLPHSL)","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"ma77_buffer_lpage::Int = 4096\nma77_buffer_npage::Int = 1600\nma77_file_size::Int = 2097152\nma77_maxstore::Int = 0\nma77_nemin::Int = 8\nma77_order::Ma77.Ordering = Ma77.METIS\nma77_print_level::Int = -1\nma77_small::Float64 = 1e-20\nma77_static::Float64 = 0.\nma77_u::Float64 = 1e-8\nma77_umax::Float64 = 1e-4","category":"page"},{"location":"options/#Ma86-(requires-MadNLPHSL)","page":"Options","title":"Ma86 (requires MadNLPHSL)","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"ma86_num_threads::Int = 1\nma86_print_level::Float64 = -1\nma86_nemin::Int = 32\nma86_order::Ma86.Ordering = Ma86.METIS\nma86_scaling::Ma86.Scaling = Ma86.SCALING_NONE\nma86_small::Float64 = 1e-20\nma86_static::Float64 = 0.\nma86_u::Float64 = 1e-8\nma86_umax::Float64 = 1e-4","category":"page"},{"location":"options/#Ma97-(requires-MadNLPHSL)","page":"Options","title":"Ma97 (requires MadNLPHSL)","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"ma97_num_threads::Int = 1\nma97_print_level::Int = -1\nma97_nemin::Int = 8\nma97_order::Ma97.Ordering = Ma97.METIS\nma97_scaling::Ma97.Scaling = Ma97.SCALING_NONE\nma97_small::Float64 = 1e-20\nma97_u::Float64 = 1e-8\nma97_umax::Float64 = 1e-4","category":"page"},{"location":"options/#Mumps-(requires-MadNLPMumps)","page":"Options","title":"Mumps (requires MadNLPMumps)","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"mumps_dep_tol::Float64 = 0.\nmumps_mem_percent::Int = 1000\nmumps_permuting_scaling::Int = 7\nmumps_pivot_order::Int = 7\nmumps_pivtol::Float64 = 1e-6\nmumps_pivtolmax::Float64 = .1\nmumps_scaling::Int = 77","category":"page"},{"location":"options/#Umfpack-(requires-MadNLPUmfpack)","page":"Options","title":"Umfpack (requires MadNLPUmfpack)","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"umfpack_pivtol::Float64 = 1e-4\numfpack_pivtolmax::Float64 = 1e-1\numfpack_sym_pivtol::Float64 = 1e-3\numfpack_block_size::Float64 = 16\numfpack_strategy::Float64 = 2.","category":"page"},{"location":"options/#Pardiso-(requires-MadNLPPardiso)","page":"Options","title":"Pardiso (requires MadNLPPardiso)","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"pardiso_matching_strategy::Pardiso.MatchingStrategy = COMPLETE2x2\npardiso_max_inner_refinement_steps::Int = 1\npardiso_msglvl::Int = 0\npardiso_order::Int = 2","category":"page"},{"location":"options/#PardisoMKL","page":"Options","title":"PardisoMKL","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"pardisomkl_num_threads::Int = 1\npardiso_matching_strategy::PardisoMKL.MatchingStrategy = COMPLETE2x2\npardisomkl_max_iterative_refinement_steps::Int = 1\npardisomkl_msglvl::Int = 0\npardisomkl_order::Int = 2","category":"page"},{"location":"options/#LapackGPU-(requires-MadNLPGPU)","page":"Options","title":"LapackGPU (requires MadNLPGPU)","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"lapackgpu_algorithm::LapackGPU.Algorithms = BUNCHKAUFMAN","category":"page"},{"location":"options/#LapackCPU","page":"Options","title":"LapackCPU","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"lapackcpu_algorithm::LapackCPU.Algorithms = BUNCHKAUFMAN","category":"page"},{"location":"options/#Iterator-Options","page":"Options","title":"Iterator Options","text":"","category":"section"},{"location":"options/#Richardson","page":"Options","title":"Richardson","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"richardson_max_iter::Int = 10 \n  Maximum number of Richardson iteration steps. Valid range is 1infty).\nrichardson_tol::Float64 = 1e-10 \n  Convergence tolerance of Richardson iteration. Valid range is (0infty).\nrichardson_acceptable_tol::Float64 = 1e-5 \n  Acceptable convergence tolerance of Richardson iteration. If the Richardson iteration counter exceeds richardson_max_iter without satisfying the convergence criteria set with richardson_tol, the Richardson solver checks whether the acceptable convergence criteria set with richardson_acceptable_tol is satisfied; if the acceptable convergence criteria is satisfied, the computed step is used; otherwise, the augmented system is treated to be singular. Valid range is (0infty).","category":"page"},{"location":"options/#Krylov-(requires-MadNLPIterative)","page":"Options","title":"Krylov (requires MadNLPIterative)","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"krylov_max_iter::Int = 10 \n  Maximum number of Krylov iteration steps. Valid range is 1infty).\nkrylov_tol::Float64 = 1e-10 \n  Convergence tolerance of Krylov iteration. Valid range is (0infty).\nkrylov_acceptable_tol::Float64 = 1e-5 \n  Acceptable convergence tolerance of Krylov iteration. If the Krylov iteration counter exceeds krylov_max_iter without satisfying the convergence criteria set with krylov_tol, the Krylov solver checks whether the acceptable convergence criteria set with krylov_acceptable_tol is satisfied; if the acceptable convergence criteria is satisfied, the computed step is used; otherwise, the augmented system is treated to be singular. Valid range is (0infty).\nkrylov_restart::Int = 5 \n  Maximum Krylov iteration before restarting. Valid range is 1infty).","category":"page"},{"location":"options/#Reference","page":"Options","title":"Reference","text":"","category":"section"},{"location":"options/","page":"Options","title":"Options","text":"[Bunch, 1977]: J R Bunch and L Kaufman, Some stable methods for calculating inertia and solving symmetric linear systems, Mathematics of Computation 31:137 (1977), 163-179.","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"[Greif, 2014]: Greif, Chen, Erin Moulding, and Dominique Orban. \"Bounds on eigenvalues of matrices arising from interior-point methods.\" SIAM Journal on Optimization 24.1 (2014): 49-83.","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"[Wächter, 2006]: Wächter, Andreas, and Lorenz T. Biegler. \"On the implementation of an interior-point filter line-search algorithm for large-scale nonlinear programming.\" Mathematical programming 106.1 (2006): 25-57.","category":"page"},{"location":"options/","page":"Options","title":"Options","text":"[Chiang, 2016]: Chiang, Nai-Yuan, and Victor M. Zavala. \"An inertia-free filter line-search algorithm for large-scale nonlinear programming.\" Computational Optimization and Applications 64.2 (2016): 327-354.","category":"page"},{"location":"lib/callbacks/","page":"Callback wrappers","title":"Callback wrappers","text":"CurrentModule = MadNLP","category":"page"},{"location":"lib/callbacks/#Callbacks","page":"Callback wrappers","title":"Callbacks","text":"","category":"section"},{"location":"lib/callbacks/","page":"Callback wrappers","title":"Callback wrappers","text":"In MadNLP, a nonlinear program is implemented with a given AbstractNLPModel. The model may have a form unsuitable for the interior-point algorithm. For that reason, MadNLP wraps the AbstractNLPModel internally using custom data structures, encoded as a AbstractCallback. Depending on the setting, choose to wrap the AbstractNLPModel as a DenseCallback or alternatively, as a SparseCallback.","category":"page"},{"location":"lib/callbacks/","page":"Callback wrappers","title":"Callback wrappers","text":"AbstractCallback\nDenseCallback\nSparseCallback\n","category":"page"},{"location":"lib/callbacks/#MadNLP.AbstractCallback","page":"Callback wrappers","title":"MadNLP.AbstractCallback","text":"AbstractCallback{T, VT}\n\nWrap the AbstractNLPModel passed by the user in a form amenable to MadNLP.\n\nAn AbstractCallback handles the scaling of the problem and the reformulations of the equality constraints and fixed variables.\n\n\n\n\n\n","category":"type"},{"location":"lib/callbacks/#MadNLP.DenseCallback","page":"Callback wrappers","title":"MadNLP.DenseCallback","text":"DenseCallback{T, VT} < AbstractCallback{T, VT}\n\nWrap an AbstractNLPModel using dense structures.\n\n\n\n\n\n","category":"type"},{"location":"lib/callbacks/#MadNLP.SparseCallback","page":"Callback wrappers","title":"MadNLP.SparseCallback","text":"SparseCallback{T, VT} < AbstractCallback{T, VT}\n\nWrap an AbstractNLPModel using sparse structures.\n\n\n\n\n\n","category":"type"},{"location":"lib/callbacks/","page":"Callback wrappers","title":"Callback wrappers","text":"The function create_callback allows to instantiate a AbstractCallback from a given NLPModel:","category":"page"},{"location":"lib/callbacks/","page":"Callback wrappers","title":"Callback wrappers","text":"create_callback\n","category":"page"},{"location":"lib/callbacks/#MadNLP.create_callback","page":"Callback wrappers","title":"MadNLP.create_callback","text":"create_callback(\n    ::Type{Callback},\n    nlp::AbstractNLPModel{T, VT};\n    fixed_variable_treatment=MakeParameter,\n    equality_treatment=EnforceEquality,\n) where {T, VT}\n\nWrap the nonlinear program nlp using the callback wrapper with type Callback. The option fixed_variable_treatment decides if the fixed variables are relaxed (RelaxBound) or removed (MakeParameter). The option equality_treatment decides if the the equality constraints are keep as is (EnforceEquality) or relaxed (RelaxEquality).\n\n\n\n\n\n","category":"function"},{"location":"lib/callbacks/","page":"Callback wrappers","title":"Callback wrappers","text":"Internally, a AbstractCallback reformulates the inequality constraints as equality constraints by introducing additional slack variables. The fixed variables are reformulated as parameters (using MakeParameter) or are relaxed (using RelaxBound). The equality constraints can be keep as is with EnforceEquality (default option) or relaxed as inequality constraints with RelaxEquality. In that later case, MadNLP solves a relaxation of the original problem.","category":"page"},{"location":"lib/callbacks/","page":"Callback wrappers","title":"Callback wrappers","text":"AbstractFixedVariableTreatment\nMakeParameter\nRelaxBound\n\nAbstractEqualityTreatment\nEnforceEquality\nRelaxEquality","category":"page"},{"location":"lib/callbacks/#MadNLP.AbstractFixedVariableTreatment","page":"Callback wrappers","title":"MadNLP.AbstractFixedVariableTreatment","text":"AbstractFixedVariableTreatment\n\nAbstract type to define the reformulation of the fixed variables inside MadNLP.\n\n\n\n\n\n","category":"type"},{"location":"lib/callbacks/#MadNLP.MakeParameter","page":"Callback wrappers","title":"MadNLP.MakeParameter","text":"MakeParameter{VT, VI} <: AbstractFixedVariableTreatment\n\nRemove the fixed variables from the optimization variables and define them as problem's parameters.\n\n\n\n\n\n","category":"type"},{"location":"lib/callbacks/#MadNLP.RelaxBound","page":"Callback wrappers","title":"MadNLP.RelaxBound","text":"RelaxBound <: AbstractFixedVariableTreatment\n\nRelax the fixed variables x = x_fixed as bounded variables x_fixed - ϵ  x  x_fixed + ϵ, with ϵ a small-enough parameter.\n\n\n\n\n\n","category":"type"},{"location":"lib/callbacks/#MadNLP.AbstractEqualityTreatment","page":"Callback wrappers","title":"MadNLP.AbstractEqualityTreatment","text":"AbstractEqualityTreatment\n\nAbstract type to define the reformulation of the equality constraints inside MadNLP.\n\n\n\n\n\n","category":"type"},{"location":"lib/callbacks/#MadNLP.EnforceEquality","page":"Callback wrappers","title":"MadNLP.EnforceEquality","text":"EnforceEquality <: AbstractEqualityTreatment\n\nKeep the equality constraints intact.\n\nThe solution returned by MadNLP will respect the equality constraints.\n\n\n\n\n\n","category":"type"},{"location":"lib/callbacks/#MadNLP.RelaxEquality","page":"Callback wrappers","title":"MadNLP.RelaxEquality","text":"RelaxEquality <: AbstractEqualityTreatment\n\nRelax the equality constraints g(x) = 0 with two inequality constraints, as -ϵ  g(x)  ϵ. The parameter ϵ is usually small.\n\nThe solution returned by MadNLP will satisfy the equality constraints only up to a tolerance ϵ.\n\n\n\n\n\n","category":"type"},{"location":"lib/callbacks/","page":"Callback wrappers","title":"Callback wrappers","text":"MadNLP has to keep in memory all the indexes associated to the equality and inequality constraints. Similarly, MadNLP has to keep track of the indexes of the bounded variables and the fixed variables. MadNLP provides a utility get_index_constraints to import all the indexes required by MadNLP. Each index vector is encoded as a Vector{Int}.","category":"page"},{"location":"lib/callbacks/","page":"Callback wrappers","title":"Callback wrappers","text":"get_index_constraints\n","category":"page"},{"location":"lib/callbacks/#MadNLP.get_index_constraints","page":"Callback wrappers","title":"MadNLP.get_index_constraints","text":"get_index_constraints(nlp::AbstractNLPModel)\n\nAnalyze the bounds of the variables and the constraints in the AbstractNLPModel nlp. Return a named-tuple witht the following keys:return (\n\nind_eq: indices of equality constraints.\nind_ineq: indices of inequality constraints.\nind_fixed: indices of fixed variables.\nind_lb: indices of variables with a lower-bound.\nind_ub: indices of variables with an upper-bound.\nind_llb: indices of variables with only a lower-bound.\nind_uub: indices of variables with only an upper-bound.\n\n\n\n\n\n","category":"function"},{"location":"","page":"Home","title":"Home","text":"(Image: Logo)","category":"page"},{"location":"","page":"Home","title":"Home","text":"MadNLP is an open-source nonlinear programming solver, purely implemented in Julia. MadNLP implements a filter line-search interior-point algorithm, as used in Ipopt. MadNLP seeks to streamline the development of modeling and algorithmic paradigms in order to exploit structures and to make efficient use of high-performance computers.","category":"page"},{"location":"#Design","page":"Home","title":"Design","text":"","category":"section"},{"location":"#MadNLP's-problem-structure","page":"Home","title":"MadNLP's problem structure","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"MadNLP targets the resolution of constrained nonlinear problems, formulating as","category":"page"},{"location":"","page":"Home","title":"Home","text":"  beginaligned\n    min_x   f(x) \n    textsubject to quad  g_ell leq g(x) leq g_u \n                             x_ell leq x leq x_u\n  endaligned","category":"page"},{"location":"","page":"Home","title":"Home","text":"where x in mathbbR^n is the decision variable, f mathbbR^n to mathbbR and g mathbbR^n to mathbbR^m two nonlinear functions. MadNLP makes the distinction between the bound constraints x_ell leq x leq x_u and the generic constraints g_ell leq g(x) leq g_u. No other structure is assumed a priori.","category":"page"},{"location":"","page":"Home","title":"Home","text":"MadNLP is built on top of NLPModels.jl, a generic package to represent optimization models in Julia. In addition, MadNLP is interfaced with JuMP and Plasmo.","category":"page"},{"location":"","page":"Home","title":"Home","text":"note: Note\nMadNLP requires the evaluation of both the first-order and the second-order derivatives of the nonlinear problem.","category":"page"},{"location":"#MadNLP's-algorithm","page":"Home","title":"MadNLP's algorithm","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"MadNLP implements a primal-dual filter line-search interior-point algorithm, closely related to Ipopt. The nonlinear problem is reformulated in standard form by introducing a slack variables s in mathbbR^m to rewrite all the inequality constraints as equality constraints:","category":"page"},{"location":"","page":"Home","title":"Home","text":"  beginaligned\n    min_x s   f(x) \n    textsubject to quad  g(x) - s = 0  \n                             g_ell leq s leq g_u \n                             x_ell leq x leq x_u\n  endaligned","category":"page"},{"location":"","page":"Home","title":"Home","text":"The algorithm starts from an initial primal-dual iterate (x_0 s_0 y_0). Then, at each iteration the current iterate is updated by solving the KKT linear system:","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginbmatrix\n    W_k + Sigma_x  0  A_k^top \n    0  Sigma_s  - I \n    A_k  -I  0\nendbmatrix\nbeginbmatrix\n    Delta x  Delta s  Delta y\nendbmatrix\n=\n-\nbeginbmatrix\n    nabla f(x_k) + A_k^top y_k - mu U^-1 e_n \n    y_k - mu S^-1 e_n \n    g(x_k) - s_k\nendbmatrix\n","category":"page"},{"location":"","page":"Home","title":"Home","text":"with mu being the current barrier parameter.","category":"page"},{"location":"","page":"Home","title":"Home","text":"We call the linear system the augmented KKT system at iteration k.","category":"page"},{"location":"#MadNLP's-performance","page":"Home","title":"MadNLP's performance","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"In any interior-point algorithm, the two computational bottlenecks are","category":"page"},{"location":"","page":"Home","title":"Home","text":"The evaluation of the first- and second-order derivatives.\nThe factorization of the augmented KKT system.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The first point is problem dependent, and often related to the automatic differentiation backend being employed. The second point is more problematic, as the augmented KKT system is usually symmetric indefinite and ill-conditionned. For that reason we recommend using efficient sparse-linear solvers (HSL, Mumps, Pardiso) if they are available to the user.","category":"page"},{"location":"#Citing-MadNLP.jl","page":"Home","title":"Citing MadNLP.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you use MadNLP.jl in your research, we would greatly appreciate your citing it.","category":"page"},{"location":"","page":"Home","title":"Home","text":"@article{shin2023accelerating,\n  title={Accelerating optimal power flow with {GPU}s: {SIMD} abstraction of nonlinear programs and condensed-space interior-point methods},\n  author={Shin, Sungho and Pacaud, Fran{\\c{c}}ois and Anitescu, Mihai},\n  journal={arXiv preprint arXiv:2307.16830},\n  year={2023}\n}\n@article{shin2020graph,\n  title={Graph-Based Modeling and Decomposition of Energy Infrastructures},\n  author={Shin, Sungho and Coffrin, Carleton and Sundar, Kaarthik and Zavala, Victor M},\n  journal={arXiv preprint arXiv:2010.02404},\n  year={2020}\n}","category":"page"}]
}
