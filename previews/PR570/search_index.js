var documenterSearchIndex = {"docs":
[{"location":"tutorials/gpu/#Running-MadNLP-on-the-GPU","page":"GPU acceleration","title":"Running MadNLP on the GPU","text":"MadNLP supports the solution of large-scale optimization problems on the GPU, with significant speedups reported on some instances. In this tutorial, we show how to solve a nonlinear program on a NVIDIA GPU. We start by importing MadNLPGPU:\n\nusing MadNLPGPU\nusing CUDA\n","category":"section"},{"location":"tutorials/gpu/#How-to-solve-a-problem-on-the-GPU-with-ExaModels?","page":"GPU acceleration","title":"How to solve a problem on the GPU with ExaModels?","text":"MadNLP has been designed to run entirely on the GPU, without data exchange between the host and the device. We recommend implementing the model with the modeler ExaModels whenever possible.\n\nWe import ExaModels as:\n\nusing ExaModels\n\ninfo: Info\nMadNLP does not yet support solving sparse optimization problems on AMD GPUs.","category":"section"},{"location":"tutorials/gpu/#Evaluating-the-model-on-the-GPU-with-ExaModels","page":"GPU acceleration","title":"Evaluating the model on the GPU with ExaModels","text":"The first step requires implementing the model with ExaModels.\n\nAs a demonstration, we implement the model airport from the CUTEst benchmark using ExaModels. The code writes:\n\nfunction airport_model(T, backend)\n    N = 42\n    # Data\n    r = T[0.09 , 0.3, 0.09, 0.45, 0.5, 0.04, 0.1, 0.02, 0.02, 0.07, 0.4, 0.045, 0.05, 0.056, 0.36, 0.08, 0.07, 0.36, 0.67, 0.38, 0.37, 0.05, 0.4, 0.66, 0.05, 0.07, 0.08, 0.3, 0.31, 0.49, 0.09, 0.46, 0.12, 0.07, 0.07, 0.09, 0.05, 0.13, 0.16, 0.46, 0.25, 0.1]\n    cx = T[-6.3, -7.8, -9.0, -7.2, -5.7, -1.9, -3.5, -0.5, 1.4, 4.0, 2.1, 5.5, 5.7, 5.7, 3.8, 5.3, 4.7, 3.3, 0.0, -1.0, -0.4, 4.2, 3.2, 1.7, 3.3, 2.0, 0.7, 0.1, -0.1, -3.5, -4.0, -2.7, -0.5, -2.9, -1.2, -0.4, -0.1, -1.0, -1.7, -2.1, -1.8, 0.0]\n    cy = T[8.0, 5.1, 2.0, 2.6, 5.5, 7.1, 5.9, 6.6, 6.1, 5.6, 4.9, 4.7, 4.3, 3.6, 4.1, 3.0, 2.4, 3.0, 4.7, 3.4, 2.3, 1.5, 0.5, -1.7, -2.0, -3.1, -3.5, -2.4, -1.3, 0.0, -1.7, -2.1, -0.4, -2.9, -3.4, -4.3, -5.2, -6.5, -7.5, -6.4, -5.1, 0.0]\n    # Wrap all data in a single iterator for ExaModels\n    data = [(i, cx[i], cy[i], r[i]) for i in 1:N]\n    IJ = [(i, j) for i in 1:N-1 for j in i+1:N]\n    # Write model using ExaModels\n    core = ExaModels.ExaCore(T; backend=backend)\n    x = ExaModels.variable(core, 1:N, lvar = -10.0, uvar=10.0)\n    y = ExaModels.variable(core, 1:N, lvar = -10.0, uvar=10.0)\n    ExaModels.objective(\n        core,\n        ((x[i] - x[j])^2 + (y[i] - y[j])^2) for (i, j) in IJ\n    )\n    ExaModels.constraint(core, (x[i]-dcx)^2 + (y[i] - dcy)^2 - dr for (i, dcx, dcy, dr) in data; lcon=-Inf)\n    return ExaModels.ExaModel(core)\nend\n\nThe first argument T specifies the numerical precision (Float32, Float64 or any AbstractFloat) whereas the second argument backend sets the device used to evaluate the model. We instantiate the model on the GPU with:\n\nnlp = airport_model(Float64, CUDABackend())\n\nBy passing a CUDABackend(), we make sure that all the attributes in nlp are instantiated on the GPU. E.g., the initial point becomes a CuVector:\n\nNLPModels.get_x0(nlp)\n\ninfo: Info\nIf your problem is implemented with JuMP in a given model, ExaModel can load it for you on the GPU just by using nlp = ExaModel(model; backend=CUDABackend()).","category":"section"},{"location":"tutorials/gpu/#Solving-the-problem-on-the-GPU-with-MadNLP","page":"GPU acceleration","title":"Solving the problem on the GPU with MadNLP","text":"Once the model nlp loaded on the GPU, you can solve it using the function madnlp:\n\nresults = madnlp(nlp; linear_solver=CUDSSSolver)\nnothing\n\nNote that we are using the sparse linear solver NVIDIA cuDSS to solve the primal-dual KKT system entirely on the GPU.\n\nWhen solving an optimization problem on the GPU, MadNLP proceeds to some automatic modifications. In order:\n\nIt increases the parameter bound_relax_factor to 1e-4.\nIt relaxes all the equality constraints g(x) = 0 as a pair of inequality constraints -tau leq g(x) leq tau, with tau being equal to bound_relax_factor;\nIt reduces the KKT system down to sparse condensed system, exploiting the fact that the relaxed problem has only (potentially tight) inequality constraints. Up to a given primal regularization, the resulting KKT system is positive definite and can be factorized using a pivoting-free factorization routines (e.g. a Cholesky or an LDL' decompositions). This is the so-called Lifted-KKT formulation documented in this article.\nThe new condensed KKT system increases the ill-conditioning inherent to the interior-point method, amplifying the loss of accuracy when the iterates get close to a local solution. As a result, the termination tolerance tol is also increased to 1e-4 to recover convergence.\n\nAs a result, the convergence observed can be significantly different than what we observe on the CPU. In particular, relaxing the parameter bound_relax_factor has a non-marginal impact on the feasible set's geometry. You can limit the loss of accuracy by specifying explicitly the relaxation and tolerance parameters to MadNLP:\n\nresults = madnlp(\n    nlp;\n    tol=1e-7,\n    bound_relax_factor=1e-7,\n    linear_solver=CUDSSSolver,\n)\nnothing\n\nDecreasing the tolerance tol too much is likely to cause numerical issues inside the algorithm. In general, we recommend keeping the value of bound_relax_factor below tol.","category":"section"},{"location":"tutorials/gpu/#Solving-the-problem-on-the-GPU-with-HyKKT","page":"GPU acceleration","title":"Solving the problem on the GPU with HyKKT","text":"Some applications require accurate solutions. In that case, we recommend using the extension HybridKKT.jl, which implements the Golub & Greif augmented Lagrangian formulation detailed in this article. Compared to Lifted-KKT, the Hybrid-KKT strategy is more accurate (it doesn't relax the equality constraints in the problem) but slightly slower (it computes the descent direction using a conjugate gradient at every IPM iterations).\n\nOnce the package HybridKKT installed, the solution proceeds as\n\nusing HybridKKT\n\nresults = madnlp(\n    nlp;\n    linear_solver=MadNLPGPU.CUDSSSolver,\n    kkt_system=HybridKKT.HybridCondensedKKTSystem,\n    equality_treatment=MadNLP.EnforceEquality,\n    fixed_variable_treatment=MadNLP.MakeParameter,\n)","category":"section"},{"location":"tutorials/gpu/#How-to-solve-on-the-GPU-a-legacy-model-running-on-the-CPU?","page":"GPU acceleration","title":"How to solve on the GPU a legacy model running on the CPU?","text":"If your optimization problem is not instantiated on the GPU, you can still solve it on the GPU by wrapping your model in a SparseWrapperModel. Oftentimes, this is the most convenient solution if your problem does not formulate easilly with ExaModels. In that case, the evaluation of the model runs on the CPU, but all MadNLP's internals are instantiated on the GPU (including the sparse linear solver).\n\nTo give an example, we suppose we have instantiate our airport model on the CPU:\n\nnlp_cpu = airport_model(Float64, nothing)\n\nYou can wrap it on the GPU using:\n\nnlp_wrapped = MadNLP.SparseWrapperModel(CuArray, nlp_cpu)\n\nThen, we proceed as above to call MadNLP:\n\nresults = madnlp(nlp_wrapped; linear_solver=CUDSSSolver)\nnothing","category":"section"},{"location":"tutorials/gpu/#How-to-solve-on-the-GPU-a-JuMP-model?","page":"GPU acceleration","title":"How to solve on the GPU a JuMP model?","text":"If your model is implemented with JuMP, you can solve it on the GPU using the same trick as in the previous paragraph just by passing a new option array_type specifying which array type to use to instantiate the SparseWrapperModel.\n\nThis is demonstrated in the following example:\n\nusing CUDA\nusing JuMP\nusing MadNLP, MadNLPGPU\n\nmodel = Model(MadNLP.Optimizer)\n@variable(model, x1 <= 0.5)\n@variable(model, x2)\n\n@objective(model, Min, 100.0 * (x2 - x1^2)^2 + (1.0 - x1)^2)\n@constraint(model, x1 * x2 >= 1.0)\n@constraint(model, x1 + x2^2 >= 0.0)\n\nJuMP.set_optimizer_attribute(model, \"array_type\", CuArray)\nJuMP.set_optimizer_attribute(model, \"linear_solver\", CUDSSSolver)\nJuMP.optimize!(model) # Solve the model on the GPU!","category":"section"},{"location":"installation/#Installation","page":"Installation","title":"Installation","text":"To install MadNLP, simply proceed to\n\npkg> add MadNLP\n\nBy default, MadNLP uses the sparse linear solver MUMPS. In addition the user can install the following extensions to use a specialized linear solver.","category":"section"},{"location":"installation/#GPU-support","page":"Installation","title":"GPU support","text":"MadNLPGPU provides GPU support for MadNLP:\n\npkg> add MadNLPGPU\n\nMadNLPGPU automatically detects if you have a NVIDIA or an AMD GPU on your machine, and adapts in consequence.","category":"section"},{"location":"installation/#HSL-linear-solver","page":"Installation","title":"HSL linear solver","text":"Obtain a license and download HSL_jll.jl. Install the package into your current environment using:\n\nimport Pkg\nPkg.develop(path = \"/full/path/to/HSL_jll.jl\")\n\nIf the user has already compiled the HSL solver library, one can simply override the path to the artifact by editing ~/.julia/artifacts/Overrides.toml\n\n# replace HSL_jll artifact /usr/local/lib/libhsl.so\necece3e2c69a413a0e935cf52e03a3ad5492e137 = \"/usr/local\"\n\nOnce HSL_jll has been installed on your machine, you can install MadNLPHSL as\n\npkg> add MadNLPHSL","category":"section"},{"location":"installation/#Pardiso-linear-solver","page":"Installation","title":"Pardiso linear solver","text":"To use Pardiso, the user needs to obtain the Pardiso shared libraries from https://panua.ch/, provide the absolute path to the shared library:\n\njulia> ENV[\"MADNLP_PARDISO_LIBRARY_PATH\"] = \"/usr/lib/libpardiso600-GNU800-X86-64.so\"\n\nand place the license file in the home directory. After obtaining the library and the license file, run\n\npkg> add MadNLPPardiso\npkg> build MadNLPPardiso\n\nThe build process requires a C compiler.","category":"section"},{"location":"man/kkt/#KKT-systems","page":"KKT systems","title":"KKT systems","text":"KKT systems are linear system with a special KKT structure. MadNLP uses a special structure AbstractKKTSystem to represent internally the KKT system. The AbstractKKTSystem fulfills two goals:\n\nStore the values of the Hessian of the Lagrangian and of the Jacobian.\nAssemble the corresponding KKT matrix K.","category":"section"},{"location":"man/kkt/#A-brief-look-at-the-math","page":"KKT systems","title":"A brief look at the math","text":"We have seen previously that MadNLP reformulates the inequality constraints as equality constraints, by introducing slack variables. In what follows, we denote by w = (x s) the primal iterate concatenating the original decision variable x and the slack variable s. Once reformulated, the problem has the following structure:\n\n  beginaligned\n    min_w   f(w)   \n    textsubject to quad  c(w) = 0   quad w geq 0  \n  endaligned\n\nAt each iteration, MadNLP aims at solving the following system of nonlinear equations, parameterized by a positive barrier parameter mu:\n\nbeginaligned\n nabla f(w) + nabla c(w)^top y - z = 0   \n c(w) = 0   \n WZe = mu e   (w z)  0  \nendaligned\n\nMadNLP solves this system using the Newton method, and computes a descent direction (Delta w Delta y Delta z) as solution of\n\nbeginbmatrix\nH_k +delta_x I  J_k^top  -I \nJ_k  -delta_y  0 \nZ_k  0  W_k\nendbmatrix\nbeginbmatrix\nDelta x  Delta y  Delta z\nendbmatrix\n= -\nbeginbmatrix\n nabla f(w_k) + nabla c(w_k)^top y_k - z_k \n c(w_k)  \n W_k Z_k e - mu e\nendbmatrix  \n\nwith delta_x and delta_y appropriate primal-dual regularization terms that guarantee (Delta x Delta y Delta z) is a descent direction for the current filter. The system needs evaluating the Jacobian of the constraints J_k = nabla c(w_k)^top and the Hessian of the Lagrangian H_k = nabla_xx^2 L(w_k y_k z_k).\n\nWe note by (r_1 r_2 r_3) the right-hand-side. The primal-dual KKT system can be symmetrized as\n\nbeginbmatrix\nH_k +delta_x I  J_k^top  Z_k^12 \nJ_k  -delta_y  0 \nZ_k^12  0  -W_k\nendbmatrix\nbeginbmatrix\nDelta x  Delta y  -Z_k^12 Delta z\nendbmatrix\n=\nbeginbmatrix\nr_1  r_2  Z_k^-12 r_3\nendbmatrix\n\nThis system is implemented as an AbstractUnreducedKKTSystem in MadNLP.\n\nWe can obtain a smaller augmented KKT system by eliminating the last block of rows associated to Delta z = W_k^-1 (r_3 - Z_k Delta x):\n\nbeginbmatrix\nH_k + Sigma_k + delta_x I  J_k^top \nJ_k  -delta_y\nendbmatrix\nbeginbmatrix\nDelta x  Delta y\nendbmatrix\n=\nbeginbmatrix\nr_1 + W_k^-1 r_3  r_2\nendbmatrix\n\nwith Sigma_k = W_k^-1 Z_k. This system is implemented as an AbstractReducedKKTSystem in MadNLP.","category":"section"},{"location":"man/kkt/#Assembling-a-KKT-system,-step-by-step","page":"KKT systems","title":"Assembling a KKT system, step by step","text":"The primal-dual KKT systems depend on the Hessian of the Lagrangian H_k and the Jacobian J_k. Hence, we have to update the values in the KKT system at each iteration of the interior-point algorithm.\n\nBy default, MadNLP stores the KKT system as a SparseKKTSystem. The KKT system takes as input a SparseCallback wrapping a given NLPModel nlp. We instantiate the callback cb with the function create_callback:\n\ncb = MadNLP.create_callback(\n    MadNLP.SparseCallback,\n    nlp,\n)\n","category":"section"},{"location":"man/kkt/#Initializing-a-KKT-system","page":"KKT systems","title":"Initializing a KKT system","text":"The size of the KKT system depends directly on the problem's characteristics (number of variables, number of of equality and inequality constraints). A SparseKKTSystem stores the Hessian and the Jacobian in sparse (COO) format. The KKT matrix can be factorized using either a dense or a sparse linear solvers. Here we use the solver provided in Lapack:\n\nlinear_solver = LapackCPUSolver\n\nWe can instantiate a SparseKKTSystem using the function create_kkt_system:\n\nkkt = MadNLP.create_kkt_system(\n    MadNLP.SparseKKTSystem,\n    cb,\n    linear_solver,\n)\n\n\nOnce the KKT system built, one has to initialize it to use it inside the interior-point algorithm:\n\nMadNLP.initialize!(kkt);\n\n\nThe user can query the KKT matrix inside kkt, simply as\n\nkkt_matrix = MadNLP.get_kkt(kkt)\n\nThis returns a reference to the KKT matrix stores internally inside kkt. Each time the matrix is assembled inside kkt, kkt_matrix is updated automatically.","category":"section"},{"location":"man/kkt/#Updating-a-KKT-system","page":"KKT systems","title":"Updating a KKT system","text":"We suppose now we want to refresh the values stored in the KKT system.","category":"section"},{"location":"man/kkt/#Updating-the-values-of-the-Hessian","page":"KKT systems","title":"Updating the values of the Hessian","text":"The Hessian part of the KKT system can be queried as\n\nhess_values = MadNLP.get_hessian(kkt)\n\n\nFor a SparseKKTSystem, hess_values is a Vector{Float64} storing the nonzero values of the Hessian. Then, one can update the vector hess_values by using NLPModels.jl:\n\nn = NLPModels.get_nvar(nlp)\nm = NLPModels.get_ncon(nlp)\nx = NLPModels.get_x0(nlp) # primal variables\nl = zeros(m) # dual variables\n\nNLPModels.hess_coord!(nlp, x, l, hess_values)\n\n\nEventually, a post-processing step is applied to refresh all the values internally:\n\nMadNLP.compress_hessian!(kkt)\n\n\nnote: Note\nBy default, the function compress_hessian! does nothing. But it can be required for very specific use-case, for instance building internally a Schur complement matrix.","category":"section"},{"location":"man/kkt/#Updating-the-values-of-the-Jacobian","page":"KKT systems","title":"Updating the values of the Jacobian","text":"We proceed exaclty the same way to update the values in the Jacobian. One queries the Jacobian values in the KKT system as\n\njac_values = MadNLP.get_jacobian(kkt)\n\n\nWe can refresh the values with NLPModels as\n\nNLPModels.jac_coord!(nlp, x, jac_values)\n\n\nAnd then applies a post-processing step as\n\nMadNLP.compress_jacobian!(kkt)\n","category":"section"},{"location":"man/kkt/#Updating-the-values-of-the-diagonal-matrices","page":"KKT systems","title":"Updating the values of the diagonal matrices","text":"Once the Hessian and the Jacobian updated, the algorithm can apply primal and dual regularization terms on the diagonal of the KKT system, to improve the numerical behavior in the linear solver. This operation is implemented inside the regularize_diagonal! function:\n\npr_value = 1.0\ndu_value = 0.0\n\nMadNLP.regularize_diagonal!(kkt, pr_value, du_value)\n","category":"section"},{"location":"man/kkt/#Assembling-the-KKT-matrix","page":"KKT systems","title":"Assembling the KKT matrix","text":"Once the values updated, one can assemble the resulting KKT matrix. This translates to\n\nMadNLP.build_kkt!(kkt)\n\nBy doing so, the values stored inside kkt will be transferred to the KKT matrix kkt_matrix (as returned by the function get_kkt):\n\nkkt_matrix\n\nInternally, a SparseKKTSystem stores the KKT system in a sparse COO format. When build_kkt! is called, the sparse COO matrix is transferred to SparseMatrixCSC if the linear solver is sparse, or alternatively to a Matrix if the linear solver is dense.\n\nnote: Note\nThe KKT system stores only the lower-triangular part of the KKT system, as it is symmetric.","category":"section"},{"location":"man/kkt/#Solution-of-the-KKT-system","page":"KKT systems","title":"Solution of the KKT system","text":"Now the KKT system is assembled in a matrix K (here stored in kkt_matrix), we want to solve a linear system K x = b, for instance to evaluate the next descent direction. To do so, we use the linear solver stored internally inside kkt (here an instance of LapackCPUSolver).\n\nWe start by factorizing the KKT matrix K:\n\nMadNLP.factorize!(kkt.linear_solver)\n\n\nBy default, MadNLP uses a LBL factorization to decompose the symmetric indefinite KKT matrix.\n\nOnce the KKT matrix has been factorized, we can compute the solution of the linear system with a backsolve. The function takes as input a AbstractKKTVector, an object used to do algebraic manipulation with a AbstractKKTSystem. We start by instantiating two UnreducedKKTVector (encoding respectively the right-hand-side and the solution):\n\nb = MadNLP.UnreducedKKTVector(kkt)\nfill!(MadNLP.full(b), 1.0)\nx = copy(b)\n\n\nThe right-hand-side encodes a vector of 1:\n\nMadNLP.full(b)\n\nWe solve the system K x = b using the solve! function:\n\nMadNLP.solve!(kkt, x)\nMadNLP.full(x)\n\nWe verify that the solution is correct by multiplying it on the left with the KKT system, using mul!:\n\nmul!(b, kkt, x) # overwrite b!\nMadNLP.full(b)\n\nWe recover a vector filled with 1, which was the initial right-hand-side.","category":"section"},{"location":"lib/linear_solvers/#Linear-solvers","page":"Linear Solvers","title":"Linear solvers","text":"","category":"section"},{"location":"lib/linear_solvers/#Direct-linear-solvers","page":"Linear Solvers","title":"Direct linear solvers","text":"Each linear solver employed in MadNLP implements the following interface.","category":"section"},{"location":"lib/linear_solvers/#Iterative-refinement","page":"Linear Solvers","title":"Iterative refinement","text":"MadNLP uses iterative refinement to improve the accuracy of the solution returned by the linear solver.","category":"section"},{"location":"lib/linear_solvers/#MadNLP.AbstractLinearSolver","page":"Linear Solvers","title":"MadNLP.AbstractLinearSolver","text":"AbstractLinearSolver\n\nAbstract type for linear solver targeting the resolution of the linear system Ax=b.\n\n\n\n\n\n","category":"type"},{"location":"lib/linear_solvers/#MadNLP.introduce","page":"Linear Solvers","title":"MadNLP.introduce","text":"introduce(::AbstractLinearSolver)\n\nPrint the name of the linear solver.\n\n\n\n\n\n","category":"function"},{"location":"lib/linear_solvers/#MadNLP.factorize!","page":"Linear Solvers","title":"MadNLP.factorize!","text":"factorize!(::AbstractLinearSolver)\n\nFactorize the matrix A and updates the factors inside the AbstractLinearSolver instance.\n\n\n\n\n\n","category":"function"},{"location":"lib/linear_solvers/#MadNLP.solve!","page":"Linear Solvers","title":"MadNLP.solve!","text":"solve!(::AbstractLinearSolver, x::AbstractVector)\n\nSolve the linear system Ax = b.\n\nThis function assumes the linear system has been factorized previously with factorize!.\n\n\n\n\n\n","category":"function"},{"location":"lib/linear_solvers/#MadNLP.is_inertia","page":"Linear Solvers","title":"MadNLP.is_inertia","text":"is_inertia(::AbstractLinearSolver)\n\nReturn true if the linear solver supports the computation of the inertia of the linear system.\n\n\n\n\n\n","category":"function"},{"location":"lib/linear_solvers/#MadNLP.inertia","page":"Linear Solvers","title":"MadNLP.inertia","text":"inertia(::AbstractLinearSolver)\n\nReturn the inertia (n, m, p) of the linear system as a tuple.\n\nNote\n\nThe inertia is defined as a tuple (n m p), with\n\nn: number of positive eigenvalues\nm: number of negative eigenvalues\np: number of zero eigenvalues\n\n\n\n\n\n","category":"function"},{"location":"lib/linear_solvers/#MadNLP.solve_refine!","page":"Linear Solvers","title":"MadNLP.solve_refine!","text":"solve_refine!(x::VT, ::AbstractIterator, b::VT, w::VT) where {VT <: AbstractKKTVector}\n\nSolve the linear system Ax = b using iterative refinement. The object AbstractIterator stores an instance of a AbstractLinearSolver for the backsolve operations.\n\nNotes\n\nThis function assumes the matrix stored in the linear solver has been factorized previously.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#KKT-systems","page":"KKT systems","title":"KKT systems","text":"MadNLP manipulates KKT systems using two abstractions: an AbstractKKTSystem storing the KKT system' matrix and an AbstractKKTVector storing the KKT system's right-hand-side.","category":"section"},{"location":"lib/kkt/#AbstractKKTSystem","page":"KKT systems","title":"AbstractKKTSystem","text":"MadNLP implements three different types of AbstractKKTSystem, depending how far we reduce the KKT system.\n\nEach AbstractKKTSystem follows the interface described below:","category":"section"},{"location":"lib/kkt/#Sparse-KKT-systems","page":"KKT systems","title":"Sparse KKT systems","text":"By default, MadNLP stores a AbstractReducedKKTSystem in sparse format, as implemented by SparseKKTSystem:\n\nThe user has the alternative choices:","category":"section"},{"location":"lib/kkt/#Dense-KKT-systems","page":"KKT systems","title":"Dense KKT systems","text":"MadNLP provides also two structures to store the KKT system in a dense matrix. Although less efficient than their sparse counterparts, these two structures allow to store the KKT system efficiently when the problem is instantiated on the GPU.","category":"section"},{"location":"lib/kkt/#AbstractKKTVector","page":"KKT systems","title":"AbstractKKTVector","text":"Each instance of AbstractKKTVector implements the following interface.\n\nBy default, MadNLP provides one implementation of an AbstractKKTVector.","category":"section"},{"location":"lib/kkt/#MadNLP.AbstractKKTSystem","page":"KKT systems","title":"MadNLP.AbstractKKTSystem","text":"AbstractKKTSystem{T, VT<:AbstractVector{T}, MT<:AbstractMatrix{T}, QN<:AbstractHessian{T}}\n\nAbstract type for KKT system.\n\n\n\n\n\n","category":"type"},{"location":"lib/kkt/#MadNLP.AbstractUnreducedKKTSystem","page":"KKT systems","title":"MadNLP.AbstractUnreducedKKTSystem","text":"AbstractUnreducedKKTSystem{T, VT, MT, QN} <: AbstractKKTSystem{T, VT, MT, QN}\n\nAugmented KKT system associated to the linearization of the KKT conditions at the current primal-dual iterate (x s y z ν w).\n\nThe associated matrix is\n\n[Wₓₓ  0   Aₑ'  Aᵢ' V½  0  ]  [Δx]\n[0    0   0   -I   0   W½ ]  [Δs]\n[Aₑ   0   0    0   0   0  ]  [Δy]\n[Aᵢ  -I   0    0   0   0  ]  [Δz]\n[V½   0   0    0  -X   0  ]  [Δτ]\n[0    W½  0    0   0  -S  ]  [Δρ]\n\nwith\n\nWₓₓ: Hessian of the Lagrangian.\nAₑ: Jacobian of the equality constraints\nAᵢ: Jacobian of the inequality constraints\nX = diag(x)\nS = diag(s)\nV = diag(ν)\nW = diag(w)\nΔτ = -W^-½Δν\nΔρ = -W^-½Δw\n\n\n\n\n\n","category":"type"},{"location":"lib/kkt/#MadNLP.AbstractReducedKKTSystem","page":"KKT systems","title":"MadNLP.AbstractReducedKKTSystem","text":"AbstractReducedKKTSystem{T, VT, MT, QN} <: AbstractKKTSystem{T, VT, MT, QN}\n\nThe reduced KKT system is a simplification of the original Augmented KKT system. Comparing to AbstractUnreducedKKTSystem), AbstractReducedKKTSystem removes the two last rows associated to the bounds' duals (ν w).\n\nAt a primal-dual iterate (x s y z), the matrix writes\n\n[Wₓₓ + Σₓ   0    Aₑ'   Aᵢ']  [Δx]\n[0          Σₛ    0    -I ]  [Δs]\n[Aₑ         0     0     0 ]  [Δy]\n[Aᵢ        -I     0     0 ]  [Δz]\n\nwith\n\nWₓₓ: Hessian of the Lagrangian.\nAₑ: Jacobian of the equality constraints\nAᵢ: Jacobian of the inequality constraints\nΣₓ = X¹ V\nΣₛ = S¹ W\n\n\n\n\n\n","category":"type"},{"location":"lib/kkt/#MadNLP.AbstractCondensedKKTSystem","page":"KKT systems","title":"MadNLP.AbstractCondensedKKTSystem","text":"AbstractCondensedKKTSystem{T, VT, MT, QN} <: AbstractKKTSystem{T, VT, MT, QN}\n\nThe condensed KKT system simplifies further the AbstractReducedKKTSystem by removing the rows associated to the slack variables s and the inequalities.\n\nAt the primal-dual iterate (x y), the matrix writes\n\n[Wₓₓ + Σₓ + Aᵢ' Σₛ Aᵢ    Aₑ']  [Δx]\n[         Aₑ              0 ]  [Δy]\n\nwith\n\nWₓₓ: Hessian of the Lagrangian.\nAₑ: Jacobian of the equality constraints\nAᵢ: Jacobian of the inequality constraints\nΣₓ = X¹ V\nΣₛ = S¹ W\n\n\n\n\n\n","category":"type"},{"location":"lib/kkt/#MadNLP.create_kkt_system","page":"KKT systems","title":"MadNLP.create_kkt_system","text":"create_kkt_system(\n    ::Type{KKT},\n    cb::AbstractCallback,\n    ind_cons::NamedTuple,\n    linear_solver::Type{LinSol};\n    opt_linear_solver=default_options(linear_solver),\n    hessian_approximation=ExactHessian,\n) where {KKT<:AbstractKKTSystem, LinSol<:AbstractLinearSolver}\n\nInstantiate a new KKT system with type KKT, associated to the the nonlinear program encoded inside the callback cb. The NamedTuple ind_cons stores the indexes of all the variables and constraints in the callback cb. In addition, the user should pass the linear solver linear_solver that will be used to solve the KKT system after it has been assembled.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.num_variables","page":"KKT systems","title":"MadNLP.num_variables","text":"Number of primal variables (including slacks) associated to the KKT system.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.get_kkt","page":"KKT systems","title":"MadNLP.get_kkt","text":"get_kkt(kkt::AbstractKKTSystem)::AbstractMatrix\n\nReturn a pointer to the KKT matrix implemented in kkt. The pointer is passed afterward to a linear solver.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.get_jacobian","page":"KKT systems","title":"MadNLP.get_jacobian","text":"Get Jacobian matrix\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.get_hessian","page":"KKT systems","title":"MadNLP.get_hessian","text":"Get Hessian matrix\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.initialize!","page":"KKT systems","title":"MadNLP.initialize!","text":"initialize!(kkt::AbstractKKTSystem)\n\nInitialize KKT system with default values. Called when we initialize the MadNLPSolver storing the current KKT system kkt.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.build_kkt!","page":"KKT systems","title":"MadNLP.build_kkt!","text":"build_kkt!(kkt::AbstractKKTSystem)\n\nAssemble the KKT matrix before calling the factorization routine.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.compress_hessian!","page":"KKT systems","title":"MadNLP.compress_hessian!","text":"compress_hessian!(kkt::AbstractKKTSystem)\n\nCompress the Hessian inside kkt's internals. This function is called every time a new Hessian is evaluated.\n\nDefault implementation do nothing.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.compress_jacobian!","page":"KKT systems","title":"MadNLP.compress_jacobian!","text":"compress_jacobian!(kkt::AbstractKKTSystem)\n\nCompress the Jacobian inside kkt's internals. This function is called every time a new Jacobian is evaluated.\n\nBy default, the function updates in the Jacobian the coefficients associated to the slack variables.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.jtprod!","page":"KKT systems","title":"MadNLP.jtprod!","text":"jtprod!(y::AbstractVector, kkt::AbstractKKTSystem, x::AbstractVector)\n\nMultiply with transpose of Jacobian and store the result in y, such that y = A x (with A current Jacobian).\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.regularize_diagonal!","page":"KKT systems","title":"MadNLP.regularize_diagonal!","text":"regularize_diagonal!(kkt::AbstractKKTSystem, primal_values::Number, dual_values::Number)\n\nRegularize the values in the diagonal of the KKT system in an incremental fashion. Called internally inside the interior-point routine.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.is_inertia_correct","page":"KKT systems","title":"MadNLP.is_inertia_correct","text":"is_inertia_correct(kkt::AbstractKKTSystem, n::Int, m::Int, p::Int)\n\nCheck if the inertia (n m p) returned by the linear solver is adapted to the KKT system implemented in kkt.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.nnz_jacobian","page":"KKT systems","title":"MadNLP.nnz_jacobian","text":"Nonzero in Jacobian\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.SparseKKTSystem","page":"KKT systems","title":"MadNLP.SparseKKTSystem","text":"SparseKKTSystem{T, VT, MT, QN} <: AbstractReducedKKTSystem{T, VT, MT, QN}\n\nImplement the AbstractReducedKKTSystem in sparse COO format.\n\n\n\n\n\n","category":"type"},{"location":"lib/kkt/#MadNLP.SparseUnreducedKKTSystem","page":"KKT systems","title":"MadNLP.SparseUnreducedKKTSystem","text":"SparseUnreducedKKTSystem{T, VT, MT, QN} <: AbstractUnreducedKKTSystem{T, VT, MT, QN}\n\nImplement the AbstractUnreducedKKTSystem in sparse COO format.\n\n\n\n\n\n","category":"type"},{"location":"lib/kkt/#MadNLP.SparseCondensedKKTSystem","page":"KKT systems","title":"MadNLP.SparseCondensedKKTSystem","text":"SparseCondensedKKTSystem{T, VT, MT, QN} <: AbstractCondensedKKTSystem{T, VT, MT, QN}\n\nImplement the AbstractCondensedKKTSystem in sparse COO format.\n\n\n\n\n\n","category":"type"},{"location":"lib/kkt/#MadNLP.ScaledSparseKKTSystem","page":"KKT systems","title":"MadNLP.ScaledSparseKKTSystem","text":"ScaledSparseKKTSystem{T, VT, MT, QN} <: AbstractReducedKKTSystem{T, VT, MT, QN}\n\nScaled version of the AbstractReducedKKTSystem (using the K2.5 formulation introduced in [GOS]).\n\nThe K2.5 formulation of the augmented KKT system has a better conditioning than the original (K2) formulation. It is recommend switching to a ScaledSparseKKTSystem if you encounter numerical difficulties in MadNLP.\n\nAt a primal-dual iterate (x s y z), the matrix writes\n\n[√Ξₓ Wₓₓ √Ξₓ + Δₓ   0      √Ξₓ Aₑ'   √Ξₓ Aᵢ']  [√Ξₓ⁻¹ Δx]\n[0                  Δₛ     0           -√Ξₛ ]  [√Ξₛ⁻¹ Δs]\n[Aₑ√Ξₓ              0      0              0 ]  [Δy      ]\n[Aᵢ√Ξₓ             -√Ξₛ    0              0 ]  [Δz      ]\n\nwith\n\nWₓₓ: Hessian of the Lagrangian.\nAₑ: Jacobian of the equality constraints\nAᵢ: Jacobian of the inequality constraints\nΔₓ = Xᵤ Zₗˣ + Xₗ Zᵤˣ\nΔₛ = Sᵤ Zₗˢ + Sₗ Zᵤˢ\nΞₓ = Xₗ Xᵤ\nΞₛ = Sₗ Sᵤ\n\nReferences\n\n[GOS] Ghannad, Alexandre, Dominique Orban, and Michael A. Saunders. \"Linear systems arising in interior methods for convex optimization: a symmetric formulation with bounded condition number.\" Optimization Methods and Software 37, no. 4 (2022): 1344-1369.\n\n\n\n\n\n","category":"type"},{"location":"lib/kkt/#MadNLP.DenseKKTSystem","page":"KKT systems","title":"MadNLP.DenseKKTSystem","text":"DenseKKTSystem{T, VT, MT, QN, VI} <: AbstractReducedKKTSystem{T, VT, MT, QN}\n\nImplement AbstractReducedKKTSystem with dense matrices.\n\nRequires a dense linear solver to be factorized (otherwise an error is returned).\n\n\n\n\n\n","category":"type"},{"location":"lib/kkt/#MadNLP.DenseCondensedKKTSystem","page":"KKT systems","title":"MadNLP.DenseCondensedKKTSystem","text":"DenseCondensedKKTSystem{T, VT, MT, QN} <: AbstractCondensedKKTSystem{T, VT, MT, QN}\n\nImplement AbstractCondensedKKTSystem with dense matrices.\n\nRequires a dense linear solver to factorize the associated KKT system (otherwise an error is returned).\n\n\n\n\n\n","category":"type"},{"location":"lib/kkt/#MadNLP.AbstractKKTVector","page":"KKT systems","title":"MadNLP.AbstractKKTVector","text":"AbstractKKTVector{T, VT}\n\nSupertype for KKT's right-hand-side vectors (x s y z ν w).\n\n\n\n\n\n","category":"type"},{"location":"lib/kkt/#MadNLP.number_primal","page":"KKT systems","title":"MadNLP.number_primal","text":"number_primal(X::AbstractKKTVector)\n\nGet total number of primal values (x s) in KKT vector X.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.number_dual","page":"KKT systems","title":"MadNLP.number_dual","text":"number_dual(X::AbstractKKTVector)\n\nGet total number of dual values (y z) in KKT vector X.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.full","page":"KKT systems","title":"MadNLP.full","text":"full(X::AbstractKKTVector)\n\nReturn the all the values stored inside the KKT vector X.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.primal","page":"KKT systems","title":"MadNLP.primal","text":"primal(X::AbstractKKTVector)\n\nReturn the primal values (x s) stored in the KKT vector X.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.dual","page":"KKT systems","title":"MadNLP.dual","text":"dual(X::AbstractKKTVector)\n\nReturn the dual values (y z) stored in the KKT vector X.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.primal_dual","page":"KKT systems","title":"MadNLP.primal_dual","text":"primal_dual(X::AbstractKKTVector)\n\nReturn both the primal and the dual values (x s y z) stored in the KKT vector X.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.dual_lb","page":"KKT systems","title":"MadNLP.dual_lb","text":"dual_lb(X::AbstractKKTVector)\n\nReturn the dual values ν associated to the lower-bound stored in the KKT vector X.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.dual_ub","page":"KKT systems","title":"MadNLP.dual_ub","text":"dual_ub(X::AbstractKKTVector)\n\nReturn the dual values w associated to the upper-bound stored in the KKT vector X.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.UnreducedKKTVector","page":"KKT systems","title":"MadNLP.UnreducedKKTVector","text":"UnreducedKKTVector{T, VT<:AbstractVector{T}} <: AbstractKKTVector{T, VT}\n\nFull KKT vector (x s y z ν w), associated to a AbstractUnreducedKKTSystem.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/kktsystem/#Implementing-a-custom-KKT-system","page":"Custom KKT system","title":"Implementing a custom KKT system","text":"This tutorial explains how to implement a custom AbstractKKTSystem in MadNLP.","category":"section"},{"location":"tutorials/kktsystem/#Structure-exploiting-methods","page":"Custom KKT system","title":"Structure exploiting methods","text":"MadNLP gives the user the possibility to exploit the problem's structure at the linear algebra level, when solving the KKT system at every Newton iteration. By default, the KKT system is factorized using a sparse linear solver (MUMPS or HSL ma27/ma57). A sparse linear solver analyses the problem's algebraic structure when computing the symbolic factorization, with a heuristic that determines an optimal elimination tree. As an alternative, the problem's structure can be exploited directly, by specifying the order of the pivots to perform (e.g. using a block elimination algorithm). Doing so usually leads to significant speed-up in the algorithm.\n\nWe recall that the KKT system solved at each Newton iteration has the structure:\n\noverline\nbeginpmatrix\n W        J^top  - I     I \n J        0       0       0 \n -Z_ell  0       X_ell  0 \n Z_u      0       0       X_u\nendpmatrix^K\nbeginpmatrix\n    Delta x \n    Delta y \n    Delta z_ell \n    Delta z_u\nendpmatrix\n=\nbeginpmatrix\n    r_1  r_2  r_3  r_4\nendpmatrix\n\nwith W a sparse matrix storing the Hessian of the Lagrangian, and J a sparse matrix storing the Jacobian of the constraints. We note the diagonal matrices Z_ell = diag(z_ell), Z_u = diag(z_u), X_ell = diag(x_ell - x), X_u = diag(x - x_u).\n\nIn MadNLP, every linear system with the structure K is implemented as an AbstractKKTSystem. By default, MadNLP represents a KKT system as a SparseKKTSystem:\n\nnlp = HS15Model()\nresults = madnlp(nlp; kkt_system=MadNLP.SparseKKTSystem, linear_solver=LapackCPUSolver)\nnothing\n","category":"section"},{"location":"tutorials/kktsystem/#Solving-AbstractKKTSystem-in-MadNLP","page":"Custom KKT system","title":"Solving AbstractKKTSystem in MadNLP","text":"The AbstractKKTSystem object is an abstraction to solve the generic system K x = b. Depending on the implementation, the structure of the linear system is exploited in different fashions. Solving a KKT system amounts to the four following operations:\n\nQuerying the current sensitivities to assemble the different blocks constituting the matrix K.\nAssembling a reduced sparse matrix condensing the sparse matrix K to an equivalent smaller symmetric system.\nCalling a linear solver to solve the condensed system.\nCalling a routine to unpack the condensed solution to get the original descent direction (Delta x Delta y Delta z_ell Delta z_u).\n\nExploiting the problem's structure usually happens in steps (2) and (4). We skim through the four successive steps in more details.","category":"section"},{"location":"tutorials/kktsystem/#Getting-the-sensitivities","page":"Custom KKT system","title":"Getting the sensitivities","text":"The KKT system requires the following information:\n\nthe (approximated) Hessian of the Lagrangian W ;\nthe constraints' Jacobian J ;\nthe diagonal matrices Z_ell, Z_u and X_ell, X_u.\n\nThe Hessian and the Jacobian are assumed sparse by default.\n\nAt every IPM iteration, MadNLP updates automatically the values in W, J and in the diagonal matrices Z_ell Z_u X_ell X_u. By default, we expect the following attributes available in every instance kkt of an AbstractKKTSystem:\n\nkkt.hess: stores the nonzeroes of the Hessian W;\nkkt.jac: stores the nonzeroes of the Jacobian J;\nkkt.l_diag: stores the diagonal entries in X_ell;\nkkt.u_diag: stores the diagonal entries in X_u;\nkkt.l_lower: stores the diagonal entries in Z_ell;\nkkt.u_lower: stores the diagonal entries in Z_u.\n\nThe attributes kkt.hess and kkt.jac are accessed respectively using the getters get_hessian and get_jacobian.\n\nEvery time MadNLP queries the Hessian and the Jacobian, it updates the nonzeroes values in kkt.hess and kkt.jac. Rightafter, MadNLP calls respectively the functions compress_hessian! and compress_jacobian! to update all the internal values in the KKT system kkt.\n\nTo recap, every time we evaluate the Hessian and the Jacobian, MadNLP calls automatically the functions:\n\nhess = MadNLP.get_hessian(kkt)\nMadNLP.compress_hessian!(kkt)\n\nto update the values in the Hessian, and for the Jacobian:\n\njac = MadNLP.get_jacobian(kkt)\nMadNLP.compress_jacobian!(kkt)","category":"section"},{"location":"tutorials/kktsystem/#Assembling-the-KKT-system","page":"Custom KKT system","title":"Assembling the KKT system","text":"Once the sensitivities have been updated, we can assemble the KKT matrix K and condense it to an equivalent system K_c before factorizing it with a linear solver. The assembling of the KKT system is done in the function build_kkt!.\n\nThe system is usually stored in the attribute kkt.aug_com. Its dimension depends on the condensation used. The matrix kkt.aug_com can be dense or sparse, depending on the condensation used. MadNLP uses the getter get_kkt to query the matrix kkt.aug_com stored in the KKT system kkt.","category":"section"},{"location":"tutorials/kktsystem/#Solving-the-system","page":"Custom KKT system","title":"Solving the system","text":"Once the matrix K_c is assembled, we pass it to the linear solver for factorization. The linear solver is stored internally in kkt: by default, it is stored in the attribute kkt.linear_solver. The factorization is handled internally in MadNLP.\n\nOnce factorized, it remains to solve the linear system using a backsolve. The backsolve has to be implemented by the user in the function solve!. It reduces the right-hand-side (RHS) down to a form adapted to the condensed matrix K_c and calls the linear solver to perform the backsolve. Then the condensed solution is unpacked to recover the full solution (Delta x Delta y Delta z_ell Delta z_u).\n\nTo recap, MadNLP assembles and solves the KKT linear system using the following operations:\n\n# Assemble\nMadNLP.build_kkt!(kkt)\n# Factorize  the KKT system\nMadNLP.factorize!(kkt.linear_solver)\n# Backsolve\nMadNLP.solve!(kkt, w)\n","category":"section"},{"location":"tutorials/kktsystem/#Example:-Implementing-a-new-KKT-system","page":"Custom KKT system","title":"Example: Implementing a new KKT system","text":"As an example, we detail how to implement a custom KKT system in MadNLP. Note that we consider this usage as an advanced use of MadNLP. After this work of caution, let's dive into the details!\n\nIn this example, we want to approximate the Hessian of the Lagrangian W as a diagonal matrix D_w and solve the following KKT system at each IPM iteration:\n\nbeginpmatrix\n D_w  J^top  - I  I \n J  0   0  0 \n -Z_ell   0  X_ell  0 \n Z_u  W   0  X_u\nendpmatrix\nbeginpmatrix\n    Delta x \n    Delta y \n    Delta z_ell \n    Delta z_u\nendpmatrix\n=\nbeginpmatrix\n    r_1  r_2  r_3  r_4\nendpmatrix\n\nThis new system is not equivalent to the original system K, but it's much easier to solve at it does not involve the generic Hessian W. If the diagonal values of D_w are constant and are equal to alpha, the algorithm becomes equivalent to a gradient descent with step alpha^-1.\n\nUsing the relations Delta z_ell = X_ell^-1 (r_3 + Z_ell Delta x) and Delta z_u = X_u^-1 (r_3 - Z_u Delta x), we condense the matrix down to the reduced form:\n\nbeginpmatrix\n D_w + Sigma_x  J^top \n J  0  \nendpmatrix\nbeginpmatrix\n    Delta x  \n    Delta y\nendpmatrix\n=\nbeginpmatrix\n    r_1 + X_ell^-1 r_3 - X_u^-1 r_4 r_2\nendpmatrix\n\nwith the diagonal matrix Sigma_x = -X_ell^-1 Z_ell - X_u^-1 Z_u. The new system is symmetric indefinite, but much easier to solve than the original one.\n\nThe previous reduction is standard in NLP solvers: MadNLP implements the reduced KKT system operating in the space (Delta x Delta y) using the abstraction AbstractReducedKKTSystem. If D_w is replaced by the original Hessian matrix W, we recover exactly the SparseKKTSystem used by default in MadNLP.","category":"section"},{"location":"tutorials/kktsystem/#Creating-the-KKT-system","page":"Custom KKT system","title":"Creating the KKT system","text":"We create a new KKT system DiagonalHessianKKTSystem, inheriting from AbstractReducedKKTSystem. Using generic types, the structure DiagonalHessianKKTSystem is defined as:\n\nstruct DiagonalHessianKKTSystem{T, VT, MT, QN, LS, VI, VI32} <: MadNLP.AbstractReducedKKTSystem{T, VT, MT, QN}\n    # Nonzeroes values for Hessian and Jacobian\n    hess::VT\n    jac_callback::VT\n    jac::VT\n    # Diagonal matrices\n    reg::VT\n    pr_diag::VT\n    du_diag::VT\n    l_diag::VT\n    u_diag::VT\n    l_lower::VT\n    u_lower::VT\n    # Augmented system K\n    aug_raw::MadNLP.SparseMatrixCOO{T,Int32,VT, VI32}\n    aug_com::MT\n    aug_csc_map::Union{Nothing, VI}\n    # Diagonal of the Hessian\n    diag_hess::VT\n    # Jacobian\n    jac_raw::MadNLP.SparseMatrixCOO{T,Int32,VT, VI32}\n    jac_com::MT\n    jac_csc_map::Union{Nothing, VI}\n    # LinearSolver\n    linear_solver::LS\n    # Info\n    n_var::Int\n    n_ineq::Int\n    n_tot::Int\n    ind_ineq::VI\n    ind_lb::VI\n    ind_ub::VI\n    # Quasi-Newton approximation\n    quasi_newton::QN\nend\n\n\ninfo: Info\nHere, we define a DiagonalHessianKKTSystem as a subtype of a AbstractReducedKKTSystem. Depending on the condensation, the following alternatives are available:AbstractUnreducedKKTSystem: no condensation is applied.\nAbstractCondensedKKTSystem: the reduced KKT system is condensed further by removing the blocks associated to the slack variables.\n\ninfo: Info\nThe attributes pr_diag and du_diag store respectively the primal regularization (terms in the diagonal of the (1, 1) block) and the dual regularization (terms in the diagonal of the (2, 2) block). By default, the dual regularization is keep equal to 0, whereas the primal regularization is set equal to Sigma_x.\n\nMadNLP instantiates a new KKT system with the function create_kkt_system, with the following signature:\n\nfunction MadNLP.create_kkt_system(\n    ::Type{DiagonalHessianKKTSystem},\n    cb::MadNLP.SparseCallback{T,VT},\n    linear_solver::Type;\n    opt_linear_solver=MadNLP.default_options(linear_solver),\n    hessian_approximation=MadNLP.ExactHessian,\n    qn_options=MadNLP.QuasiNewtonOptions(),\n) where {T,VT}\n\nWe pass as arguments:\n\nthe type of the KKT system to build (here, DiagonalHessianKKTSystem),\nthe structure used to evaluate the callbacks cb,\nthe index of the constraints,\na generic linear solver linear_solver.\n\nThis function instantiates all the data structures needed in DiagonalHessianKKTSystem. The most difficult part is to assemble the sparse matrices aug_raw and jac_raw, here stored in COO format. This is done in four steps:\n\nStep 1. We import the sparsity pattern of the Jacobian :\n\n    jac_sparsity_I = MadNLP.create_array(cb, Int32, cb.nnzj)\n    jac_sparsity_J = MadNLP.create_array(cb, Int32, cb.nnzj)\n    MadNLP._jac_sparsity_wrapper!(cb, jac_sparsity_I, jac_sparsity_J)\n\nStep 2. We build the resulting KKT matrix aug_raw in COO format, knowing that D_w is diagonal:\n\n    # System's dimension\n    n_hess = n_tot # Diagonal Hessian!\n    n_jac = length(jac_sparsity_I)\n    aug_vec_length = n_tot+m\n    aug_mat_length = n_tot+m+n_hess+n_jac+n_slack\n\n    # Build vectors to store COO coortinates\n    I = MadNLP.create_array(cb, Int32, aug_mat_length)\n    J = MadNLP.create_array(cb, Int32, aug_mat_length)\n    V = VT(undef, aug_mat_length)\n    fill!(V, 0.0)  # Need to initiate V to avoid NaN\n\n    offset = n_tot+n_jac+n_slack+n_hess+m\n\n    # Primal regularization block\n    I[1:n_tot] .= 1:n_tot\n    J[1:n_tot] .= 1:n_tot\n    # Hessian block\n    I[n_tot+1:n_tot+n_hess] .= 1:n_tot # diagonal Hessian!\n    J[n_tot+1:n_tot+n_hess] .= 1:n_tot # diagonal Hessian!\n    # Jacobian block\n    I[n_tot+n_hess+1:n_tot+n_hess+n_jac] .= (jac_sparsity_I.+n_tot)\n    J[n_tot+n_hess+1:n_tot+n_hess+n_jac] .= jac_sparsity_J\n    # Slack block\n    I[n_tot+n_hess+n_jac+1:n_tot+n_hess+n_jac+n_slack] .= ind_ineq .+ n_tot\n    J[n_tot+n_hess+n_jac+1:n_tot+n_hess+n_jac+n_slack] .= (n+1:n+n_slack)\n    # Dual regularization block\n    I[n_tot+n_hess+n_jac+n_slack+1:offset] .= (n_tot+1:n_tot+m)\n    J[n_tot+n_hess+n_jac+n_slack+1:offset] .= (n_tot+1:n_tot+m)\n\n    aug_raw = MadNLP.SparseMatrixCOO(aug_vec_length, aug_vec_length, I, J, V)\n\nStep 3. We convert aug_raw from COO to CSC using the following utilities:\n\n    aug_com, aug_csc_map = MadNLP.coo_to_csc(aug_raw)\n\nStep 4. We pass the matrix in CSC format to the linear solver:\n\n    _linear_solver = linear_solver(\n        aug_com; opt = opt_linear_solver\n    )\n\n\ninfo: Info\nStoring the Hessian and Jacobian, even in sparse format, is expensive in term of memory. For that reason, MadNLP stores the Hessian and Jacobian only once in the KKT system.","category":"section"},{"location":"tutorials/kktsystem/#Getting-the-sensitivities-2","page":"Custom KKT system","title":"Getting the sensitivities","text":"MadNLP requires the following getters to update the sensitivities. As much as we can, we try to update the values inplace in the matrix aug_raw. We use the default implementation of compress_jacobian! in MadNLP:\n\nfunction MadNLP.compress_jacobian!(kkt::DiagonalHessianKKTSystem)\n    ns = length(kkt.ind_ineq)\n    kkt.jac[end-ns+1:end] .= -1.0\n    MadNLP.transfer!(kkt.jac_com, kkt.jac_raw, kkt.jac_csc_map)\n    return\nend\n\nThe term -1.0 accounts for the slack variables used to reformulate the inequality constraints as equality constraints.\n\nFor compress_hessian!, we take into account that the diagonal matrix D_w is the diagonal of the Hessian:\n\nfunction MadNLP.compress_hessian!(kkt::DiagonalHessianKKTSystem)\n    kkt.diag_hess .= 1.0\n    return\nend\n\nMadNLP also needs the following basic functions to get the different matrices and the dimension of the linear system:\n\nMadNLP.num_variables(kkt::DiagonalHessianKKTSystem) = length(kkt.diag_hess)\nMadNLP.get_kkt(kkt::DiagonalHessianKKTSystem) = kkt.aug_com\nMadNLP.get_jacobian(kkt::DiagonalHessianKKTSystem) = kkt.jac_callback\nfunction MadNLP.jtprod!(y::AbstractVector, kkt::DiagonalHessianKKTSystem, x::AbstractVector)\n    mul!(y, kkt.jac_com', x)\nend","category":"section"},{"location":"tutorials/kktsystem/#Assembling-the-KKT-system-2","page":"Custom KKT system","title":"Assembling the KKT system","text":"Once the sensitivities are updated, we assemble the new matrix K_c first in COO format in kkt.aug_raw, before converting the matrix to CSC format in kkt.jac_com using the utility MadNLP.transfer!:\n\nfunction MadNLP.build_kkt!(kkt::DiagonalHessianKKTSystem)\n    MadNLP.transfer!(kkt.aug_com, kkt.aug_raw, kkt.aug_csc_map)\nend","category":"section"},{"location":"tutorials/kktsystem/#Solving-the-system-2","page":"Custom KKT system","title":"Solving the system","text":"It remains to implement the backsolve. For the reduced KKT formulation, the RHS r_1 + X_ell^-1 r_3 - X_u^-1 r_4 is built automatically using the function MadNLP.reduce_rhs!. The backsolve solves for (Delta x Delta y). The dual's descent direction Delta z_ell and Delta z_u are recovered afterwards using the function MadNLP.finish_aug_solve!:\n\nfunction MadNLP.solve!(kkt::DiagonalHessianKKTSystem, w::MadNLP.AbstractKKTVector)\n    MadNLP.reduce_rhs!(w.xp_lr, dual_lb(w), kkt.l_diag, w.xp_ur, dual_ub(w), kkt.u_diag)\n    MadNLP.solve!(kkt.linear_solver, primal_dual(w))\n    MadNLP.finish_aug_solve!(kkt, w)\n    return w\nend\n\nnote: Note\nThe function solve! takes as second argument a vector w being an AbstractKKTVector. An AbstractKKTVector is a convenient data structure used in MadNLP to store and access the elements in the primal-dual vector (Delta x Delta y Delta z_ell Delta z_u).\n\nwarning: Warning\nWhen calling solve!, the values in the vector w are updated inplace. The vector w should be initialized with the RHS (r_1 r_2 r_3 r_4) before calling the function solve!. The function modifies the values directly in the vector w to return the solution (Delta x Delta y Delta z_ell Delta z_u).\n\nLast, MadNLP implements an iterative refinement method to get accurate descent directions in the final iterations. The iterative refinement algorithm implements Richardson's method, which requires multiplying the KKT matrix K on the right by any vector w = (w_x w_y w_z_l w_z_u). This is provided in MadNLP by overloading the function LinearAlgebra.mul!:\n\nfunction LinearAlgebra.mul!(\n    w::MadNLP.AbstractKKTVector{T},\n    kkt::DiagonalHessianKKTSystem,\n    x::MadNLP.AbstractKKTVector{T},\n    alpha = one(T),\n    beta = zero(T),\n) where {T}\n\n    mul!(primal(w), Diagonal(kkt.diag_hess), primal(x), alpha, beta)\n    mul!(primal(w), kkt.jac_com', dual(x), alpha, one(T))\n    mul!(dual(w), kkt.jac_com,  primal(x), alpha, beta)\n\n    # Reduce KKT vector\n    MadNLP._kktmul!(w,x,kkt.reg,kkt.du_diag,kkt.l_lower,kkt.u_lower,kkt.l_diag,kkt.u_diag, alpha, beta)\n    return w\nend","category":"section"},{"location":"tutorials/kktsystem/#Demonstration","page":"Custom KKT system","title":"Demonstration","text":"We now have all the elements needed to solve the problem with the new KKT linear system DiagonalHessianKKTSystem. We just have to pass the KKT system to MadNLP using the option kkt_system:\n\nnlp = HS15Model()\nresults = madnlp(nlp; kkt_system=DiagonalHessianKKTSystem, linear_solver=LapackCPUSolver)\nnothing\n","category":"section"},{"location":"tutorials/multiprecision/#Running-MadNLP-in-arbitrary-precision","page":"Multi-precision","title":"Running MadNLP in arbitrary precision","text":"MadNLP supports solving optimization problems in arbitrary precision. By default, MadNLP adapts its precision following the NLPModel passed in input. Most models are using Float64 (in fact, almost all optimization modelers are implemented using double precision), but for certain applications it can be useful to use arbitrary precision to get more accurate solution.\n\ninfo: Info\nThere exists different packages to instantiate a optimization model in arbitrary precision in Julia. Most of them leverage the flexibility offered by NLPModels.jl. In particular, we recommend:CUTEst.jl: supports Float32, Float64 and Float128.\nExaModels: supports AbstractFloat.","category":"section"},{"location":"tutorials/multiprecision/#Defining-a-problem-in-arbitrary-precision","page":"Multi-precision","title":"Defining a problem in arbitrary precision","text":"As a demonstration, we implement the model airport from CUTEst using ExaModels. The code writes:\n\nusing ExaModels\n\nfunction airport_model(T)\n    N = 42\n    # Data\n    r = T[0.09 , 0.3, 0.09, 0.45, 0.5, 0.04, 0.1, 0.02, 0.02, 0.07, 0.4, 0.045, 0.05, 0.056, 0.36, 0.08, 0.07, 0.36, 0.67, 0.38, 0.37, 0.05, 0.4, 0.66, 0.05, 0.07, 0.08, 0.3, 0.31, 0.49, 0.09, 0.46, 0.12, 0.07, 0.07, 0.09, 0.05, 0.13, 0.16, 0.46, 0.25, 0.1]\n    cx = T[-6.3, -7.8, -9.0, -7.2, -5.7, -1.9, -3.5, -0.5, 1.4, 4.0, 2.1, 5.5, 5.7, 5.7, 3.8, 5.3, 4.7, 3.3, 0.0, -1.0, -0.4, 4.2, 3.2, 1.7, 3.3, 2.0, 0.7, 0.1, -0.1, -3.5, -4.0, -2.7, -0.5, -2.9, -1.2, -0.4, -0.1, -1.0, -1.7, -2.1, -1.8, 0.0]\n    cy = T[8.0, 5.1, 2.0, 2.6, 5.5, 7.1, 5.9, 6.6, 6.1, 5.6, 4.9, 4.7, 4.3, 3.6, 4.1, 3.0, 2.4, 3.0, 4.7, 3.4, 2.3, 1.5, 0.5, -1.7, -2.0, -3.1, -3.5, -2.4, -1.3, 0.0, -1.7, -2.1, -0.4, -2.9, -3.4, -4.3, -5.2, -6.5, -7.5, -6.4, -5.1, 0.0]\n    # Wrap all data in a single iterator for ExaModels\n    data = [(i, cx[i], cy[i], r[i]) for i in 1:N]\n    IJ = [(i, j) for i in 1:N-1 for j in i+1:N]\n    # Write model using ExaModels\n    core = ExaModels.ExaCore(T)\n    x = ExaModels.variable(core, 1:N, lvar = -10.0, uvar=10.0)\n    y = ExaModels.variable(core, 1:N, lvar = -10.0, uvar=10.0)\n    ExaModels.objective(\n        core,\n        ((x[i] - x[j])^2 + (y[i] - y[j])^2) for (i, j) in IJ\n    )\n    ExaModels.constraint(core, (x[i]-dcx)^2 + (y[i] - dcy)^2 - dr for (i, dcx, dcy, dr) in data; lcon=-Inf)\n    return ExaModels.ExaModel(core)\nend\n\nThe function airport_model takes as input the type used to define the model in ExaModels. For example, ExaCore(Float64) instantiates a model with Float64, whereas ExaCore(Float32) instantiates a model using Float32. Thus, instantiating the instance airport using Float32 simply amounts to\n\nnlp = airport_model(Float32)\n\n\nWe verify that the model is correctly instantiated using Float32:\n\nx0 = NLPModels.get_x0(nlp)\nprintln(typeof(x0))","category":"section"},{"location":"tutorials/multiprecision/#Solving-a-problem-in-Float32","page":"Multi-precision","title":"Solving a problem in Float32","text":"Now that we have instantiated our model in Float32, we solve it using MadNLP. As nlp is using Float32, MadNLP will automatically adjust its internal types to Float32 during the instantiation. By default, the convergence tolerance is also adjusted to the input type, such that tol = sqrt(eps(T)). Hence, in our case the tolerance is set automatically to\n\ntol = sqrt(eps(Float32))\n\nWe solve the problem using Lapack as linear solver:\n\nresults = madnlp(nlp; linear_solver=LapackCPUSolver)\nnothing\n\nnote: Note\nNote that the distribution of Lapack shipped with Julia supports Float32, so here we do not have to worry whether the type is supported by the linear solver. Almost all linear solvers shipped with MadNLP supports Float32.\n\nThe final solution is\n\nresults.solution\n\n\nand the objective is\n\nresults.objective\n\n\nFor completeness, we compare with the solution returned when we solve the same problem using Float64:\n\nnlp_64 = airport_model(Float64)\nresults_64 = madnlp(nlp_64; linear_solver=LapackCPUSolver)\nnothing\n\nThe final objective is now\n\nresults_64.objective\n\n\nAs expected when solving an optimization problem with Float32, the relative difference between the two solutions is far from being negligible:\n\nrel_diff = abs(results.objective - results_64.objective) / results_64.objective","category":"section"},{"location":"tutorials/multiprecision/#Solving-a-problem-in-Float128","page":"Multi-precision","title":"Solving a problem in Float128","text":"Now, we go in the opposite direction and solve a problem using Float128 to get a better accuracy. We start by loading the library Quadmath to work with quadruple precision:\n\nusing Quadmath\n\nWe can instantiate our problem using Float128 directly as:\n\nnlp_128 = airport_model(Float128)\n\nwarning: Warning\nOn the contrary to Float32, a few linear solvers support Float128 out of the box. Currently, the only solvers suporting quadruple in MadNLP are LDLSolver and the HSL solvers (require MadNLPHSL). LDLSolvers uses an LDL factorization implemented in pure Julia. The solver LDLSolver is not adapted to solve large-scale nonconvex nonlinear programs, but works if the problem is small enough (as it is the case here).\n\nOnce we have replaced the solver by LDLSolver, solving the problem with MadNLP in Float128 just amounts to\n\nresults_128 = madnlp(nlp_128; linear_solver=LDLSolver)\nnothing\n\n\nNote that the final tolerance is much lower than before. We get the solution in quadruple precision\n\nresults_128.solution\n\nas well as the final objective:\n\nresults_128.objective","category":"section"},{"location":"tutorials/quasi_newton/#Quasi-Newton-methods","page":"Quasi-Newton","title":"Quasi-Newton methods","text":"Sometimes, the second-order derivatives are just too expensive to evaluate. In that case, it is often a good idea to approximate the Hessian matrix using a Quasi-Newton algorithm. The BFGS algorithm uses the first-order derivatives (gradient and tranposed-Jacobian vector product) to approximate the Hessian of the Lagrangian. LBFGS is a variant of BFGS that computes a low-rank approximation of the Hessian matrix. By doing so, LBFGS does not have to store a large dense matrix in memory, rendering the algorithm appropriate in the large-scale regime.\n\nMadNLP implements several quasi-Newton approximation for the Hessian matrix.","category":"section"},{"location":"tutorials/quasi_newton/#How-to-use-LBFGS-in-MadNLP?","page":"Quasi-Newton","title":"How to use LBFGS in MadNLP?","text":"First, we focus on the limited-memory version of the BFGS algorithm, commonly known as LBFGS. We refer to this article for a detailed description of the method.\n\nWe look at the elec optimization problem from the COPS benchmark:\n\nusing Random\nusing ExaModels\n\nfunction elec_model(np)\n    Random.seed!(1)\n    # Set the starting point to a quasi-uniform distribution of electrons on a unit sphere\n    theta = (2pi) .* rand(np)\n    phi = pi .* rand(np)\n\n    core = ExaModels.ExaCore(Float64)\n    x = ExaModels.variable(core, 1:np; start = [cos(theta[i])*sin(phi[i]) for i=1:np])\n    y = ExaModels.variable(core, 1:np; start = [sin(theta[i])*sin(phi[i]) for i=1:np])\n    z = ExaModels.variable(core, 1:np; start = [cos(phi[i]) for i=1:np])\n    # Coulomb potential\n    itr = [(i,j) for i in 1:np-1 for j in i+1:np]\n    ExaModels.objective(core, 1.0 / sqrt((x[i] - x[j])^2 + (y[i] - y[j])^2 + (z[i] - z[j])^2) for (i,j) in itr)\n    # Unit-ball\n    ExaModels.constraint(core, x[i]^2 + y[i]^2 + z[i]^2 - 1 for i=1:np)\n\n    return ExaModels.ExaModel(core)\nend\n\n\nThe problem computes the positions of the electrons in an atom. The potential of each electron depends on the positions of all the other electrons, coupling together all the variables in the problem and resulting in a dense Hessian matrix. Hence, the problem is good candidate for a quasi-Newton algorithm.\n\nWe start by solving the problem with the default options in MadNLP, using a dense linear solver from LAPACK:\n\nusing MadNLP\nnh = 10\nnlp = elec_model(nh)\nresults_hess = madnlp(\n    nlp;\n    linear_solver=LapackCPUSolver,\n)\nnothing\n\n\nWe observe that MadNLP converges in 21 iterations.\n\nTo replace the second-order derivatives by an LBFGS approximation, you should pass the option hessian_approximation=CompactLBFGS to MadNLP.\n\nresults_qn = madnlp(\n    nlp;\n    linear_solver=LapackCPUSolver,\n    hessian_approximation=MadNLP.CompactLBFGS,\n)\nnothing\n\n\nWe observe that MadNLP converges in 93 iterations, but the solution time has decreased by an order of magnitude. As expected, the number of Hessian evaluations is 0.","category":"section"},{"location":"tutorials/quasi_newton/#How-to-tune-the-options-in-LBFGS?","page":"Quasi-Newton","title":"How to tune the options in LBFGS?","text":"You can tune the LBFGS options by using the option quasi_newton_options. The option takes as input a QuasiNewtonOptions structure, with the following attributes (we put on the right their default values):\n\ninit_strategy::BFGSInitStrategy = SCALAR1\nmax_history::Int = 6\ninit_value::Float64 = 1.0\nsigma_min::Float64 = 1e-8\nsigma_max::Float64 = 1e+8\n\nThe most important parameter is max_history, which encodes the number of vectors used in the low-rank approximation. For instance, we can increase the history to use the 20 past iterates using:\n\nqn_options = MadNLP.QuasiNewtonOptions(; max_history=20)\nresults_qn = madnlp(\n    nlp;\n    linear_solver=LapackCPUSolver,\n    hessian_approximation=MadNLP.CompactLBFGS,\n    quasi_newton_options=qn_options,\n)\nnothing\n\n\nWe observe that the total number of iterations has been reduced from 93 to 60.","category":"section"},{"location":"tutorials/quasi_newton/#How-is-LBFGS-implemented-in-MadNLP?","page":"Quasi-Newton","title":"How is LBFGS implemented in MadNLP?","text":"MadNLP implements the compact LBFGS algorithm described in this article. At each iteration, the Hessian W_k is approximated by a low rank positive definite matrix B_k, defined as\n\nB_k = xi_k I + U_k V_k^top\n\n\nwith xi  0 a scaling factor, U_k and V_k two n times 2p matrices. The number p denotes the number of vectors used when computing the limited memory updates (the parameter max_history in MadNLP): the larger, the more accurate is the low-rank approximation.\n\nReplacing the Hessian of the Lagrangian W_k by the low-rank matrix B_k, the KKT system solved in MadNLP rewrites as\n\nbeginbmatrix\nxi I + U V^top + Sigma_x  0  A^top \n0  Sigma_s  -I \nA  -I  0\nendbmatrix\nbeginbmatrix\nDelta x  Delta s  Delta y\nendbmatrix\n=\nbeginbmatrix\nr_1  r_2  r_3\nendbmatrix\n\n\nThe resulting KKT system has a low-rank structure  we can leverage using the Sherman-Morrison formula. The method is detailed e.g. in Section 3.8 of that paper.\n\ninfo: Info\nAs MadNLP is designed to solve generic constrained optimization problems, it does not approximate directly the inverse of the Hessian matrix, as done in classical LBFGS implementations specialized on solving nonlinear problems with bound constraints. If your problem has no generic nonlinear constraints, we recommend for optimal performance using LBFGSB or the LBFGS implemented in JSOSolvers.jl.","category":"section"},{"location":"tutorials/quasi_newton/#How-to-use-BFGS-in-MadNLP?","page":"Quasi-Newton","title":"How to use BFGS in MadNLP?","text":"On the contrary to LBFGS, the algorithm BFGS stores the matrix approximating the Hessian in a dense matrix with size (n n), where n is the number of variables in the problem. Hence, we recommend using LBFGS whenever n becomes greater than 1,000.\n\nThat being said, BFGS can be practical to solve medium-scaled instances, as it provides a better approximation than the LBFGS method. In particular, this is the case for the elec optimization problem. As all the matrices involved are dense, BFGS works only if MadNLP is storing its KKT system in dense format. We have to specify that explicitly by passing to MadNLP the option kkt_system=MadNLP.DenseKKTSystem. Apart of that detail, solving the problem with BFGS just amounts to\n\nresults_qn = madnlp(\n    nlp;\n    linear_solver=LapackCPUSolver,\n    kkt_system=MadNLP.DenseKKTSystem,\n    hessian_approximation=MadNLP.BFGS,\n)\nnothing\n\nObserve that MadNLP here converges in 62 iterations. Indeed, BFGS computes a positive definite approximation of the Hessian matrix.\n\nIf the problem is non-convex, it is usually recommended to use the damped version of the BFGS algorithm, which can be activated by replacing MadNLP.BFGS by MadNLP.DampedBFGS in the precedent call:\n\nresults_qn = madnlp(\n    nlp;\n    linear_solver=LapackCPUSolver,\n    kkt_system=MadNLP.DenseKKTSystem,\n    hessian_approximation=MadNLP.DampedBFGS,\n)\nnothing","category":"section"},{"location":"lib/ipm/#MadNLP-solver","page":"IPM solver","title":"MadNLP solver","text":"MadNLP takes as input a nonlinear program encoded as a AbstractNLPModel and solve it using interior-point. The main entry point is the function madnlp:\n\nIn detail, the function madnlp builds a MadNLPSolver storing all the required structures in the solution algorithm. Once the MadNLPSolver instantiated, the function solve! is applied to solve the nonlinear program with MadNLP's interior-point algorithm.","category":"section"},{"location":"lib/ipm/#MadNLP.madnlp","page":"IPM solver","title":"MadNLP.madnlp","text":"madnlp(model::AbstractNLPModel; options...)\n\nBuild a MadNLPSolver and solve it using the interior-point method. Return the solution as a MadNLPExecutionStats.\n\n\n\n\n\n","category":"function"},{"location":"lib/ipm/#MadNLP.MadNLPExecutionStats","page":"IPM solver","title":"MadNLP.MadNLPExecutionStats","text":"MadNLPExecutionStats{T, VT} <: AbstractExecutionStats\n\nStore the results returned by MadNLP once the interior-point algorithm has terminated.\n\n\n\n\n\n","category":"type"},{"location":"lib/ipm/#MadNLP.MadNLPSolver","page":"IPM solver","title":"MadNLP.MadNLPSolver","text":"MadNLPSolver(nlp::AbstractNLPModel{T, VT}; options...) where {T, VT}\n\nInstantiate a new MadNLPSolver associated to the nonlinear program nlp::AbstractNLPModel. The options are passed as optional arguments.\n\nThe constructor allocates all the memory required in the interior-point algorithm, so the main algorithm remains allocation free.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/warmstart/#Warmstarting-MadNLP","page":"Warm-start","title":"Warmstarting MadNLP","text":"We use a parameterized version of the instance HS15 used in the introduction. This updated version of HS15Model stores the parameters of the model in an attribute nlp.params:\n\n\nnlp = HS15Model()\nprintln(nlp.params)\n\nBy default the parameters are set to [100.0, 1.0]. First, we find a solution associated to these parameters. Then, we warmstart MadNLP from the solution found in the first solve, after a small update in the problem's parameters.\n\ninfo: Info\nIt is known that the interior-point method has a poor support of warmstarting, on the contrary to active-set methods. However, if the new parameters remain close enough and do not lead to significant changes in the active set, warmstarting the interior-point algorithm can significantly reduces the total number of barrier iterations in the second solve.\n\nwarning: Warning\nThe warm-start described in this tutorial remains basic. Its main application is updating the solution of a parametric problem after a small update in the parameters. The warm-start always assumes that the structure of the problem remains the same between two consecutive solves. MadNLP cannot be warm-started if variables or constraints are added to the problem.","category":"section"},{"location":"tutorials/warmstart/#Naive-solution:-starting-from-the-previous-solution","page":"Warm-start","title":"Naive solution: starting from the previous solution","text":"By default, MadNLP starts its interior-point algorithm at the primal variable stored in nlp.meta.x0. We can access this attribute using the function get_x0:\n\nx0 = NLPModels.get_x0(nlp)\n\n\nHere, we observe that the initial solution is [0, 0]. We solve the problem starting from this point using the function madnlp:\n\nresults = madnlp(nlp)\nnothing\n\nMadNLP converges in 19 barrier iterations.  The solution is:\n\nprintln(\"Objective: \", results.objective)\nprintln(\"Solution:  \", results.solution)","category":"section"},{"location":"tutorials/warmstart/#Updating-the-initial-guess","page":"Warm-start","title":"Updating the initial guess","text":"We have found a solution to the problem. Now, what happens if we update the parameters inside nlp?\n\nnlp.params .= [101.0, 1.1]\n\nAs MadNLP starts the algorithm at nlp.meta.x0, we pass the solution found previously to the initial vector:\n\ncopyto!(NLPModels.get_x0(nlp), results.solution)\n\nSolving the problem again with MadNLP, we observe that MadNLP converges in only 6 iterations:\n\nresults_new = madnlp(nlp)\nnothing\n\n\nBy decreasing the initial barrier parameter, we can reduce the total number of iterations to 5:\n\nresults_new = madnlp(nlp; mu_init=1e-7)\nnothing\n\n\nThe solution with the new parameters is slightly different from the former one:\n\nresults_new.solution\n\n\ninfo: Info\nSimilarly as with the primal solution, we can pass the initial dual solution to MadNLP using the function get_y0. We can overwrite the value of y0 in nlp using:copyto!(NLPModels.get_y0(nlp), results.multipliers)and we specify to MadNLP to not recompute the dual multipliers using the option dual_initialized:madnlp(nlp; dual_initialized=true)In our particular example, setting the dual multipliers has only a minor influence on the convergence of the algorithm.\n\ninfo: Info\nMadNLP does not support passing the initial values for the bounds' multipliers z_l and z_u.","category":"section"},{"location":"tutorials/warmstart/#Advanced-solution:-keeping-the-solver-in-memory","page":"Warm-start","title":"Advanced solution: keeping the solver in memory","text":"The previous solution works but is far from being efficient: each time we call the function madnlp we create a new instance of MadNLPSolver, leading to a significant number of memory allocations. A workaround is to keep the solver in memory to have more fine-grained control on the warm-start.\n\nWe start by creating a new model nlp and we instantiate a new instance MadNLPSolver attached to this model:\n\nnlp = HS15Model()\nsolver = MadNLP.MadNLPSolver(nlp)\n\nNote that\n\nnlp === solver.nlp\n\nHence, updating the parameter values in nlp will automatically update the parameters in the solver.\n\nWe first solve the problem using the function solve!:\n\nresults = MadNLP.solve!(solver)\n\nBefore warmstarting MadNLP, we update the parameters and the primal solution in nlp:\n\nnlp.params .= [101.0, 1.1]\ncopyto!(NLPModels.get_x0(nlp), results.solution)\n\nMadNLP stores in memory the dual solutions computed during the first solve. One can access to the (scaled) multipliers as\n\nsolver.y\n\nand to the multipliers of the bound constraints with\n\n[solver.zl.values solver.zu.values]\n\nAs before, it is advised to decrease the initial barrier parameter: if the initial point is close enough to the solution, this reduces drastically the total number of iterations. We solve the problem again using:\n\nMadNLP.solve!(solver; mu_init=1e-7)\nnothing\n\nThree observations are in order:\n\nThe iteration count starts directly from the previous count (as stored in solver.cnt.k).\nMadNLP converges in only 4 iterations.\nThe symbolic factorization of the KKT system stored in solver is directly re-used, leading to significant savings.\nThe warm-start does not work if the structure of the problem changes between two consecutive solves (e.g, if variables or constraints are added to the constraints).\n\nwarning: Warning\nIf we call the function solve! a second-time, MadNLP will use the following rule:The initial primal solution is copied from NLPModels.get_x0(nlp)\nThe initial dual solution is directly taken from the values specified in solver.y, solver.zl and solver.zu. (MadNLP is not using the values stored in nlp.meta.y0 in the second solve).","category":"section"},{"location":"lib/barrier/#Barrier-strategies","page":"Barrier strategies","title":"Barrier strategies","text":"The structure AbstractBarrierUpdate encodes the barrier strategy currently used in MadNLP. The strategy is defined by the option barrier when initializing MadNLP.","category":"section"},{"location":"lib/barrier/#Monotone-strategy","page":"Barrier strategies","title":"Monotone strategy","text":"By default, MadNLP uses a monotone strategy, following the Fiacco-McCormick approach.","category":"section"},{"location":"lib/barrier/#Adaptive-strategies","page":"Barrier strategies","title":"Adaptive strategies","text":"As an alternative to the monotone strategy, MadNLP implements several adaptive rules, all described in this article.","category":"section"},{"location":"lib/barrier/#MadNLP.AbstractBarrierUpdate","page":"Barrier strategies","title":"MadNLP.AbstractBarrierUpdate","text":"AbstractBarrierUpdate{T}\n\nAbstraction used to implement the different rules to update the barrier parameter. The barrier is updated using either a monotone rule or an adaptive rule.\n\n\n\n\n\n","category":"type"},{"location":"lib/barrier/#MadNLP.update_barrier!","page":"Barrier strategies","title":"MadNLP.update_barrier!","text":"update_barrier!(barrier::AbstractBarrierUpdate{T}, solver::AbstractMadNLPSolver{T}, sc::T) where T\n\nUpdate barrier using the rule in barrier. Store the results in solver.mu.\n\n\n\n\n\n","category":"function"},{"location":"lib/barrier/#MadNLP.MonotoneUpdate","page":"Barrier strategies","title":"MadNLP.MonotoneUpdate","text":"MonotoneUpdate{T} <: AbstractBarrierUpdate{T}\n\nUpdate the barrier parameter using the classical Fiacco-McCormick monotone rule.\n\n\n\n\n\n","category":"type"},{"location":"lib/barrier/#MadNLP.AbstractAdaptiveUpdate","page":"Barrier strategies","title":"MadNLP.AbstractAdaptiveUpdate","text":"AbstractAdaptiveUpdate{T} <: AbstractBarrierUpdate{T}\n\nAbstraction used to implement the adaptive barrier updates described in [Nocedal2009].\n\nReferences\n\n[Nocedal2009] Nocedal, J., Wächter, A., & Waltz, R. A. (2009). Adaptive barrier update strategies for nonlinear interior methods. SIAM Journal on Optimization, 19(4), 1674-1693.\n\n\n\n\n\n","category":"type"},{"location":"lib/barrier/#MadNLP.QualityFunctionUpdate","page":"Barrier strategies","title":"MadNLP.QualityFunctionUpdate","text":"QualityFunctionUpdate{T} <: AbstractAdaptiveUpdate{T}\n\nFind the barrier parameter using a quality function encoding the ℓ1-norm of the KKT violations. At each IPM iteration, the minimum of the quality function is found using a Golden search algorithm.\n\nIf no sufficient progress is made, the barrier fallbacks to a monotone rule.\n\nReferences\n\nThe algorithm is described in [Nocedal2009, Section 4].\n\n\n\n\n\n","category":"type"},{"location":"lib/barrier/#MadNLP.LOQOUpdate","page":"Barrier strategies","title":"MadNLP.LOQOUpdate","text":"LOQOUpdate{T} <: AbstractAdaptiveUpdate{T}\n\nFind the barrier parameter using the rule used in the LOQO solver. The rule is explicited in [Nocedal2009, Eq (3.6)].\n\nIf no sufficient progress is made, the barrier fallbacks to a monotone rule.\n\nReferences\n\nThe algorithm is described in [Nocedal2009, Section 3].\n\n\n\n\n\n","category":"type"},{"location":"quickstart/#Quickstart","page":"Quickstart","title":"Quickstart","text":"This page presents a quickstart guide to solve a nonlinear problem with MadNLP.\n\nAs a demonstration, we show how to implement the HS15 nonlinear problem from the Hock & Schittkowski collection, first by using JuMP, and then by specifying the derivatives manually in NLPModels.\n\nThe HS15 problem is defined as:\n\nbeginaligned\nmin_x_1 x_2     100 times (x_2 - x_1^2)^2 +(1 - x_1)^2 \ntextsubject to quad   x_1  times x_2 geq 1 \n         x_1 + x_2^2 geq 0 \n         x_1 leq 05\nendaligned\n\n\nDespite its small dimension, its resolution remains challenging as the problem is nonlinear nonconvex. Note that HS15 has one bound constraint (x_1 leq 05) and two generic constraints. We display in the following picture the feasible set of HS15. The problem has two local solutions: (x_1 x_2) = (05 2) and (x_1 x_2) approx (-0792 -1262).\n\n(Image: HS15)","category":"section"},{"location":"quickstart/#Using-MadNLP-with-JuMP","page":"Quickstart","title":"Using MadNLP with JuMP","text":"JuMP is the easiest way to implement a nonlinear problem. In JuMP, the user just has only to pass the structure of the problem, the computation of the first- and second-order derivatives being handled automatically.\n\nUsing JuMP's syntax, the HS15 problem translates to\n\nusing JuMP\nmodel = Model()\n@variable(model, x1 <= 0.5)\n@variable(model, x2)\n\n@objective(model, Min, 100.0 * (x2 - x1^2)^2 + (1.0 - x1)^2)\n@constraint(model, x1 * x2 >= 1.0)\n@constraint(model, x1 + x2^2 >= 0.0)\n\nprintln(model)\n\n\nThen, solving HS15 with MadNLP directly translates to\n\nusing MadNLP\nJuMP.set_optimizer(model, MadNLP.Optimizer)\nJuMP.optimize!(model)\n\n\nUnder the hood, JuMP builds a sparse AD backend to evaluate the first and second-order derivatives of the objective and the constraints. Then, MadNLP takes as input the callbacks generated by JuMP and wraps them inside a MadNLP.MOIModel.","category":"section"},{"location":"quickstart/#Using-MadNLP-with-NLPModels","page":"Quickstart","title":"Using MadNLP with NLPModels","text":"Alternatively, we can compute the derivatives manually in an AbstractNLPModel. This second option, although more complicated, gives us more flexibility and comes without boilerplate.\n\nWe define the HS15Model as :\n\nstruct HS15Model <: NLPModels.AbstractNLPModel{Float64,Vector{Float64}}\n    meta::NLPModels.NLPModelMeta{Float64, Vector{Float64}}\n    counters::NLPModels.Counters\nend\n\nfunction HS15Model(x0)\n    return HS15Model(\n        NLPModels.NLPModelMeta(\n            2,     #nvar\n            ncon = 2,\n            nnzj = 4,\n            nnzh = 3,\n            x0 = x0,\n            y0 = zeros(2),\n            lvar = [-Inf, -Inf],\n            uvar = [0.5, Inf],\n            lcon = [1.0, 0.0],\n            ucon = [Inf, Inf],\n            minimize = true\n        ),\n        NLPModels.Counters()\n    )\nend\n\nThis structure takes as input the initial position x0 and generates an AbstractNLPModel. NLPModelMeta stores the information about the structure of the problem (variables and constraints' lower and upper bounds, number of variables, number of constraints, ...). Counters is a utility storing the number of time each callbacks is being called.\n\nThe objective function takes as input a HS15Model instance and a vector with dimension 2 storing the current values for x_1 and x_2:\n\nfunction NLPModels.obj(nlp::HS15Model, x::AbstractVector)\n    return 100.0 * (x[2] - x[1]^2)^2 + (1.0 - x[1])^2\nend\n\nThe corresponding gradient writes (note that we update the values of the gradient g inplace):\n\nfunction NLPModels.grad!(nlp::HS15Model, x::AbstractVector, g::AbstractVector)\n    z = x[2] - x[1]^2\n    g[1] = -400.0 * z * x[1] - 2.0 * (1.0 - x[1])\n    g[2] = 200.0 * z\n    return g\nend\n\nSimilarly, we define the constraints\n\nfunction NLPModels.cons!(nlp::HS15Model, x::AbstractVector, c::AbstractVector)\n    c[1] = x[1] * x[2]\n    c[2] = x[1] + x[2]^2\n    return c\nend\n\nnote: Note\nObserve that we update the values of the constraint vector c and the gradient vector g inplace.\n\nThe Jacobian is defined as\n\nfunction NLPModels.jac_structure!(nlp::HS15Model, I::AbstractVector{T}, J::AbstractVector{T}) where T\n    copyto!(I, [1, 1, 2, 2])\n    copyto!(J, [1, 2, 1, 2])\nend\n\nfunction NLPModels.jac_coord!(nlp::HS15Model, x::AbstractVector, J::AbstractVector)\n    J[1] = x[2]    # (1, 1)\n    J[2] = x[1]    # (1, 2)\n    J[3] = 1.0     # (2, 1)\n    J[4] = 2*x[2]  # (2, 2)\n    return J\nend\n\nnote: Note\nAs the Jacobian is sparse, we have to provide explicitly its sparsity structure.\n\nIt remains to implement the Hessian of the Lagrangian for a HS15Model. The Lagrangian of the problem writes\n\nL(x_1 x_2 y_1 y_2) = 100 times (x_2 - x_1^2)^2 +(1 - x_1)^2\n+ y_1 times (x_1 times x_2) + y_2 times (x_1 + x_2^2)\n\nand we aim at evaluating its second-order derivative nabla^2_xxL(x_1 x_2 y_1 y_2).\n\nWe first have to define the sparsity structure of the Hessian, which is assumed to be sparse. The Hessian is a symmetric matrix, and by convention we pass only the lower-triangular part of the matrix to the solver. Hence, we define the sparsity structure as\n\nfunction NLPModels.hess_structure!(nlp::HS15Model, I::AbstractVector{T}, J::AbstractVector{T}) where T\n    copyto!(I, [1, 2, 2])\n    copyto!(J, [1, 1, 2])\nend\n\nNow that the sparsity structure is defined, the associated Hessian writes:\n\nfunction NLPModels.hess_coord!(nlp::HS15Model, x, y, H::AbstractVector; obj_weight=1.0)\n    # Objective\n    H[1] = obj_weight * (-400.0 * x[2] + 1200.0 * x[1]^2 + 2.0)\n    H[2] = obj_weight * (-400.0 * x[1])\n    H[3] = obj_weight * 200.0\n    # First constraint\n    H[2] += y[1] * 1.0\n    # Second constraint\n    H[3] += y[2] * 2.0\n    return H\nend\n\n\nOnce the problem specified in NLPModels, we can create a new MadNLP instance and solve it:\n\nx0 = zeros(2) # initial position\nnlp = HS15Model(x0)\nsolver = MadNLP.MadNLPSolver(nlp; print_level=MadNLP.INFO)\nresults = MadNLP.solve!(solver)\nnothing\n\nMadNLP converges in 19 iterations to a (local) optimal solution. MadNLP returns a MadNLPExecutionStats storing all the results. We can query the primal and the dual solutions respectively by\n\nresults.solution\n\nand\n\nresults.multipliers\n\nIf we come back at the picture displayed at the beginning of this page, the solution found is the one at the bottom left. At this solution, only one constraint is active, explaining why only the first element in results.multipliers is non-null (up to floating point approximation).\n\nThe dual multipliers associated to the variable's lower bounds (resp. upper bounds) are stored in results.multipliers_L (resp. results.multipliers_U):\n\nresults.multipliers_L\n\nBy changing the initial point in HS15Model, MadNLP converges to the second local solution of the non-convex problem HS15:\n\nx0 = ones(2)\nnlp = HS15Model(x0)\nresults = madnlp(nlp)\nnothing\n\nresults.solution\n\nNote that we don't know beforehand towards each solution MadNLP is converging.","category":"section"},{"location":"man/linear_solvers/#Linear-solvers","page":"Linear Solvers","title":"Linear solvers","text":"We suppose that the KKT system has been assembled previously into a given AbstractKKTSystem. Then, it remains to compute the Newton step by solving the KKT system for a given right-hand-side (given as a AbstractKKTVector). That's exactly the role of the linear solver.\n\nIf we do not assume any structure, the KKT system writes in generic form\n\nK x = b\n\nwith K the KKT matrix and b the current right-hand-side. MadNLP provides a suite of specialized linear solvers to solve the linear system.","category":"section"},{"location":"man/linear_solvers/#Inertia-detection","page":"Linear Solvers","title":"Inertia detection","text":"If the matrix K has negative eigenvalues, we have no guarantee that the solution of the KKT system is a descent direction with regards to the original nonlinear problem. That's the reason why most of the linear solvers compute the inertia of the linear system when factorizing the matrix K. The inertia counts the number of positive, negative and zero eigenvalues in the matrix. If the inertia does not meet a given criteria, then the matrix K is regularized by adding a multiple of the identity to it: K_r = K + alpha I.\n\nnote: Note\nWe recall that the inertia of a matrix K is given as a triplet (nmp), with n the number of positive eigenvalues, m the number of negative eigenvalues and p the number of zero eigenvalues.","category":"section"},{"location":"man/linear_solvers/#Factorization-algorithm","page":"Linear Solvers","title":"Factorization algorithm","text":"In nonlinear programming, it is common to employ a LBL factorization to decompose the symmetric indefinite matrix K, as this algorithm returns the inertia of the matrix directly as a result of the factorization.\n\nnote: Note\nWhen MadNLP runs in inertia-free mode, the algorithm does not require to compute the inertia when factorizing the matrix K. In that case, MadNLP can use a classical LU or QR factorization to solve the linear system Kx = b.","category":"section"},{"location":"man/linear_solvers/#Solving-a-KKT-system-with-MadNLP","page":"Linear Solvers","title":"Solving a KKT system with MadNLP","text":"We suppose available a AbstractKKTSystem kkt, properly assembled following the procedure presented previously. We can query the assembled matrix K as\n\nK = MadNLP.get_kkt(kkt)\n\n\nThen, if we want to pass the KKT matrix K to Lapack, this translates to\n\nlinear_solver = LapackCPUSolver(K)\n\n\nThe instance linear_solver does not copy the matrix K and instead keep a reference to it.\n\nlinear_solver.A === K\n\nThat way every time we re-assemble the matrix K in kkt, the values are directly updated inside linear_solver.\n\nTo compute the factorization inside linear_solver, one simply as to call:\n\nMadNLP.factorize!(linear_solver)\n\n\nOnce the factorization computed, computing the backsolve for a right-hand-side b amounts to\n\nnk = size(kkt, 1)\nb = rand(nk)\nMadNLP.solve!(linear_solver, b)\n\nThe values of b being modified inplace to store the solution x of the linear system Kx =b.","category":"section"},{"location":"algorithm/#Interior-point-algorithm","page":"Algorithm","title":"Interior-point algorithm","text":"We give a brief description of the interior-point algorithm used in MadNLP, together with the principal options impacting MadNLP's behavior. The algorithm is described in more length in the Ipopt paper.\n\nMadNLP searches for a local solution of the nonlinear program:\n\n  beginaligned\n    min_x   f(x)  \n    textsubject to quad  g_ell leq g(x) leq g_u  \n                             x_ell leq x leq x_u  \n  endaligned\n\nwhere x in mathbbR^n is the decision variable, f mathbbR^n to mathbbR and g mathbbR^n to mathbbR^m two smooth nonlinear functions.","category":"section"},{"location":"algorithm/#Pre-processing","page":"Algorithm","title":"Pre-processing","text":"Before running the interior-point method, MadNLP applies a pre-processing to the problem to improve the numerical performance. The pre-processing operations are described below.","category":"section"},{"location":"algorithm/#Problem's-reformulation","page":"Algorithm","title":"Problem's reformulation","text":"","category":"section"},{"location":"algorithm/#Slack-variables","page":"Algorithm","title":"Slack variables","text":"First, MadNLP splits the equality from the inequality constraints in the definition of the feasible set:\n\ng_ell leq g(x) leq g_u  \n\nIf g_elli  g_ui, the constraint is reformulated as\n\ng_i(x) - s_i = 0    quad g_ell i leq s_i leq g_ui  \n\nwith s_i an additional slack variable. By doing so, all the inequality constraints are moved into the bound constraints. This benefits directly to the interior-point algorithm, as it becomes much easier to compute a strictly feasible initial point.\n\ninfo: Info\nMadNLP only reformulates with a slack variable the inequality constraints, the equality constraints are left untouched.\n\nAs a result, we obtain an equivalent problem with the following structure:\n\n  beginaligned\n    min_w   f(w)   \n    textsubject to quad  c(w) = 0   quad w geq 0  \n  endaligned\n\nwith w = (x s) and c(w) = (g_I(x) - s g_E(x)), with I the index set for the inequality constraints and E the index set for equality constraints.\n\ninfo: Info\nTo simplify the exposition, we have assumed that the variables have all their lower bound sets to zero, and no upper bound.\n\nThe KKT stationary conditions associated to the reformulated problem are:\n\n  beginaligned\n     nabla f(w) + nabla c(w)^top y - z = 0   \n     c(w) = 0   \n     0 leq w perp z geq 0  \n  endaligned\n\nMadNLP looks for a primal-dual solution (w y z) satisfying the KKT conditions.\n\nThe Lagrangian of the problem is:\n\nL(w y z) = f(w) + c(w)^top y - z^top w  ","category":"section"},{"location":"algorithm/#Fixed-variables","page":"Algorithm","title":"Fixed variables","text":"If for some i=1 cdotsn we have x_elli = x_ui, the variable's lower bound is equal to its upper bound, meaning the variable is fixed. By default, MadNLP removes all the fixed variables in the problem.\n\nAlternatively, MadNLP can relax all the fixed variables with a small parameter epsilon as\n\nx_elli - epsilon leq x_i leq x_ui + epsilon  \n\nThe behavior is specified by the option fixed_variable_treatment: if set to MakeParameter, MadNLP removes the fixed variables, and if it is set to RelaxBound MadNLP relaxes the bounds.","category":"section"},{"location":"algorithm/#Equality-constraints","page":"Algorithm","title":"Equality constraints","text":"As discussed before, MadNLP keeps the equality constraints g_E(x) = 0 untouched in the problem. However, it is sometimes appropriate to relax the equality constraints by converting them to (tight) inequality constraints:\n\n-tau leq g_E(x) leq tau  \n\nThat behavior is determined by the option equality_treatment. It is set to EnforceEquality by default. If otherwise it is set to RelaxEquality, the equality constraints are relaxed as inequality constraints.","category":"section"},{"location":"algorithm/#Scaling","page":"Algorithm","title":"Scaling","text":"Once the problem has been reformulated, MadNLP scales the objective and the constraints to ensure that the gradient and the rows of the Jacobian have all a norm being less than nlp_scaling_max_gradient (by default equal to 100.0). The lower the value of nlp_scaling_max_gradient, the more aggressive the scaling gets.\n\nThe scaling can be deactivated by setting nlp_scaling=false.","category":"section"},{"location":"algorithm/#Computing-the-initial-primal-dual-iterate","page":"Algorithm","title":"Computing the initial primal-dual iterate","text":"The user can pass to MadNLP an initial primal point x_0. MadNLP modifies it to ensure it is strictly feasible by applying the operation\n\nx_0j = max(x_0j kappa_1)  \n\nwith kappa_1 the parameter specified in the option bound_push.\n\nIf dual_initialized=false (default), MadNLP computes the initial dual multiplier y_0 as solution of the least-square problem\n\nbeginbmatrix\nI  J_0^top \nJ_0  0\nendbmatrix\nbeginbmatrix\nw  y_0\nendbmatrix\n=\n-\nbeginbmatrix\n nabla f(x_0) - z_0   0\nendbmatrix  \n\nOtherwise, if dual_initialized=true, MadNLP uses the values passed by the user.\n\ninfo: Info\nThe user cannot pass the initial value of the bound multiplier. MadNLP automatically sets z_0 = 1.","category":"section"},{"location":"algorithm/#Interior-point-iterations","page":"Algorithm","title":"Interior-point iterations","text":"MadNLP solves the KKT conditions iteratively using a globalized Newton algorithm. The interior-point method reformulates the KKT conditions using a homotopy method, with, for a positive barrier parameter mu  0:\n\nbeginaligned\n nabla f(w) + nabla c(w)^top y - z = 0   \n c(w) = 0   \n WZe = mu e   (w z)  0  \nendaligned\n\nThe algorithm stops as soon as max(inf_pr, inf_du, inf_compl) < tol, with\n\nbeginaligned\n textttinf_pr =  nabla f(w) + nabla c(w)^top y - z _infty   \n textttinf_du =  c(w) _infty   \n textttinf_compl =  WZe _infty  \nendaligned\n\nThe tolerance tol is set to 1e-8 by default.\n\nIf the stopping criterion is not satisfied, MadNLP moves to the next iteration (here denoted with an index k). The iteration proceeds in four steps.","category":"section"},{"location":"algorithm/#Step-1:-Computing-the-first-and-second-order-sensitivities","page":"Algorithm","title":"Step 1: Computing the first and second-order sensitivities","text":"First, MadNLP evaluates the Jacobian of the constraints J_k = nabla c(w_k)^top and the Hessian of the Lagrangian H_k = nabla_xx^2 L(w_k y_k z_k).\n\nMadNLP evaluates the sensitivities inside an AbstractCallback, which acts as a buffer between the solver and the model implemented with NLPModels. Compared to the original model, the AbstractCallback adds the slack variable and scales the problem appropriately.\n\nBy default the two matrices J_k and H_k are assumed sparse, with only the non-zero entries being stored. If convenient, the user can evaluate them in dense format by switching to a DenseCallback by setting the option:\n\ncallback=DenseCallback","category":"section"},{"location":"algorithm/#Step-2:-Updating-the-barrier-parameter","page":"Algorithm","title":"Step 2: Updating the barrier parameter","text":"Once the sensitivities are evaluated, MadNLP updates the barrier parameter mu. By default, MadNLP uses the monotone update rule inspired by the Fiacco-McCormick rule: MonotoneUpdate. Alternatively, MadNLP provides two adaptive update rules, implemented in the solver respectively as QualityFunctionUpdate and LOQOUpdate. The user can change the barrier update, e.g. by setting the option\n\nbarrier=QualityFunctionUpdate()\n\ninfo: Info\nWe recommend using an adaptive barrier update for difficult problems, in particular if we have a poor initial iterate x_0.","category":"section"},{"location":"algorithm/#Step-3:-Solving-the-primal-dual-KKT-system","page":"Algorithm","title":"Step 3: Solving the primal-dual KKT system","text":"With the new barrier parameter, we can compute a new KKT residual. MadNLP aims at decreasing this residual by computing a Newton step (Delta w Delta y Delta z) solution of the primal-dual KKT system:\n\nbeginbmatrix\nH_k +delta_x I  J_k^top  -I \nJ_k  -delta_y  0 \nZ_k  0  W_k\nendbmatrix\nbeginbmatrix\nDelta x  Delta y  Delta z\nendbmatrix\n= -\nbeginbmatrix\n nabla f(w_k) + nabla c(w_k)^top y_k - z_k \n c(w_k)  \n W_k Z_k e - mu e\nendbmatrix  \n\nwith delta_x and delta_y appropriate primal-dual regularization terms whose exact roles are detailed hereafter.","category":"section"},{"location":"algorithm/#Solution-of-the-primal-dual-KKT-system","page":"Algorithm","title":"Solution of the primal-dual KKT system","text":"The linear system is called a primal-dual KKT system. MadNLP can solve a symmetrized version of the primal-dual KKT system, known as an AbstractUnreducedKKTSystem. However, it is beneficial to remove the blocks associated to Delta z  and solve the smaller AbstractReducedKKTSystem:\n\nbeginbmatrix\nH_k + Sigma_k + delta_x I  J_k^top \nJ_k  -delta_y\nendbmatrix\nbeginbmatrix\nDelta x  Delta y\nendbmatrix\n= -\nbeginbmatrix\n nabla f(w_k) + nabla c(w_k)^top y_k - mu_k W_k^-1 e \n c(w_k)\nendbmatrix\n\nwith the diagonal matrix Sigma_k = W_k^-1 Z_k. The default implementation of the AbstractReducedKKTSystem is provided in SparseKKTSystem. If the problem exhibits significant ill-conditioning, the user can also use a ScaledSparseKKTSystem, which is more numerically stable than the SparseKKTSystem.\n\nThere exists a smaller system AbstractCondensedKKTSystem that also removes the blocks associated to the inequality constraints. This system is useful if the problem has many inequality constraints, or more importantly, to run MadNLP on the GPU. The user can switch the KKT system by using the option\n\nkkt_system=SparseUnreducedKKTSystem\n\nFor certain highly structured problem, it may be beneficial to implement a custom KKT system to exploit the problem's structure.","category":"section"},{"location":"algorithm/#Inertia-correction","page":"Algorithm","title":"Inertia-correction","text":"The vector (Delta w Delta y Delta z) is a descent direction if the reduced Hessian (the Hessian H_k projected on the null-space of the Jacobian J_k) of the primal-dual KKT system is positive definite. The reduced Hessian is positive definite if and only if the inertia of the reduced primal-dual KKT system (the number of positive, zero and negative eigenvalues) should exactly be equal to (n 0 m).\n\nMadNLP uses an inertia correction mechanism that increases the values of the primal and dual regularizations (delta_x delta_y) in the KKT system until the inertia of the system is exactly (n 0 m). This procedure requires using an inertia-revealing inertia solver that returns explicitly the inertia of the system as an output of the factorization. This procedure can be costly, as it involves re-factorizing the linear system each time a new regularization is tried.\n\nIf the linear solver does not compute the inertia of the KKT system, MadNLP uses the inertia-free algorithm described in this article. This procedure is less stringent than the classical inertia-based correction, in the sense that it doesn't require the reduced Hessian to be positive definite. The inertia-free correction can be activated explicitly by using the option inertia_correction_method=InertiaFree.","category":"section"},{"location":"algorithm/#Step-4:-Finding-the-next-iterate-using-the-filter-line-search","page":"Algorithm","title":"Step 4: Finding the next iterate using the filter line-search","text":"MadNLP uses the descent direction (Delta w Delta y Delta z) to compute the next iterate as\n\nw_k+1 = w_k + alpha_k^p Delta w   quad\ny_k+1 = y_k + alpha_k^d Delta y   quad\nz_k+1 = z_k + alpha_k^d Delta z  \n\nFirst, the algorithm computes the maximum primal-dual steps (alpha_k^pmax alpha_k^dmax) satisfying the fraction-to-boundary rule:\n\nw_k + alpha_k^pmax Delta w geq (1-tau) w_k    quad\nz_k + alpha_k^dmax Delta z geq (1-tau) z_k   \n\nThis ensures that the iterates remain positive: (w_k+1 z_k+1)  0. In MadNLP, tau = max(tau_min 1-mu), with tau_min a parameter defined by the option tau_min.\n\nOnce the maximum steps computed with the fraction-to-boundary rule, MadNLP finds the new iterate by using a backtracking line-search by testing different trial values for alpha_kl = frac12^l alpha_k^pmax. Once a new trial iterate w_kl = w_k + alpha_kl Delta w achieves sufficient progress, the line-search stops and MadNLP proceeds to the next iteration. The progress is measured by a filter, which accepts a new point either if it reduces the value of the barrier function phi_mu(w) = f(w) - mu sum_i=1^n log(w_i) or it reduces the constraint violation c(x)_1, by comparing these two values with those of a list of past iterates.","category":"section"},{"location":"algorithm/#Feasility-restoration-phase","page":"Algorithm","title":"Feasility restoration phase","text":"If no acceptable step is found by the line search, MadNLP switches to a restoration phase that attempts at projecting the current iterate onto the feasible set. The problem solved by the restoration phase is:\n\nbeginaligned\nmin_w   c(w) _1 \ntextsubject to quad  x geq 0  \nendaligned\n\nOnce the constraint violation has been significantly reduced, MadNLP returns to the classical algorithm. If the feasibility restoration phase converges to a solution of the feasibility restoration problem with a positive objective, the problem is detected as being locally infeasible.","category":"section"},{"location":"options/#MadNLP.MadNLPOptions","page":"Options","title":"MadNLP.MadNLPOptions","text":"Option Default Value Description\n  \nPrimary options  \n  \ntol 1e-8 termination tolerance on KKT residual\ncallback SparseCallback type of callback (SparseCallback or DenseCallback)\nkkt_system SparseKKTSystem type of primal-dual KKT system\nlinear_solver MumpsSolver linear solver used for solving primal-dual KKT system\n  \nGeneral options  \n  \nrethrow_error true rethrow any error encountered during the algorithm\ndisable_garbage_collector false disable garbage collector in MadNLP\nblas_num_thread 1 number of threads to use in the BLAS backend\nOutput options  \n  \noutput_file \"\" if not \"\", the output log is teed to the file at this path\nprint_level INFO verbosity level in MadNLP\nfile_print_level INFO verbosity level in file output\n  \nTermination options  \n  \nmax_iter 3000 maximum number of interior-point iterations\nmax_wall_time 1e6 maximum wall time in seconds\nacceptable_tol 1e-6 acceptable tolerance on KKT residual\nacceptable_iter 15 number of acceptable iterates before stopping algorithm\ndiverging_iter 1e20 threshold on KKT residual to declare algorithm as diverging\ns_max 100.0 scaling threshold for KKT residual\n  \nNLP options  \n  \nkappa_d 1e-5 weight for linear damping term\nfixed_variable_treatment MakeParameter treatment for the fixed variables (MakeParameter or RelaxBound)\nequality_treatment EnforceEquality treatment for the equality constraints (EnforceEquality or RelaxEquality)\nbound_relax_factor 1e-8 factor for initial relaxation of bounds\njacobian_constant false set to true if the constraints are linear.\nhessian_constant false set to true if the problem is linear or quadratic\nhessian_approximation ExactHessian method used to approximate the Hessian\nquasi_newton_options QuasiNewtonOptions() options for quasi-Newton algorithm\ninertia_correction_method InertiaAuto inertia correction mechanism\ninertia_free_tol 0.0 tolerance for inertia free method\n  \nInitialization  \n  \ndual_initialized false specify if dual initial point is available\ndual_initialization_method DualInitializeLeastSquares method to compute the initial dual multipliers\nconstr_mult_init_max 1e3 maximum allowable value in initial dual multipliers\nbound_push 1e-2 minimum absolute distance from the initial point to bound\nbound_fac 1e-2 minimum relative distance from the initial point to bound\nnlp_scaling true scale nonlinear program\nnlp_scaling_max_gradient 100.0 maximum gradient after NLP scaling\n  \nHessian perturbation  \n  \nmin_hessian_perturbation 1e-20 smallest perturbation of Hessian block in inertia correction\nfirst_hessian_perturbation 1e-4 first value tried in inertia correction\nmax_hessian_perturbation 1e20 largest perturbation of Hessian block in inertia correction\nperturb_inc_fact_first 1e2 increase factor for primal perturbation for very first perturbation\nperturb_inc_fact 8. increase factor for primal perturbation\nperturb_dec_fact 1/3 decrease factor for primal perturbation\njacobian_regularization_exponent 1/4 exponent for mu in the regularization of rank-defficient Jacobian\njacobian_regularization_value 1e-8 size of regularization for rank-defficient Jacobian\n  \nFeasible restoration  \n  \nsoft_resto_pderror_reduction_factor 0.9999 required reduction in primal-dual error in the soft restoration phase\nrequired_infeasibility_reduction 0.9 required reduction of infeasibility before leaving restoration phase\n  \nLine search  \n  \nobj_max_inc 5. upper bound on the acceptable increase of barrier objective function\nkappha_soc 0.99 factor in the sufficient reduction rule for second order correction\nmax_soc 4 maximum number of second order correction trial steps at each iteration\nalpha_min_frac 0.05 safety factor for the minimal step size\ns_theta 1.1 exponent for current constraint violation in the switching rule\ns_phi 2.3 exponent for linear barrier function model in the switching rule\neta_phi 1e-4 relaxation factor in the Armijo condition\nkappa_soc 0.99 factor in the sufficient reduction rule for second order correction\ngamma_theta 1e-5 relaxation factor in the filter margin for the constraint violation\ngamma_phi 1e-5 relaxation factor in the filter margin for the barrier function\ndelta 1.0 multiplier for constraint violation in the switching rule\nkappa_sigma 1e10 factor limiting the deviation of dual variables from primal estimates\nbarrier_tol_factor 10.0 factor for mu in barrier stop test\nrho 1000.0 value in penalty parameter update formula\n  \nBarrier  \n  \nbarrier MonotoneUpdate algorithm to update barrier parameter\ntau_min 0.99 lower bound on fraction-to-the-boundary parameter tau\n  \n\n\n\n\n\n","category":"type"},{"location":"lib/callbacks/#Callbacks","page":"Callback wrappers","title":"Callbacks","text":"In MadNLP, a nonlinear program is implemented with a given AbstractNLPModel. The model may have a form unsuitable for the interior-point algorithm. For that reason, MadNLP wraps the AbstractNLPModel internally using custom data structures, encoded as a AbstractCallback. Depending on the setting, choose to wrap the AbstractNLPModel as a DenseCallback or alternatively, as a SparseCallback.\n\nThe function create_callback allows to instantiate a AbstractCallback from a given NLPModel:\n\nInternally, a AbstractCallback reformulates the inequality constraints as equality constraints by introducing additional slack variables. The fixed variables are reformulated as parameters (using MakeParameter) or are relaxed (using RelaxBound). The equality constraints can be keep as is with EnforceEquality (default option) or relaxed as inequality constraints with RelaxEquality. In that later case, MadNLP solves a relaxation of the original problem.\n\nMadNLP has to keep in memory all the indexes associated to the equality and inequality constraints, as well as the indexes of the bounded variables and the fixed variables. The indexes are stored explicitly as a Vector{Int} in the AbstractCallback structure used by MadNLP.","category":"section"},{"location":"lib/callbacks/#MadNLP.AbstractCallback","page":"Callback wrappers","title":"MadNLP.AbstractCallback","text":"AbstractCallback{T, VT}\n\nWrap the AbstractNLPModel passed by the user in a form amenable to MadNLP.\n\nAn AbstractCallback handles the scaling of the problem and the reformulations of the equality constraints and fixed variables.\n\n\n\n\n\n","category":"type"},{"location":"lib/callbacks/#MadNLP.DenseCallback","page":"Callback wrappers","title":"MadNLP.DenseCallback","text":"DenseCallback{T, VT} < AbstractCallback{T, VT}\n\nWrap an AbstractNLPModel using dense structures.\n\n\n\n\n\n","category":"type"},{"location":"lib/callbacks/#MadNLP.SparseCallback","page":"Callback wrappers","title":"MadNLP.SparseCallback","text":"SparseCallback{T, VT} < AbstractCallback{T, VT}\n\nWrap an AbstractNLPModel using sparse structures.\n\n\n\n\n\n","category":"type"},{"location":"lib/callbacks/#MadNLP.create_callback","page":"Callback wrappers","title":"MadNLP.create_callback","text":"create_callback(\n    ::Type{Callback},\n    nlp::AbstractNLPModel{T, VT};\n    fixed_variable_treatment=MakeParameter,\n    equality_treatment=EnforceEquality,\n) where {T, VT}\n\nWrap the nonlinear program nlp using the callback wrapper with type Callback. The option fixed_variable_treatment decides if the fixed variables are relaxed (RelaxBound) or removed (MakeParameter). The option equality_treatment decides if the the equality constraints are keep as is (EnforceEquality) or relaxed (RelaxEquality).\n\n\n\n\n\n","category":"function"},{"location":"lib/callbacks/#MadNLP.AbstractFixedVariableTreatment","page":"Callback wrappers","title":"MadNLP.AbstractFixedVariableTreatment","text":"AbstractFixedVariableTreatment\n\nAbstract type to define the reformulation of the fixed variables inside MadNLP.\n\n\n\n\n\n","category":"type"},{"location":"lib/callbacks/#MadNLP.MakeParameter","page":"Callback wrappers","title":"MadNLP.MakeParameter","text":"MakeParameter{VT, VI} <: AbstractFixedVariableTreatment\n\nRemove the fixed variables from the optimization variables and define them as problem's parameters.\n\n\n\n\n\n","category":"type"},{"location":"lib/callbacks/#MadNLP.RelaxBound","page":"Callback wrappers","title":"MadNLP.RelaxBound","text":"RelaxBound <: AbstractFixedVariableTreatment\n\nRelax the fixed variables x = x_fixed as bounded variables x_fixed - ϵ  x  x_fixed + ϵ, with ϵ a small-enough parameter.\n\n\n\n\n\n","category":"type"},{"location":"lib/callbacks/#MadNLP.AbstractEqualityTreatment","page":"Callback wrappers","title":"MadNLP.AbstractEqualityTreatment","text":"AbstractEqualityTreatment\n\nAbstract type to define the reformulation of the equality constraints inside MadNLP.\n\n\n\n\n\n","category":"type"},{"location":"lib/callbacks/#MadNLP.EnforceEquality","page":"Callback wrappers","title":"MadNLP.EnforceEquality","text":"EnforceEquality <: AbstractEqualityTreatment\n\nKeep the equality constraints intact.\n\nThe solution returned by MadNLP will respect the equality constraints.\n\n\n\n\n\n","category":"type"},{"location":"lib/callbacks/#MadNLP.RelaxEquality","page":"Callback wrappers","title":"MadNLP.RelaxEquality","text":"RelaxEquality <: AbstractEqualityTreatment\n\nRelax the equality constraints g(x) = 0 with two inequality constraints, as -ϵ  g(x)  ϵ. The parameter ϵ is usually small.\n\nThe solution returned by MadNLP will satisfy the equality constraints only up to a tolerance ϵ.\n\n\n\n\n\n","category":"type"},{"location":"","page":"Home","title":"Home","text":"(Image: Logo)\n\nMadNLP is an open-source nonlinear programming solver, purely implemented in Julia. MadNLP implements a filter line-search interior-point algorithm, as used in Ipopt. MadNLP seeks to streamline the development of modeling and algorithmic paradigms in order to exploit structures and to make efficient use of high-performance computers. Notably, MadNLP is part of MadSuite, a suite of GPU-accelerated optimization solvers.","category":"section"},{"location":"#Design","page":"Home","title":"Design","text":"","category":"section"},{"location":"#MadNLP's-problem-structure","page":"Home","title":"MadNLP's problem structure","text":"MadNLP targets the solution of constrained nonlinear problems, formulating as\n\n  beginaligned\n    min_x   f(x) \n    textsubject to quad  g_ell leq g(x) leq g_u \n                             x_ell leq x leq x_u\n  endaligned\n\nwhere x in mathbbR^n is the decision variable, f mathbbR^n to mathbbR and g mathbbR^n to mathbbR^m two smooth nonlinear functions. MadNLP makes the distinction between the bound constraints x_ell leq x leq x_u and the generic constraints g_ell leq g(x) leq g_u. No other structure is assumed a priori.\n\nMadNLP is built on top of NLPModels.jl, a generic package to represent optimization models in Julia. In addition, MadNLP is interfaced with ExaModels, JuMP and Plasmo.","category":"section"},{"location":"#MadNLP's-performance","page":"Home","title":"MadNLP's performance","text":"In any interior-point algorithm, the two computational bottlenecks are\n\nThe evaluation of the first- and second-order derivatives.\nThe factorization of the primal-dual KKT system.\n\nThe first point is problem-dependent and is often related to the automatic differentiation backend being employed. The second point is more problematic, as the primal-dual KKT system is symmetric indefinite and ill-conditioned.","category":"section"},{"location":"#Linear-solvers","page":"Home","title":"Linear solvers","text":"By default, MadNLP is using MUMPS to solve the primal-dual KKT system. Other efficient linear solvers are interfaced, all listed in the table below.\n\nLinear solver Type Hardware\n  \nMadNLP (base)  \n  \nMumpsSolver (default) sparse CPU\nUmfpackSolver sparse CPU\nLapackCPUSolver dense CPU\nLDLSolver sparse CPU\nCHOLMODSolver sparse CPU\n  \nMadNLPGPU  \n  \nCUDSSSolver sparse CUDA\nLapackCUDASolver dense CUDA\nLapackROCmSolver dense AMD GPU\n  \nMadNLPHSL  \n  \nMa27Solver sparse CPU\nMa57Solver sparse CPU\nMa86Solver sparse CPU\nMa97Solver sparse CPU\n  \nMadNLPPardiso  \n  \nPardisoSolver sparse CPU\nPardisoMKLSolver sparse CPU\n\nwarning: Warning\nIn general, we recommend using a sparse linear solver as soon as the number of variables in the problem is greater than 1,000.","category":"section"},{"location":"#Citing-MadNLP.jl","page":"Home","title":"Citing MadNLP.jl","text":"If you use MadNLP.jl in your research, we would greatly appreciate your citing it.\n\n@article{shin2024accelerating,\n  title     = {Accelerating optimal power flow with {GPU}s: {SIMD} abstraction of nonlinear programs and condensed-space interior-point methods},\n  author    = {Shin, Sungho and Anitescu, Mihai and Pacaud, Fran{\\c{c}}ois},\n  journal   = {Electric Power Systems Research},\n  volume    = {236},\n  pages     = {110651},\n  year      = {2024},\n  publisher = {Elsevier}\n}\n\n@article{shin2021graph,\n  title     = {Graph-based modeling and decomposition of energy infrastructures},\n  author    = {Shin, Sungho and Coffrin, Carleton and Sundar, Kaarthik and Zavala, Victor M},\n  journal   = {IFAC-PapersOnLine},\n  volume    = {54},\n  number    = {3},\n  pages     = {693--698},\n  year      = {2021},\n  publisher = {Elsevier}\n}","category":"section"}]
}
