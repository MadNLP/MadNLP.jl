var documenterSearchIndex = {"docs":
[{"location":"tutorials/gpu/#Running-MadNLP-on-the-GPU","page":"GPU acceleration","title":"Running MadNLP on the GPU","text":"MadNLP supports the solution of large-scale optimization problems on the GPU, with significant speedups reported on some instances. In this tutorial, we show how to solve a nonlinear program on the GPU with ExaModels and MadNLP.","category":"section"},{"location":"tutorials/gpu/#Generic-principles","page":"GPU acceleration","title":"Generic principles","text":"MadNLP has been designed to run entirely on the GPU, without data exchange between the host and the device. If the model is well specified, deporting the solution on a NVIDIA GPU is seamless, using:\n\nExaModels to evaluate the nonlinear model and its derivatives on the GPU;\nNVIDIA cuDSS to solve the linear KKT systems on the GPU;\nKernelAbstractions to deport MadNLP's internal computations on the GPU.\n\nWe import the aforementioned packages as:\n\nusing ExaModels\nusing MadNLPGPU\nusing CUDA\n\ninfo: Info\nOn the contrary to ExaModels, MadNLP does not yet support solving sparse optimization problems on AMD GPUs.","category":"section"},{"location":"tutorials/gpu/#Evaluating-the-model-on-the-GPU-with-ExaModels","page":"GPU acceleration","title":"Evaluating the model on the GPU with ExaModels","text":"The first step requires implementing the model with ExaModels.\n\nAs a demonstration, we implement the model airport from the CUTEst benchmark using ExaModels. The code writes:\n\nfunction airport_model(T, backend)\n    N = 42\n    # Data\n    r = T[0.09 , 0.3, 0.09, 0.45, 0.5, 0.04, 0.1, 0.02, 0.02, 0.07, 0.4, 0.045, 0.05, 0.056, 0.36, 0.08, 0.07, 0.36, 0.67, 0.38, 0.37, 0.05, 0.4, 0.66, 0.05, 0.07, 0.08, 0.3, 0.31, 0.49, 0.09, 0.46, 0.12, 0.07, 0.07, 0.09, 0.05, 0.13, 0.16, 0.46, 0.25, 0.1]\n    cx = T[-6.3, -7.8, -9.0, -7.2, -5.7, -1.9, -3.5, -0.5, 1.4, 4.0, 2.1, 5.5, 5.7, 5.7, 3.8, 5.3, 4.7, 3.3, 0.0, -1.0, -0.4, 4.2, 3.2, 1.7, 3.3, 2.0, 0.7, 0.1, -0.1, -3.5, -4.0, -2.7, -0.5, -2.9, -1.2, -0.4, -0.1, -1.0, -1.7, -2.1, -1.8, 0.0]\n    cy = T[8.0, 5.1, 2.0, 2.6, 5.5, 7.1, 5.9, 6.6, 6.1, 5.6, 4.9, 4.7, 4.3, 3.6, 4.1, 3.0, 2.4, 3.0, 4.7, 3.4, 2.3, 1.5, 0.5, -1.7, -2.0, -3.1, -3.5, -2.4, -1.3, 0.0, -1.7, -2.1, -0.4, -2.9, -3.4, -4.3, -5.2, -6.5, -7.5, -6.4, -5.1, 0.0]\n    # Wrap all data in a single iterator for ExaModels\n    data = [(i, cx[i], cy[i], r[i]) for i in 1:N]\n    IJ = [(i, j) for i in 1:N-1 for j in i+1:N]\n    # Write model using ExaModels\n    core = ExaModels.ExaCore(T; backend=backend)\n    x = ExaModels.variable(core, 1:N, lvar = -10.0, uvar=10.0)\n    y = ExaModels.variable(core, 1:N, lvar = -10.0, uvar=10.0)\n    ExaModels.objective(\n        core,\n        ((x[i] - x[j])^2 + (y[i] - y[j])^2) for (i, j) in IJ\n    )\n    ExaModels.constraint(core, (x[i]-dcx)^2 + (y[i] - dcy)^2 - dr for (i, dcx, dcy, dr) in data; lcon=-Inf)\n    return ExaModels.ExaModel(core)\nend\n\nThe first argument T specifies the numerical precision (Float32, Float64 or any AbstractFloat) whereas the second argument backend sets the device used to evaluate the model. We instantiate the model on the GPU with:\n\nnlp = airport_model(Float64, CUDABackend())\n\nBy passing a CUDABackend(), we make sure that all the attributes in nlp are instantiated on the GPU. E.g., the initial point becomes a CuVector:\n\nNLPModels.get_x0(nlp)\n\ninfo: Info\nIf your problem is implemented with JuMP in model, ExaModel can load it for you on the GPU just by using nlp = ExaModel(model; backend=CUDABackend()).","category":"section"},{"location":"tutorials/gpu/#Solving-the-problem-on-the-GPU-with-MadNLP","page":"GPU acceleration","title":"Solving the problem on the GPU with MadNLP","text":"Once the model nlp loaded on the GPU, you can solve it using the function madnlp:\n\nresults = madnlp(nlp; linear_solver=CUDSSSolver)\nnothing\n\nWhen solving an optimization problem on the GPU, MadNLP proceeds to some automatic modifications. In order:\n\nIt increases the parameter bound_relax_factor to 1e-4.\nIt relaxes all the equality constraints g(x) = 0 as a pair of inequality constraints -tau leq g(x) leq tau, with tau being equal to bound_relax_factor;\nIt reduces the KKT system down to sparse condensed system, exploiting the fact that the relaxed problem has only (potentially tight) inequality constraints. Up to a given primal regularization, the resulting KKT system is positive definite and can be factorized using a pivoting-free factorization routines (e.g. a Cholesky or an LDL' decompositions). This is the so-called Lifted-KKT formulation documented in this article.\nThe new condensed KKT system increases the ill-conditioning inherent to the interior-point method, amplifying the loss of accuracy when the iterates get close to a local solution. As a result, the termination tolerance tol is also increased to 1e-4 to recover convergence.\n\nAs a result, the convergence observed can be significantly different than what we observe on the CPU. In particular, relaxing the parameter bound_relax_factor has a non-marginal impact on the feasible set's geometry. You can limit the loss of accuracy by specifying explicitly the relaxation and tolerance parameters to MadNLP:\n\nresults = madnlp(\n    nlp;\n    tol=1e-7,\n    bound_relax_factor=1e-7,\n    linear_solver=CUDSSSolver,\n)\nnothing\n\nDecreasing the tolerance tol too much is likely to cause some numerical issues inside the algorithm. In general, we recommend keeping the value of bound_relax_factor below tol.","category":"section"},{"location":"tutorials/gpu/#Solving-the-problem-on-the-GPU-with-HyKKT","page":"GPU acceleration","title":"Solving the problem on the GPU with HyKKT","text":"Some applications require accurate solutions. In that case, we recommend using the extension HybridKKT.jl, which implements the Golub & Greif augmented Lagrangian formulation detailed in this article. Compared to Lifted-KKT, the Hybrid-KKT strategy is more accurate (it doesn't relax the equality constraints in the problem) but slightly slower (it computes the descent direction using a conjugate gradient at every IPM iterations).\n\nOnce the package HybridKKT installed, the solution proceeds as\n\nusing HybridKKT\n\nresults = madnlp(\n    nlp;\n    linear_solver=MadNLPGPU.CUDSSSolver,\n    kkt_system=HybridKKT.HybridCondensedKKTSystem,\n    equality_treatment=MadNLP.EnforceEquality,\n    fixed_variable_treatment=MadNLP.MakeParameter,\n)\nnothing","category":"section"},{"location":"man/solver/","page":"IPM solver","title":"IPM solver","text":"MadNLP is an interior-point solver based on a filter line-search. We detail here the inner machinery happening at each MadNLP's iteration.\n\nWe recall that MadNLP is a primal-dual interior-point method and starts from an initial primal-dual variables (x_0 s_0 y_0).","category":"section"},{"location":"man/solver/#What-happens-at-iteration-k?","page":"IPM solver","title":"What happens at iteration k?","text":"At iteration k, MadNLP aims at finding a new iterate (x_k+1 s_k+1 y_k+1) improving the current iterate (x_k s_k y_k), in the sense that the new iterate (i) improves the objective or (ii) decreases the infeasibility. The exact trade-off between (i) and (ii) is handled by the filter line-search.\n\nThe algorithm follows the steps:\n\nCheck problem convergence E_0(x_k s_k y_k)  varepsilon_tol\nIf necessary, update the barrier parameter mu_k\nEvaluate the Hessian of the Lagrangian W_k and the Jacobian A_k with the callbacks\nAssemble the KKT system and compute the search direction d_k by solving the resulting linear system.\nIf necessary, regularize the KKT system to guarantee that d_k is a descent direction\nRun the backtracking line-search and find a step alpha_k\nDefine the next iterate as x_k+1 = x_k + alpha_k d_k^x, s_k+1 = s_k + alpha_k d_k^s, y_k+1 = y_k + alpha_k d_k^y.\n\nWe detail each step in the following paragraphs.\n\nnote: Note\nIn general, Step 3 (Hessian and Jacobian evaluations) and Step 4 (solving the KKT system) are the two most numerically demanding steps.","category":"section"},{"location":"man/solver/#Step-1:-When-the-algorithm-stops?","page":"IPM solver","title":"Step 1: When the algorithm stops?","text":"MadNLP stops once the solution satisfies a specified accuracy varepsilon_tol (by default 10^-8). MadNLP uses the same stopping criterion as Ipopt by defining\n\nE_mu(x_k s_k y_k nu_k w_k) =\nmax left\nbeginaligned\n nabla f(x_k) + A_k^top y_k + nu_k + w_k _infty \n g(x_k) - s_k _infty \n X_knu_k - mu e _infty \n S_k w_k - mu e _infty\nendaligned\nright\n\n\nand stopping the algorithm whenever\n\nE_0(x_k s_k y_k nu_k w_k)  varepsilon_tol\n","category":"section"},{"location":"man/solver/#Step-2:-How-to-update-the-barrier-parameter-\\mu?","page":"IPM solver","title":"Step 2: How to update the barrier parameter mu?","text":"TODO","category":"section"},{"location":"man/solver/#Step-4:-How-do-we-solve-the-KKT-system?","page":"IPM solver","title":"Step 4: How do we solve the KKT system?","text":"Solving the KKT system happens in two substeps:\n\nAssembling the KKT matrix K.\nSolve the system Kx = b with a linear solver.\n\nIn substep 1, MadNLP reads the Hessian and the Jacobian computed at Step 3 and build the associated KKT system in an AbstractKKTSystem. As a result, we get a matrix K_k encoding the KKT system at iteration k.\n\nIn substep 2, the matrix K_k is factorized by a compatible linear solver. Then a solution x is returned by applying a backsolve.","category":"section"},{"location":"man/solver/#Step-5:-How-to-regularize-the-KKT-system?","page":"IPM solver","title":"Step 5: How to regularize the KKT system?","text":"TODO","category":"section"},{"location":"man/solver/#Step-6:-What-is-a-filter-line-search?","page":"IPM solver","title":"Step 6: What is a filter line-search?","text":"TODO","category":"section"},{"location":"man/solver/#What-happens-if-the-line-search-failed?","page":"IPM solver","title":"What happens if the line-search failed?","text":"If inside the line-search algorithm the step alpha_k becomes negligible (10^-8) then we consider the line-search has failed to find a next iterate along the current direction d_k. If that happens during several consecutive iterations, MadNLP enters into a feasible restoration phase. The goal of feasible restoration is to decrease the primal infeasibility, to move the current iterate closer to the feasible set.","category":"section"},{"location":"installation/#Installation","page":"Installation","title":"Installation","text":"To install MadNLP, simply proceed to\n\npkg> add MadNLP\n\nIn addition to MUMPS, SuiteSparse and LAPACK, the user can install the following extensions to use a specialized linear solver.","category":"section"},{"location":"installation/#HSL-linear-solver","page":"Installation","title":"HSL linear solver","text":"Obtain a license and download HSL_jll.jl from https://licences.stfc.ac.uk/products/Software/HSL/LibHSL. Install this download into your current environment using:\n\nimport Pkg\nPkg.develop(path = \"/full/path/to/HSL_jll.jl\")\n\nIf the user has already compiled the HSL solver library, one can simply override the path to the artifact by editing ~/.julia/artifacts/Overrides.toml\n\n# replace HSL_jll artifact /usr/local/lib/libhsl.so\necece3e2c69a413a0e935cf52e03a3ad5492e137 = \"/usr/local\"","category":"section"},{"location":"installation/#Pardiso-linear-solver","page":"Installation","title":"Pardiso linear solver","text":"To use Pardiso, the user needs to obtain the Pardiso shared libraries from https://panua.ch/, provide the absolute path to the shared library:\n\njulia> ENV[\"MADNLP_PARDISO_LIBRARY_PATH\"] = \"/usr/lib/libpardiso600-GNU800-X86-64.so\"\n\nand place the license file in the home directory. After obtaining the library and the license file, run\n\npkg> build MadNLPPardiso\n\nThe build process requires a C compiler.","category":"section"},{"location":"man/kkt/#KKT-systems","page":"KKT systems","title":"KKT systems","text":"KKT systems are linear system with a special KKT structure. MadNLP uses a special structure AbstractKKTSystem to represent internally the KKT system. The AbstractKKTSystem fulfills two goals:\n\nStore the values of the Hessian of the Lagrangian and of the Jacobian.\nAssemble the corresponding KKT matrix K.","category":"section"},{"location":"man/kkt/#A-brief-look-at-the-math","page":"KKT systems","title":"A brief look at the math","text":"We recall that at each iteration the interior-point algorithm aims at solving the following relaxed KKT equations (mu playing the role of a homotopy parameter) with a Newton method:\n\nF_mu(x s y v w) = 0\n\nwith\n\nF_mu(x s y v w) =\nleft\nbeginaligned\n     nabla f(x) + A^top y + nu + w  (F_1) \n     - y - w   (F_2) \n     g(x) - s   (F_3) \n     X v - mu e_n  (F_4) \n     S w - mu e_m  (F_5)\nendaligned\nright\n\nThe Newton step associated to the KKT equations writes\n\noverline\nbeginpmatrix\n W  0  A^top  - I  0 \n 0  0  -I  0  -I \n A  -I  0 0  0 \n V  0  0  X  0 \n 0  W  0  0  S\nendpmatrix^K_3\nbeginpmatrix\n    Delta x \n    Delta s \n    Delta y \n    Delta v \n    Delta w\nendpmatrix\n= -\nbeginpmatrix\n    F_1  F_2  F_3  F_4  F_5\nendpmatrix\n\nThe matrix K_3 is unsymmetric, but we can obtain an equivalent symmetric system by eliminating the two last rows:\n\noverline\nbeginpmatrix\n W + Sigma_x  0  A^top \n 0  Sigma_s  -I \n A  -I  0\nendpmatrix^K_2\nbeginpmatrix\n    Delta x \n    Delta s \n    Delta y\nendpmatrix\n= -\nbeginpmatrix\n    F_1 + X^-1 F_4  F_2 + S^-1 F_5  F_3\nendpmatrix\n\nwith Sigma_x = X^-1 v and Sigma_s = S^-1 w. The matrix K_2, symmetric, has a structure more favorable for a direct linear solver.\n\nIn MadNLP, the matrix K_3 is encoded as an AbstractUnreducedKKTSystem and the matrix K_2 is encoded as an AbstractReducedKKTSystem.","category":"section"},{"location":"man/kkt/#Assembling-a-KKT-system,-step-by-step","page":"KKT systems","title":"Assembling a KKT system, step by step","text":"We note that both K_3 and K_2 depend on the Hessian of the Lagrangian W, the Jacobian A and the diagonal matrices Sigma_x = X^1V and Sigma_s = S^-1W. Hence, we have to update the KKT system at each iteration of the interior-point algorithm.\n\nBy default, MadNLP stores the KKT system as a SparseKKTSystem. The KKT system takes as input a SparseCallback wrapping a given NLPModel nlp. We instantiate the callback cb with the function create_callback:\n\ncb = MadNLP.create_callback(\n    MadNLP.SparseCallback,\n    nlp,\n)\n","category":"section"},{"location":"man/kkt/#Initializing-a-KKT-system","page":"KKT systems","title":"Initializing a KKT system","text":"The size of the KKT system depends directly on the problem's characteristics (number of variables, number of of equality and inequality constraints). A SparseKKTSystem stores the Hessian and the Jacobian in sparse (COO) format. The KKT matrix can be factorized using either a dense or a sparse linear solvers. Here we use the solver provided in Lapack:\n\nlinear_solver = LapackCPUSolver\n\nWe can instantiate a SparseKKTSystem using the function create_kkt_system:\n\nkkt = MadNLP.create_kkt_system(\n    MadNLP.SparseKKTSystem,\n    cb,\n    linear_solver,\n)\n\n\nOnce the KKT system built, one has to initialize it to use it inside the interior-point algorithm:\n\nMadNLP.initialize!(kkt);\n\n\nThe user can query the KKT matrix inside kkt, simply as\n\nkkt_matrix = MadNLP.get_kkt(kkt)\n\nThis returns a reference to the KKT matrix stores internally inside kkt. Each time the matrix is assembled inside kkt, kkt_matrix is updated automatically.","category":"section"},{"location":"man/kkt/#Updating-a-KKT-system","page":"KKT systems","title":"Updating a KKT system","text":"We suppose now we want to refresh the values stored in the KKT system.","category":"section"},{"location":"man/kkt/#Updating-the-values-of-the-Hessian","page":"KKT systems","title":"Updating the values of the Hessian","text":"The Hessian part of the KKT system can be queried as\n\nhess_values = MadNLP.get_hessian(kkt)\n\n\nFor a SparseKKTSystem, hess_values is a Vector{Float64} storing the nonzero values of the Hessian. Then, one can update the vector hess_values by using NLPModels.jl:\n\nn = NLPModels.get_nvar(nlp)\nm = NLPModels.get_ncon(nlp)\nx = NLPModels.get_x0(nlp) # primal variables\nl = zeros(m) # dual variables\n\nNLPModels.hess_coord!(nlp, x, l, hess_values)\n\n\nEventually, a post-processing step is applied to refresh all the values internally:\n\nMadNLP.compress_hessian!(kkt)\n\n\nnote: Note\nBy default, the function compress_hessian! does nothing. But it can be required for very specific use-case, for instance building internally a Schur complement matrix.","category":"section"},{"location":"man/kkt/#Updating-the-values-of-the-Jacobian","page":"KKT systems","title":"Updating the values of the Jacobian","text":"We proceed exaclty the same way to update the values in the Jacobian. One queries the Jacobian values in the KKT system as\n\njac_values = MadNLP.get_jacobian(kkt)\n\n\nWe can refresh the values with NLPModels as\n\nNLPModels.jac_coord!(nlp, x, jac_values)\n\n\nAnd then applies a post-processing step as\n\nMadNLP.compress_jacobian!(kkt)\n","category":"section"},{"location":"man/kkt/#Updating-the-values-of-the-diagonal-matrices","page":"KKT systems","title":"Updating the values of the diagonal matrices","text":"Once the Hessian and the Jacobian updated, the algorithm can apply primal and dual regularization terms on the diagonal of the KKT system, to improve the numerical behavior in the linear solver. This operation is implemented inside the regularize_diagonal! function:\n\npr_value = 1.0\ndu_value = 0.0\n\nMadNLP.regularize_diagonal!(kkt, pr_value, du_value)\n","category":"section"},{"location":"man/kkt/#Assembling-the-KKT-matrix","page":"KKT systems","title":"Assembling the KKT matrix","text":"Once the values updated, one can assemble the resulting KKT matrix. This translates to\n\nMadNLP.build_kkt!(kkt)\n\nBy doing so, the values stored inside kkt will be transferred to the KKT matrix kkt_matrix (as returned by the function get_kkt):\n\nkkt_matrix\n\nInternally, a SparseKKTSystem stores the KKT system in a sparse COO format. When build_kkt! is called, the sparse COO matrix is transferred to SparseMatrixCSC if the linear solver is sparse, or alternatively to a Matrix if the linear solver is dense.\n\nnote: Note\nThe KKT system stores only the lower-triangular part of the KKT system, as it is symmetric.","category":"section"},{"location":"man/kkt/#Solution-of-the-KKT-system","page":"KKT systems","title":"Solution of the KKT system","text":"Now the KKT system is assembled in a matrix K (here stored in kkt_matrix), we want to solve a linear system K x = b, for instance to evaluate the next descent direction. To do so, we use the linear solver stored internally inside kkt (here an instance of LapackCPUSolver).\n\nWe start by factorizing the KKT matrix K:\n\nMadNLP.factorize!(kkt.linear_solver)\n\n\nBy default, MadNLP uses a LBL factorization to decompose the symmetric indefinite KKT matrix.\n\nOnce the KKT matrix has been factorized, we can compute the solution of the linear system with a backsolve. The function takes as input a AbstractKKTVector, an object used to do algebraic manipulation with a AbstractKKTSystem. We start by instantiating two UnreducedKKTVector (encoding respectively the right-hand-side and the solution):\n\nb = MadNLP.UnreducedKKTVector(kkt)\nfill!(MadNLP.full(b), 1.0)\nx = copy(b)\n\n\nThe right-hand-side encodes a vector of 1:\n\nMadNLP.full(b)\n\nWe solve the system K x = b using the solve! function:\n\nMadNLP.solve!(kkt, x)\nMadNLP.full(x)\n\nWe verify that the solution is correct by multiplying it on the left with the KKT system, using mul!:\n\nmul!(b, kkt, x) # overwrite b!\nMadNLP.full(b)\n\nWe recover a vector filled with 1, which was the initial right-hand-side.","category":"section"},{"location":"lib/linear_solvers/#Linear-solvers","page":"Linear Solvers","title":"Linear solvers","text":"","category":"section"},{"location":"lib/linear_solvers/#Direct-linear-solvers","page":"Linear Solvers","title":"Direct linear solvers","text":"Each linear solver employed in MadNLP implements the following interface.","category":"section"},{"location":"lib/linear_solvers/#Iterative-refinement","page":"Linear Solvers","title":"Iterative refinement","text":"MadNLP uses iterative refinement to improve the accuracy of the solution returned by the linear solver.","category":"section"},{"location":"lib/linear_solvers/#MadNLP.AbstractLinearSolver","page":"Linear Solvers","title":"MadNLP.AbstractLinearSolver","text":"AbstractLinearSolver\n\nAbstract type for linear solver targeting the resolution of the linear system Ax=b.\n\n\n\n\n\n","category":"type"},{"location":"lib/linear_solvers/#MadNLP.introduce","page":"Linear Solvers","title":"MadNLP.introduce","text":"introduce(::AbstractLinearSolver)\n\nPrint the name of the linear solver.\n\n\n\n\n\n","category":"function"},{"location":"lib/linear_solvers/#MadNLP.factorize!","page":"Linear Solvers","title":"MadNLP.factorize!","text":"factorize!(::AbstractLinearSolver)\n\nFactorize the matrix A and updates the factors inside the AbstractLinearSolver instance.\n\n\n\n\n\n","category":"function"},{"location":"lib/linear_solvers/#MadNLP.solve!","page":"Linear Solvers","title":"MadNLP.solve!","text":"solve!(::AbstractLinearSolver, x::AbstractVector)\n\nSolve the linear system Ax = b.\n\nThis function assumes the linear system has been factorized previously with factorize!.\n\n\n\n\n\n","category":"function"},{"location":"lib/linear_solvers/#MadNLP.is_inertia","page":"Linear Solvers","title":"MadNLP.is_inertia","text":"is_inertia(::AbstractLinearSolver)\n\nReturn true if the linear solver supports the computation of the inertia of the linear system.\n\n\n\n\n\n","category":"function"},{"location":"lib/linear_solvers/#MadNLP.inertia","page":"Linear Solvers","title":"MadNLP.inertia","text":"inertia(::AbstractLinearSolver)\n\nReturn the inertia (n, m, p) of the linear system as a tuple.\n\nNote\n\nThe inertia is defined as a tuple (n m p), with\n\nn: number of positive eigenvalues\nm: number of negative eigenvalues\np: number of zero eigenvalues\n\n\n\n\n\n","category":"function"},{"location":"lib/linear_solvers/#MadNLP.solve_refine!","page":"Linear Solvers","title":"MadNLP.solve_refine!","text":"solve_refine!(x::VT, ::AbstractIterator, b::VT, w::VT) where {VT <: AbstractKKTVector}\n\nSolve the linear system Ax = b using iterative refinement. The object AbstractIterator stores an instance of a AbstractLinearSolver for the backsolve operations.\n\nNotes\n\nThis function assumes the matrix stored in the linear solver has been factorized previously.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#KKT-systems","page":"KKT systems","title":"KKT systems","text":"MadNLP manipulates KKT systems using two abstractions: an AbstractKKTSystem storing the KKT system' matrix and an AbstractKKTVector storing the KKT system's right-hand-side.","category":"section"},{"location":"lib/kkt/#AbstractKKTSystem","page":"KKT systems","title":"AbstractKKTSystem","text":"MadNLP implements three different types of AbstractKKTSystem, depending how far we reduce the KKT system.\n\nEach AbstractKKTSystem follows the interface described below:","category":"section"},{"location":"lib/kkt/#Sparse-KKT-systems","page":"KKT systems","title":"Sparse KKT systems","text":"By default, MadNLP stores a AbstractReducedKKTSystem in sparse format, as implemented by SparseKKTSystem:\n\nAlternatively, the user has the choice to store the KKT system as a SparseUnreducedKKTSystem or as a SparseCondensedKKTSystem:","category":"section"},{"location":"lib/kkt/#Dense-KKT-systems","page":"KKT systems","title":"Dense KKT systems","text":"MadNLP provides also two structures to store the KKT system in a dense matrix. Although less efficient than their sparse counterparts, these two structures allow to store the KKT system efficiently when the problem is instantiated on the GPU.","category":"section"},{"location":"lib/kkt/#AbstractKKTVector","page":"KKT systems","title":"AbstractKKTVector","text":"Each instance of AbstractKKTVector implements the following interface.\n\nBy default, MadNLP provides one implementation of an AbstractKKTVector.","category":"section"},{"location":"lib/kkt/#MadNLP.AbstractKKTSystem","page":"KKT systems","title":"MadNLP.AbstractKKTSystem","text":"AbstractKKTSystem{T, VT<:AbstractVector{T}, MT<:AbstractMatrix{T}, QN<:AbstractHessian{T}}\n\nAbstract type for KKT system.\n\n\n\n\n\n","category":"type"},{"location":"lib/kkt/#MadNLP.AbstractUnreducedKKTSystem","page":"KKT systems","title":"MadNLP.AbstractUnreducedKKTSystem","text":"AbstractUnreducedKKTSystem{T, VT, MT, QN} <: AbstractKKTSystem{T, VT, MT, QN}\n\nAugmented KKT system associated to the linearization of the KKT conditions at the current primal-dual iterate (x s y z ν w).\n\nThe associated matrix is\n\n[Wₓₓ  0   Aₑ'  Aᵢ' V½  0  ]  [Δx]\n[0    0   0   -I   0   W½ ]  [Δs]\n[Aₑ   0   0    0   0   0  ]  [Δy]\n[Aᵢ  -I   0    0   0   0  ]  [Δz]\n[V½   0   0    0  -X   0  ]  [Δτ]\n[0    W½  0    0   0  -S  ]  [Δρ]\n\nwith\n\nWₓₓ: Hessian of the Lagrangian.\nAₑ: Jacobian of the equality constraints\nAᵢ: Jacobian of the inequality constraints\nX = diag(x)\nS = diag(s)\nV = diag(ν)\nW = diag(w)\nΔτ = -W^-½Δν\nΔρ = -W^-½Δw\n\n\n\n\n\n","category":"type"},{"location":"lib/kkt/#MadNLP.AbstractReducedKKTSystem","page":"KKT systems","title":"MadNLP.AbstractReducedKKTSystem","text":"AbstractReducedKKTSystem{T, VT, MT, QN} <: AbstractKKTSystem{T, VT, MT, QN}\n\nThe reduced KKT system is a simplification of the original Augmented KKT system. Comparing to AbstractUnreducedKKTSystem), AbstractReducedKKTSystem removes the two last rows associated to the bounds' duals (ν w).\n\nAt a primal-dual iterate (x s y z), the matrix writes\n\n[Wₓₓ + Σₓ   0    Aₑ'   Aᵢ']  [Δx]\n[0          Σₛ    0    -I ]  [Δs]\n[Aₑ         0     0     0 ]  [Δy]\n[Aᵢ        -I     0     0 ]  [Δz]\n\nwith\n\nWₓₓ: Hessian of the Lagrangian.\nAₑ: Jacobian of the equality constraints\nAᵢ: Jacobian of the inequality constraints\nΣₓ = X¹ V\nΣₛ = S¹ W\n\n\n\n\n\n","category":"type"},{"location":"lib/kkt/#MadNLP.AbstractCondensedKKTSystem","page":"KKT systems","title":"MadNLP.AbstractCondensedKKTSystem","text":"AbstractCondensedKKTSystem{T, VT, MT, QN} <: AbstractKKTSystem{T, VT, MT, QN}\n\nThe condensed KKT system simplifies further the AbstractReducedKKTSystem by removing the rows associated to the slack variables s and the inequalities.\n\nAt the primal-dual iterate (x y), the matrix writes\n\n[Wₓₓ + Σₓ + Aᵢ' Σₛ Aᵢ    Aₑ']  [Δx]\n[         Aₑ              0 ]  [Δy]\n\nwith\n\nWₓₓ: Hessian of the Lagrangian.\nAₑ: Jacobian of the equality constraints\nAᵢ: Jacobian of the inequality constraints\nΣₓ = X¹ V\nΣₛ = S¹ W\n\n\n\n\n\n","category":"type"},{"location":"lib/kkt/#MadNLP.create_kkt_system","page":"KKT systems","title":"MadNLP.create_kkt_system","text":"create_kkt_system(\n    ::Type{KKT},\n    cb::AbstractCallback,\n    ind_cons::NamedTuple,\n    linear_solver::Type{LinSol};\n    opt_linear_solver=default_options(linear_solver),\n    hessian_approximation=ExactHessian,\n) where {KKT<:AbstractKKTSystem, LinSol<:AbstractLinearSolver}\n\nInstantiate a new KKT system with type KKT, associated to the the nonlinear program encoded inside the callback cb. The NamedTuple ind_cons stores the indexes of all the variables and constraints in the callback cb. In addition, the user should pass the linear solver linear_solver that will be used to solve the KKT system after it has been assembled.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.num_variables","page":"KKT systems","title":"MadNLP.num_variables","text":"Number of primal variables (including slacks) associated to the KKT system.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.get_kkt","page":"KKT systems","title":"MadNLP.get_kkt","text":"get_kkt(kkt::AbstractKKTSystem)::AbstractMatrix\n\nReturn a pointer to the KKT matrix implemented in kkt. The pointer is passed afterward to a linear solver.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.get_jacobian","page":"KKT systems","title":"MadNLP.get_jacobian","text":"Get Jacobian matrix\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.get_hessian","page":"KKT systems","title":"MadNLP.get_hessian","text":"Get Hessian matrix\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.initialize!","page":"KKT systems","title":"MadNLP.initialize!","text":"initialize!(kkt::AbstractKKTSystem)\n\nInitialize KKT system with default values. Called when we initialize the MadNLPSolver storing the current KKT system kkt.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.build_kkt!","page":"KKT systems","title":"MadNLP.build_kkt!","text":"build_kkt!(kkt::AbstractKKTSystem)\n\nAssemble the KKT matrix before calling the factorization routine.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.compress_hessian!","page":"KKT systems","title":"MadNLP.compress_hessian!","text":"compress_hessian!(kkt::AbstractKKTSystem)\n\nCompress the Hessian inside kkt's internals. This function is called every time a new Hessian is evaluated.\n\nDefault implementation do nothing.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.compress_jacobian!","page":"KKT systems","title":"MadNLP.compress_jacobian!","text":"compress_jacobian!(kkt::AbstractKKTSystem)\n\nCompress the Jacobian inside kkt's internals. This function is called every time a new Jacobian is evaluated.\n\nBy default, the function updates in the Jacobian the coefficients associated to the slack variables.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.jtprod!","page":"KKT systems","title":"MadNLP.jtprod!","text":"jtprod!(y::AbstractVector, kkt::AbstractKKTSystem, x::AbstractVector)\n\nMultiply with transpose of Jacobian and store the result in y, such that y = A x (with A current Jacobian).\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.regularize_diagonal!","page":"KKT systems","title":"MadNLP.regularize_diagonal!","text":"regularize_diagonal!(kkt::AbstractKKTSystem, primal_values::Number, dual_values::Number)\n\nRegularize the values in the diagonal of the KKT system in an incremental fashion. Called internally inside the interior-point routine.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.is_inertia_correct","page":"KKT systems","title":"MadNLP.is_inertia_correct","text":"is_inertia_correct(kkt::AbstractKKTSystem, n::Int, m::Int, p::Int)\n\nCheck if the inertia (n m p) returned by the linear solver is adapted to the KKT system implemented in kkt.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.nnz_jacobian","page":"KKT systems","title":"MadNLP.nnz_jacobian","text":"Nonzero in Jacobian\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.SparseKKTSystem","page":"KKT systems","title":"MadNLP.SparseKKTSystem","text":"SparseKKTSystem{T, VT, MT, QN} <: AbstractReducedKKTSystem{T, VT, MT, QN}\n\nImplement the AbstractReducedKKTSystem in sparse COO format.\n\n\n\n\n\n","category":"type"},{"location":"lib/kkt/#MadNLP.SparseUnreducedKKTSystem","page":"KKT systems","title":"MadNLP.SparseUnreducedKKTSystem","text":"SparseUnreducedKKTSystem{T, VT, MT, QN} <: AbstractUnreducedKKTSystem{T, VT, MT, QN}\n\nImplement the AbstractUnreducedKKTSystem in sparse COO format.\n\n\n\n\n\n","category":"type"},{"location":"lib/kkt/#MadNLP.SparseCondensedKKTSystem","page":"KKT systems","title":"MadNLP.SparseCondensedKKTSystem","text":"SparseCondensedKKTSystem{T, VT, MT, QN} <: AbstractCondensedKKTSystem{T, VT, MT, QN}\n\nImplement the AbstractCondensedKKTSystem in sparse COO format.\n\n\n\n\n\n","category":"type"},{"location":"lib/kkt/#MadNLP.DenseKKTSystem","page":"KKT systems","title":"MadNLP.DenseKKTSystem","text":"DenseKKTSystem{T, VT, MT, QN, VI} <: AbstractReducedKKTSystem{T, VT, MT, QN}\n\nImplement AbstractReducedKKTSystem with dense matrices.\n\nRequires a dense linear solver to be factorized (otherwise an error is returned).\n\n\n\n\n\n","category":"type"},{"location":"lib/kkt/#MadNLP.DenseCondensedKKTSystem","page":"KKT systems","title":"MadNLP.DenseCondensedKKTSystem","text":"DenseCondensedKKTSystem{T, VT, MT, QN} <: AbstractCondensedKKTSystem{T, VT, MT, QN}\n\nImplement AbstractCondensedKKTSystem with dense matrices.\n\nRequires a dense linear solver to factorize the associated KKT system (otherwise an error is returned).\n\n\n\n\n\n","category":"type"},{"location":"lib/kkt/#MadNLP.AbstractKKTVector","page":"KKT systems","title":"MadNLP.AbstractKKTVector","text":"AbstractKKTVector{T, VT}\n\nSupertype for KKT's right-hand-side vectors (x s y z ν w).\n\n\n\n\n\n","category":"type"},{"location":"lib/kkt/#MadNLP.number_primal","page":"KKT systems","title":"MadNLP.number_primal","text":"number_primal(X::AbstractKKTVector)\n\nGet total number of primal values (x s) in KKT vector X.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.number_dual","page":"KKT systems","title":"MadNLP.number_dual","text":"number_dual(X::AbstractKKTVector)\n\nGet total number of dual values (y z) in KKT vector X.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.full","page":"KKT systems","title":"MadNLP.full","text":"full(X::AbstractKKTVector)\n\nReturn the all the values stored inside the KKT vector X.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.primal","page":"KKT systems","title":"MadNLP.primal","text":"primal(X::AbstractKKTVector)\n\nReturn the primal values (x s) stored in the KKT vector X.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.dual","page":"KKT systems","title":"MadNLP.dual","text":"dual(X::AbstractKKTVector)\n\nReturn the dual values (y z) stored in the KKT vector X.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.primal_dual","page":"KKT systems","title":"MadNLP.primal_dual","text":"primal_dual(X::AbstractKKTVector)\n\nReturn both the primal and the dual values (x s y z) stored in the KKT vector X.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.dual_lb","page":"KKT systems","title":"MadNLP.dual_lb","text":"dual_lb(X::AbstractKKTVector)\n\nReturn the dual values ν associated to the lower-bound stored in the KKT vector X.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.dual_ub","page":"KKT systems","title":"MadNLP.dual_ub","text":"dual_ub(X::AbstractKKTVector)\n\nReturn the dual values w associated to the upper-bound stored in the KKT vector X.\n\n\n\n\n\n","category":"function"},{"location":"lib/kkt/#MadNLP.UnreducedKKTVector","page":"KKT systems","title":"MadNLP.UnreducedKKTVector","text":"UnreducedKKTVector{T, VT<:AbstractVector{T}} <: AbstractKKTVector{T, VT}\n\nFull KKT vector (x s y z ν w), associated to a AbstractUnreducedKKTSystem.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/kktsystem/#Implementing-a-custom-KKT-system","page":"Custom KKT system","title":"Implementing a custom KKT system","text":"This tutorial explains how to implement a custom AbstractKKTSystem in MadNLP.","category":"section"},{"location":"tutorials/kktsystem/#Structure-exploiting-methods","page":"Custom KKT system","title":"Structure exploiting methods","text":"MadNLP gives the user the possibility to exploit the problem's structure at the linear algebra level, when solving the KKT system at every Newton iteration. By default, the KKT system is factorized using a sparse linear solver (MUMPS or HSL ma27/ma57). A sparse linear solver analyses the problem's algebraic structure when computing the symbolic factorization, with a heuristic that determines an optimal elimination tree. As an alternative, the problem's structure can be exploited directly, by specifying the order of the pivots to perform (e.g. using a block elimination algorithm). Doing so usually leads to significant speed-up in the algorithm.\n\nWe recall that the KKT system solved at each Newton iteration has the structure:\n\noverline\nbeginpmatrix\n W        J^top  - I     I \n J        0       0       0 \n -Z_ell  0       X_ell  0 \n Z_u      0       0       X_u\nendpmatrix^K\nbeginpmatrix\n    Delta x \n    Delta y \n    Delta z_ell \n    Delta z_u\nendpmatrix\n=\nbeginpmatrix\n    r_1  r_2  r_3  r_4\nendpmatrix\n\nwith W a sparse matrix storing the Hessian of the Lagrangian, and J a sparse matrix storing the Jacobian of the constraints. We note the diagonal matrices Z_ell = diag(z_ell), Z_u = diag(z_u), X_ell = diag(x_ell - x), X_u = diag(x - x_u).\n\nIn MadNLP, every linear system with the structure K is implemented as an AbstractKKTSystem. By default, MadNLP represents a KKT system as a SparseKKTSystem:\n\nnlp = HS15Model()\nresults = madnlp(nlp; kkt_system=MadNLP.SparseKKTSystem, linear_solver=LapackCPUSolver)\nnothing\n","category":"section"},{"location":"tutorials/kktsystem/#Solving-AbstractKKTSystem-in-MadNLP","page":"Custom KKT system","title":"Solving AbstractKKTSystem in MadNLP","text":"The AbstractKKTSystem object is an abstraction to solve the generic system K x = b. Depending on the implementation, the structure of the linear system is exploited in different fashions. Solving a KKT system amounts to the four following operations:\n\nQuerying the current sensitivities to assemble the different blocks constituting the matrix K.\nAssembling a reduced sparse matrix condensing the sparse matrix K to an equivalent smaller symmetric system.\nCalling a linear solver to solve the condensed system.\nCalling a routine to unpack the condensed solution to get the original descent direction (Delta x Delta y Delta z_ell Delta z_u).\n\nExploiting the problem's structure usually happens in steps (2) and (4). We skim through the four successive steps in more details.","category":"section"},{"location":"tutorials/kktsystem/#Getting-the-sensitivities","page":"Custom KKT system","title":"Getting the sensitivities","text":"The KKT system requires the following information:\n\nthe (approximated) Hessian of the Lagrangian W ;\nthe constraints' Jacobian J ;\nthe diagonal matrices Z_ell, Z_u and X_ell, X_u.\n\nThe Hessian and the Jacobian are assumed sparse by default.\n\nAt every IPM iteration, MadNLP updates automatically the values in W, J and in the diagonal matrices Z_ell Z_u X_ell X_u. By default, we expect the following attributes available in every instance kkt of an AbstractKKTSystem:\n\nkkt.hess: stores the nonzeroes of the Hessian W;\nkkt.jac: stores the nonzeroes of the Jacobian J;\nkkt.l_diag: stores the diagonal entries in X_ell;\nkkt.u_diag: stores the diagonal entries in X_u;\nkkt.l_lower: stores the diagonal entries in Z_ell;\nkkt.u_lower: stores the diagonal entries in Z_u.\n\nThe attributes kkt.hess and kkt.jac are accessed respectively using the getters get_hessian and get_jacobian.\n\nEvery time MadNLP queries the Hessian and the Jacobian, it updates the nonzeroes values in kkt.hess and kkt.jac. Rightafter, MadNLP calls respectively the functions compress_hessian! and compress_jacobian! to update all the internal values in the KKT system kkt.\n\nTo recap, every time we evaluate the Hessian and the Jacobian, MadNLP calls automatically the functions:\n\nhess = MadNLP.get_hessian(kkt)\nMadNLP.compress_hessian!(kkt)\n\nto update the values in the Hessian, and for the Jacobian:\n\njac = MadNLP.get_jacobian(kkt)\nMadNLP.compress_jacobian!(kkt)","category":"section"},{"location":"tutorials/kktsystem/#Assembling-the-KKT-system","page":"Custom KKT system","title":"Assembling the KKT system","text":"Once the sensitivities have been updated, we can assemble the KKT matrix K and condense it to an equivalent system K_c before factorizing it with a linear solver. The assembling of the KKT system is done in the function build_kkt!.\n\nThe system is usually stored in the attribute kkt.aug_com. Its dimension depends on the condensation used. The matrix kkt.aug_com can be dense or sparse, depending on the condensation used. MadNLP uses the getter get_kkt to query the matrix kkt.aug_com stored in the KKT system kkt.","category":"section"},{"location":"tutorials/kktsystem/#Solving-the-system","page":"Custom KKT system","title":"Solving the system","text":"Once the matrix K_c is assembled, we pass it to the linear solver for factorization. The linear solver is stored internally in kkt: by default, it is stored in the attribute kkt.linear_solver. The factorization is handled internally in MadNLP.\n\nOnce factorized, it remains to solve the linear system using a backsolve. The backsolve has to be implemented by the user in the function solve!. It reduces the right-hand-side (RHS) down to a form adapted to the condensed matrix K_c and calls the linear solver to perform the backsolve. Then the condensed solution is unpacked to recover the full solution (Delta x Delta y Delta z_ell Delta z_u).\n\nTo recap, MadNLP assembles and solves the KKT linear system using the following operations:\n\n# Assemble\nMadNLP.build_kkt!(kkt)\n# Factorize  the KKT system\nMadNLP.factorize!(kkt.linear_solver)\n# Backsolve\nMadNLP.solve!(kkt, w)\n","category":"section"},{"location":"tutorials/kktsystem/#Example:-Implementing-a-new-KKT-system","page":"Custom KKT system","title":"Example: Implementing a new KKT system","text":"As an example, we detail how to implement a custom KKT system in MadNLP. Note that we consider this usage as an advanced use of MadNLP. After this work of caution, let's dive into the details!\n\nIn this example, we want to approximate the Hessian of the Lagrangian W as a diagonal matrix D_w and solve the following KKT system at each IPM iteration:\n\nbeginpmatrix\n D_w  J^top  - I  I \n J  0   0  0 \n -Z_ell   0  X_ell  0 \n Z_u  W   0  X_u\nendpmatrix\nbeginpmatrix\n    Delta x \n    Delta y \n    Delta z_ell \n    Delta z_u\nendpmatrix\n=\nbeginpmatrix\n    r_1  r_2  r_3  r_4\nendpmatrix\n\nThis new system is not equivalent to the original system K, but it's much easier to solve at it does not involve the generic Hessian W. If the diagonal values of D_w are constant and are equal to alpha, the algorithm becomes equivalent to a gradient descent with step alpha^-1.\n\nUsing the relations Delta z_ell = X_ell^-1 (r_3 + Z_ell Delta x) and Delta z_u = X_u^-1 (r_3 - Z_u Delta x), we condense the matrix down to the reduced form:\n\nbeginpmatrix\n D_w + Sigma_x  J^top \n J  0  \nendpmatrix\nbeginpmatrix\n    Delta x  \n    Delta y\nendpmatrix\n=\nbeginpmatrix\n    r_1 + X_ell^-1 r_3 - X_u^-1 r_4 r_2\nendpmatrix\n\nwith the diagonal matrix Sigma_x = -X_ell^-1 Z_ell - X_u^-1 Z_u. The new system is symmetric indefinite, but much easier to solve than the original one.\n\nThe previous reduction is standard in NLP solvers: MadNLP implements the reduced KKT system operating in the space (Delta x Delta y) using the abstraction AbstractReducedKKTSystem. If D_w is replaced by the original Hessian matrix W, we recover exactly the SparseKKTSystem used by default in MadNLP.","category":"section"},{"location":"tutorials/kktsystem/#Creating-the-KKT-system","page":"Custom KKT system","title":"Creating the KKT system","text":"We create a new KKT system DiagonalHessianKKTSystem, inheriting from AbstractReducedKKTSystem. Using generic types, the structure DiagonalHessianKKTSystem is defined as:\n\nstruct DiagonalHessianKKTSystem{T, VT, MT, QN, LS, VI, VI32} <: MadNLP.AbstractReducedKKTSystem{T, VT, MT, QN}\n    # Nonzeroes values for Hessian and Jacobian\n    hess::VT\n    jac_callback::VT\n    jac::VT\n    # Diagonal matrices\n    reg::VT\n    pr_diag::VT\n    du_diag::VT\n    l_diag::VT\n    u_diag::VT\n    l_lower::VT\n    u_lower::VT\n    # Augmented system K\n    aug_raw::MadNLP.SparseMatrixCOO{T,Int32,VT, VI32}\n    aug_com::MT\n    aug_csc_map::Union{Nothing, VI}\n    # Diagonal of the Hessian\n    diag_hess::VT\n    # Jacobian\n    jac_raw::MadNLP.SparseMatrixCOO{T,Int32,VT, VI32}\n    jac_com::MT\n    jac_csc_map::Union{Nothing, VI}\n    # LinearSolver\n    linear_solver::LS\n    # Info\n    n_var::Int\n    n_ineq::Int\n    n_tot::Int\n    ind_ineq::VI\n    ind_lb::VI\n    ind_ub::VI\n    # Quasi-Newton approximation\n    quasi_newton::QN\nend\n\n\ninfo: Info\nHere, we define a DiagonalHessianKKTSystem as a subtype of a AbstractReducedKKTSystem. Depending on the condensation, the following alternatives are available:AbstractUnreducedKKTSystem: no condensation is applied.\nAbstractCondensedKKTSystem: the reduced KKT system is condensed further by removing the blocks associated to the slack variables.\n\ninfo: Info\nThe attributes pr_diag and du_diag store respectively the primal regularization (terms in the diagonal of the (1, 1) block) and the dual regularization (terms in the diagonal of the (2, 2) block). By default, the dual regularization is keep equal to 0, whereas the primal regularization is set equal to Sigma_x.\n\nMadNLP instantiates a new KKT system with the function create_kkt_system, with the following signature:\n\nfunction MadNLP.create_kkt_system(\n    ::Type{DiagonalHessianKKTSystem},\n    cb::MadNLP.SparseCallback{T,VT},\n    linear_solver::Type;\n    opt_linear_solver=MadNLP.default_options(linear_solver),\n    hessian_approximation=MadNLP.ExactHessian,\n    qn_options=MadNLP.QuasiNewtonOptions(),\n) where {T,VT}\n\nWe pass as arguments:\n\nthe type of the KKT system to build (here, DiagonalHessianKKTSystem),\nthe structure used to evaluate the callbacks cb,\nthe index of the constraints,\na generic linear solver linear_solver.\n\nThis function instantiates all the data structures needed in DiagonalHessianKKTSystem. The most difficult part is to assemble the sparse matrices aug_raw and jac_raw, here stored in COO format. This is done in four steps:\n\nStep 1. We import the sparsity pattern of the Jacobian :\n\n    jac_sparsity_I = MadNLP.create_array(cb, Int32, cb.nnzj)\n    jac_sparsity_J = MadNLP.create_array(cb, Int32, cb.nnzj)\n    MadNLP._jac_sparsity_wrapper!(cb, jac_sparsity_I, jac_sparsity_J)\n\nStep 2. We build the resulting KKT matrix aug_raw in COO format, knowing that D_w is diagonal:\n\n    # System's dimension\n    n_hess = n_tot # Diagonal Hessian!\n    n_jac = length(jac_sparsity_I)\n    aug_vec_length = n_tot+m\n    aug_mat_length = n_tot+m+n_hess+n_jac+n_slack\n\n    # Build vectors to store COO coortinates\n    I = MadNLP.create_array(cb, Int32, aug_mat_length)\n    J = MadNLP.create_array(cb, Int32, aug_mat_length)\n    V = VT(undef, aug_mat_length)\n    fill!(V, 0.0)  # Need to initiate V to avoid NaN\n\n    offset = n_tot+n_jac+n_slack+n_hess+m\n\n    # Primal regularization block\n    I[1:n_tot] .= 1:n_tot\n    J[1:n_tot] .= 1:n_tot\n    # Hessian block\n    I[n_tot+1:n_tot+n_hess] .= 1:n_tot # diagonal Hessian!\n    J[n_tot+1:n_tot+n_hess] .= 1:n_tot # diagonal Hessian!\n    # Jacobian block\n    I[n_tot+n_hess+1:n_tot+n_hess+n_jac] .= (jac_sparsity_I.+n_tot)\n    J[n_tot+n_hess+1:n_tot+n_hess+n_jac] .= jac_sparsity_J\n    # Slack block\n    I[n_tot+n_hess+n_jac+1:n_tot+n_hess+n_jac+n_slack] .= ind_ineq .+ n_tot\n    J[n_tot+n_hess+n_jac+1:n_tot+n_hess+n_jac+n_slack] .= (n+1:n+n_slack)\n    # Dual regularization block\n    I[n_tot+n_hess+n_jac+n_slack+1:offset] .= (n_tot+1:n_tot+m)\n    J[n_tot+n_hess+n_jac+n_slack+1:offset] .= (n_tot+1:n_tot+m)\n\n    aug_raw = MadNLP.SparseMatrixCOO(aug_vec_length, aug_vec_length, I, J, V)\n\nStep 3. We convert aug_raw from COO to CSC using the following utilities:\n\n    aug_com, aug_csc_map = MadNLP.coo_to_csc(aug_raw)\n\nStep 4. We pass the matrix in CSC format to the linear solver:\n\n    _linear_solver = linear_solver(\n        aug_com; opt = opt_linear_solver\n    )\n\n\ninfo: Info\nStoring the Hessian and Jacobian, even in sparse format, is expensive in term of memory. For that reason, MadNLP stores the Hessian and Jacobian only once in the KKT system.","category":"section"},{"location":"tutorials/kktsystem/#Getting-the-sensitivities-2","page":"Custom KKT system","title":"Getting the sensitivities","text":"MadNLP requires the following getters to update the sensitivities. As much as we can, we try to update the values inplace in the matrix aug_raw. We use the default implementation of compress_jacobian! in MadNLP:\n\nfunction MadNLP.compress_jacobian!(kkt::DiagonalHessianKKTSystem)\n    ns = length(kkt.ind_ineq)\n    kkt.jac[end-ns+1:end] .= -1.0\n    MadNLP.transfer!(kkt.jac_com, kkt.jac_raw, kkt.jac_csc_map)\n    return\nend\n\nThe term -1.0 accounts for the slack variables used to reformulate the inequality constraints as equality constraints.\n\nFor compress_hessian!, we take into account that the diagonal matrix D_w is the diagonal of the Hessian:\n\nfunction MadNLP.compress_hessian!(kkt::DiagonalHessianKKTSystem)\n    kkt.diag_hess .= 1.0\n    return\nend\n\nMadNLP also needs the following basic functions to get the different matrices and the dimension of the linear system:\n\nMadNLP.num_variables(kkt::DiagonalHessianKKTSystem) = length(kkt.diag_hess)\nMadNLP.get_kkt(kkt::DiagonalHessianKKTSystem) = kkt.aug_com\nMadNLP.get_jacobian(kkt::DiagonalHessianKKTSystem) = kkt.jac_callback\nfunction MadNLP.jtprod!(y::AbstractVector, kkt::DiagonalHessianKKTSystem, x::AbstractVector)\n    mul!(y, kkt.jac_com', x)\nend","category":"section"},{"location":"tutorials/kktsystem/#Assembling-the-KKT-system-2","page":"Custom KKT system","title":"Assembling the KKT system","text":"Once the sensitivities are updated, we assemble the new matrix K_c first in COO format in kkt.aug_raw, before converting the matrix to CSC format in kkt.jac_com using the utility MadNLP.transfer!:\n\nfunction MadNLP.build_kkt!(kkt::DiagonalHessianKKTSystem)\n    MadNLP.transfer!(kkt.aug_com, kkt.aug_raw, kkt.aug_csc_map)\nend","category":"section"},{"location":"tutorials/kktsystem/#Solving-the-system-2","page":"Custom KKT system","title":"Solving the system","text":"It remains to implement the backsolve. For the reduced KKT formulation, the RHS r_1 + X_ell^-1 r_3 - X_u^-1 r_4 is built automatically using the function MadNLP.reduce_rhs!. The backsolve solves for (Delta x Delta y). The dual's descent direction Delta z_ell and Delta z_u are recovered afterwards using the function MadNLP.finish_aug_solve!:\n\nfunction MadNLP.solve!(kkt::DiagonalHessianKKTSystem, w::MadNLP.AbstractKKTVector)\n    MadNLP.reduce_rhs!(w.xp_lr, dual_lb(w), kkt.l_diag, w.xp_ur, dual_ub(w), kkt.u_diag)\n    MadNLP.solve!(kkt.linear_solver, primal_dual(w))\n    MadNLP.finish_aug_solve!(kkt, w)\n    return w\nend\n\nnote: Note\nThe function solve! takes as second argument a vector w being an AbstractKKTVector. An AbstractKKTVector is a convenient data structure used in MadNLP to store and access the elements in the primal-dual vector (Delta x Delta y Delta z_ell Delta z_u).\n\nwarning: Warning\nWhen calling solve!, the values in the vector w are updated inplace. The vector w should be initialized with the RHS (r_1 r_2 r_3 r_4) before calling the function solve!. The function modifies the values directly in the vector w to return the solution (Delta x Delta y Delta z_ell Delta z_u).\n\nLast, MadNLP implements an iterative refinement method to get accurate descent directions in the final iterations. The iterative refinement algorithm implements Richardson's method, which requires multiplying the KKT matrix K on the right by any vector w = (w_x w_y w_z_l w_z_u). This is provided in MadNLP by overloading the function LinearAlgebra.mul!:\n\nfunction LinearAlgebra.mul!(\n    w::MadNLP.AbstractKKTVector{T},\n    kkt::DiagonalHessianKKTSystem,\n    x::MadNLP.AbstractKKTVector{T},\n    alpha = one(T),\n    beta = zero(T),\n) where {T}\n\n    mul!(primal(w), Diagonal(kkt.diag_hess), primal(x), alpha, beta)\n    mul!(primal(w), kkt.jac_com', dual(x), alpha, one(T))\n    mul!(dual(w), kkt.jac_com,  primal(x), alpha, beta)\n\n    # Reduce KKT vector\n    MadNLP._kktmul!(w,x,kkt.reg,kkt.du_diag,kkt.l_lower,kkt.u_lower,kkt.l_diag,kkt.u_diag, alpha, beta)\n    return w\nend","category":"section"},{"location":"tutorials/kktsystem/#Demonstration","page":"Custom KKT system","title":"Demonstration","text":"We now have all the elements needed to solve the problem with the new KKT linear system DiagonalHessianKKTSystem. We just have to pass the KKT system to MadNLP using the option kkt_system:\n\nnlp = HS15Model()\nresults = madnlp(nlp; kkt_system=DiagonalHessianKKTSystem, linear_solver=LapackCPUSolver)\nnothing\n","category":"section"},{"location":"tutorials/multiprecision/#Running-MadNLP-in-arbitrary-precision","page":"Multi-precision","title":"Running MadNLP in arbitrary precision","text":"MadNLP is written in pure Julia, and as such support solving optimization problems in arbitrary precision. By default, MadNLP adapts its precision according to the NLPModel passed in input. Most models use Float64 (in fact, almost all optimization modelers are implemented using double precision), but for certain applications it can be useful to use arbitrary precision to get more accurate solution.\n\ninfo: Info\nThere exists different packages to instantiate a optimization model in arbitrary precision in Julia. Most of them leverage the flexibility offered by NLPModels.jl. In particular, we recommend:CUTEst.jl: supports Float32, Float64 and Float128.\nExaModels: supports AbstractFloat.","category":"section"},{"location":"tutorials/multiprecision/#Defining-a-problem-in-arbitrary-precision","page":"Multi-precision","title":"Defining a problem in arbitrary precision","text":"As a demonstration, we implement the model airport from CUTEst using ExaModels. The code writes:\n\nusing ExaModels\n\nfunction airport_model(T)\n    N = 42\n    # Data\n    r = T[0.09 , 0.3, 0.09, 0.45, 0.5, 0.04, 0.1, 0.02, 0.02, 0.07, 0.4, 0.045, 0.05, 0.056, 0.36, 0.08, 0.07, 0.36, 0.67, 0.38, 0.37, 0.05, 0.4, 0.66, 0.05, 0.07, 0.08, 0.3, 0.31, 0.49, 0.09, 0.46, 0.12, 0.07, 0.07, 0.09, 0.05, 0.13, 0.16, 0.46, 0.25, 0.1]\n    cx = T[-6.3, -7.8, -9.0, -7.2, -5.7, -1.9, -3.5, -0.5, 1.4, 4.0, 2.1, 5.5, 5.7, 5.7, 3.8, 5.3, 4.7, 3.3, 0.0, -1.0, -0.4, 4.2, 3.2, 1.7, 3.3, 2.0, 0.7, 0.1, -0.1, -3.5, -4.0, -2.7, -0.5, -2.9, -1.2, -0.4, -0.1, -1.0, -1.7, -2.1, -1.8, 0.0]\n    cy = T[8.0, 5.1, 2.0, 2.6, 5.5, 7.1, 5.9, 6.6, 6.1, 5.6, 4.9, 4.7, 4.3, 3.6, 4.1, 3.0, 2.4, 3.0, 4.7, 3.4, 2.3, 1.5, 0.5, -1.7, -2.0, -3.1, -3.5, -2.4, -1.3, 0.0, -1.7, -2.1, -0.4, -2.9, -3.4, -4.3, -5.2, -6.5, -7.5, -6.4, -5.1, 0.0]\n    # Wrap all data in a single iterator for ExaModels\n    data = [(i, cx[i], cy[i], r[i]) for i in 1:N]\n    IJ = [(i, j) for i in 1:N-1 for j in i+1:N]\n    # Write model using ExaModels\n    core = ExaModels.ExaCore(T)\n    x = ExaModels.variable(core, 1:N, lvar = -10.0, uvar=10.0)\n    y = ExaModels.variable(core, 1:N, lvar = -10.0, uvar=10.0)\n    ExaModels.objective(\n        core,\n        ((x[i] - x[j])^2 + (y[i] - y[j])^2) for (i, j) in IJ\n    )\n    ExaModels.constraint(core, (x[i]-dcx)^2 + (y[i] - dcy)^2 - dr for (i, dcx, dcy, dr) in data; lcon=-Inf)\n    return ExaModels.ExaModel(core)\nend\n\nThe function airport_model takes as input the type used to define the model in ExaModels. For example, ExaCore(Float64) instantiates a model with Float64, whereas ExaCore(Float32) instantiates a model using Float32. Thus, instantiating the instance airport using Float32 simply amounts to\n\nnlp = airport_model(Float32)\n\n\nWe verify that the model is correctly instantiated using Float32:\n\nx0 = NLPModels.get_x0(nlp)\nprintln(typeof(x0))","category":"section"},{"location":"tutorials/multiprecision/#Solving-a-problem-in-Float32","page":"Multi-precision","title":"Solving a problem in Float32","text":"Now that we have defined our model in Float32, we solve it using MadNLP. As nlp is using Float32, MadNLP will automatically adjust its internal types to Float32 during the instantiation. By default, the convergence tolerance is also adjusted to the input type, such that tol = sqrt(eps(T)). Hence, in our case the tolerance is set automatically to\n\ntol = sqrt(eps(Float32))\n\nWe solve the problem using Lapack as linear solver:\n\nresults = madnlp(nlp; linear_solver=LapackCPUSolver)\n\nnote: Note\nNote that the distribution of Lapack shipped with Julia supports Float32, so here we do not have to worry whether the type is supported by the linear solver. Almost all linear solvers shipped with MadNLP supports Float32.\n\nThe final solution is\n\nresults.solution\n\n\nand the objective is\n\nresults.objective\n\n\nFor completeness, we compare with the solution returned when we solve the same problem using Float64:\n\nnlp_64 = airport_model(Float64)\nresults_64 = madnlp(nlp_64; linear_solver=LapackCPUSolver)\n\nThe final objective is now\n\nresults_64.objective\n\n\nAs expected when solving an optimization problem with Float32, the relative difference between the two solutions is far from being negligible:\n\nrel_diff = abs(results.objective - results_64.objective) / results_64.objective","category":"section"},{"location":"tutorials/multiprecision/#Solving-a-problem-in-Float128","page":"Multi-precision","title":"Solving a problem in Float128","text":"Now, we go in the opposite direction and solve a problem using Float128 to get a better accuracy. We start by loading the library Quadmath to work with quadruple precision:\n\nusing Quadmath\n\nWe can instantiate our problem using Float128 directly as:\n\nnlp_128 = airport_model(Float128)\n\nwarning: Warning\nUnfortunately, a few linear solvers support Float128 out of the box. Currently, the only solver suporting quadruple in MadNLP is LDLSolver, which implements an LDL factorization in pure Julia. The solver LDLSolver is not adapted to solve large-scale nonconvex nonlinear programs, but works if the problem is small enough (as it is the case here).\n\nReplacing the solver by LDLSolver, solving the problem with MadNLP just amounts to\n\nresults_128 = madnlp(nlp_128; linear_solver=LDLSolver)\n\n\nNote that the final tolerance is much lower than before. We get the solution in quadruple precision\n\nresults_128.solution\n\nas well as the final objective:\n\nresults_128.objective","category":"section"},{"location":"tutorials/lbfgs/#Limited-memory-BFGS","page":"LBFGS","title":"Limited-memory BFGS","text":"Sometimes, the second-order derivatives are just too expensive to evaluate. In that case, it is often a good idea to approximate the Hessian matrix. The BFGS algorithm uses the first-order derivatives (gradient and tranposed-Jacobian vector product) to approximate the Hessian of the Lagrangian. LBFGS is a variant of BFGS that computes a low-rank approximation of the Hessian matrix from the past iterates. That way, LBFGS does not have to store a large dense matrix in memory, rendering the algorithm appropriate in the large-scale regime.\n\nMadNLP implements several quasi-Newton approximation for the Hessian matrix. In this page, we focus on the limited-memory version of the BFGS algorithm, commonly known as LBFGS. We refer to this article for a detailed description of the method.","category":"section"},{"location":"tutorials/lbfgs/#How-to-set-up-LBFGS-in-MadNLP?","page":"LBFGS","title":"How to set up LBFGS in MadNLP?","text":"We look at the elec optimization problem from the COPS benchmark:\n\nfunction elec_model(np)\n    Random.seed!(1)\n    # Set the starting point to a quasi-uniform distribution of electrons on a unit sphere\n    theta = (2pi) .* rand(np)\n    phi = pi .* rand(np)\n\n    core = ExaModels.ExaCore(Float64)\n    x = ExaModels.variable(core, 1:np; start = [cos(theta[i])*sin(phi[i]) for i=1:np])\n    y = ExaModels.variable(core, 1:np; start = [sin(theta[i])*sin(phi[i]) for i=1:np])\n    z = ExaModels.variable(core, 1:np; start = [cos(phi[i]) for i=1:np])\n    # Coulomb potential\n    itr = [(i,j) for i in 1:np-1 for j in i+1:np]\n    ExaModels.objective(core, 1.0 / sqrt((x[i] - x[j])^2 + (y[i] - y[j])^2 + (z[i] - z[j])^2) for (i,j) in itr)\n    # Unit-ball\n    ExaModels.constraint(core, x[i]^2 + y[i]^2 + z[i]^2 - 1 for i=1:np)\n\n    return ExaModels.ExaModel(core)\nend\n\n\nThe problem computes the positions of the electrons in an atom. The potential of each electron depends on the positions of all the other electrons: all the variables in the problem are coupled, resulting in a dense Hessian matrix. Hence, the problem is good candidate for a quasi-Newton algorithm.\n\nWe start by solving the problem with the default options in MadNLP, using a dense linear solver from LAPACK:\n\nnh = 10\nnlp = elec_model(nh)\nresults_hess = madnlp(\n    nlp;\n    linear_solver=LapackCPUSolver,\n)\nnothing\n\n\nWe observe that MadNLP converges in 21 iterations.\n\nTo replace the second-order derivatives by an LBFGS approximation, you should pass the option hessian_approximation=CompactLBFGS to MadNLP.\n\nresults_qn = madnlp(\n    nlp;\n    linear_solver=LapackCPUSolver,\n    hessian_approximation=MadNLP.CompactLBFGS,\n)\nnothing\n\n\nWe observe that MadNLP converges in 93 iterations. As expected, the number of Hessian evaluations is 0.","category":"section"},{"location":"tutorials/lbfgs/#How-to-tune-the-options-in-LBFGS?","page":"LBFGS","title":"How to tune the options in LBFGS?","text":"You can tune the LBFGS options by using the option quasi_newton_options. The option takes as input a QuasiNewtonOptions structure, with the following attributes (we put on the right their default values):\n\ninit_strategy::BFGSInitStrategy = SCALAR1\nmax_history::Int = 6\ninit_value::Float64 = 1.0\nsigma_min::Float64 = 1e-8\nsigma_max::Float64 = 1e+8\n\nThe most important parameter is max_history, which encodes the number of vectors used in the low-rank approximation. For instance, we can increase the history to use the 20 past iterates using:\n\nqn_options = MadNLP.QuasiNewtonOptions(; max_history=20)\nresults_qn = madnlp(\n    nlp;\n    linear_solver=LapackCPUSolver,\n    hessian_approximation=MadNLP.CompactLBFGS,\n    quasi_newton_options=qn_options,\n)\nnothing\n\n\nWe observe that the total number of iterations has been reduced from 93 to 60.","category":"section"},{"location":"tutorials/lbfgs/#How-does-LBFGS-is-implemented-in-MadNLP?","page":"LBFGS","title":"How does LBFGS is implemented in MadNLP?","text":"MadNLP implements the compact LBFGS algorithm described in this article. At each iteration, the Hessian W_k is approximated by a low rank positive definite matrix B_k, defined as\n\nB_k = xi_k I + U_k V_k^top\n\n\nwith xi  0 a scaling factor, U_k and V_k two n times 2p matrices. The number p denotes the number of vectors used when computing the limited memory updates (the parameter max_history in MadNLP): the larger, the more accurate is the low-rank approximation.\n\nReplacing the Hessian of the Lagrangian W_k by the low-rank matrix B_k, the KKT system solved in MadNLP rewrites as\n\nbeginbmatrix\nxi I + U V^top + Sigma_x  0  A^top \n0  Sigma_s  -I \nA  -I  0\nendbmatrix\nbeginbmatrix\nDelta x  Delta s  Delta y\nendbmatrix\n=\nbeginbmatrix\nr_1  r_2  r_3\nendbmatrix\n\n\nThe KKT system has a low-rank structure we can exploit using the Sherman-Morrison formula. The method is detailed e.g. in Section 3.8 of that paper.\n\ninfo: Info\nAs MadNLP is designed to solve generic constrained optimization problems, it does not approximate the inverse of the Hessian matrix, as done in other implementations of the LBFGS algorithm specialized on solving nonlinear problems with bound constraints. If your problem has no generic nonlinear constraints, we recommend for optimal performance using LBFGSB or the LBFGS implemented in JSOSolvers.jl.","category":"section"},{"location":"lib/ipm/#MadNLP-solver","page":"IPM solver","title":"MadNLP solver","text":"MadNLP takes as input a nonlinear program encoded as a AbstractNLPModel and solve it using interior-point. The main entry point is the function madnlp:\n\nIn detail, the function madnlp builds a MadNLPSolver storing all the required structures in the solution algorithm. Once the MadNLPSolver instantiated, the function solve! is applied to solve the nonlinear program with MadNLP's interior-point algorithm.","category":"section"},{"location":"lib/ipm/#MadNLP.madnlp","page":"IPM solver","title":"MadNLP.madnlp","text":"madnlp(model::AbstractNLPModel; options...)\n\nBuild a MadNLPSolver and solve it using the interior-point method. Return the solution as a MadNLPExecutionStats.\n\n\n\n\n\n","category":"function"},{"location":"lib/ipm/#MadNLP.MadNLPExecutionStats","page":"IPM solver","title":"MadNLP.MadNLPExecutionStats","text":"MadNLPExecutionStats{T, VT} <: AbstractExecutionStats\n\nStore the results returned by MadNLP once the interior-point algorithm has terminated.\n\n\n\n\n\n","category":"type"},{"location":"lib/ipm/#MadNLP.MadNLPSolver","page":"IPM solver","title":"MadNLP.MadNLPSolver","text":"MadNLPSolver(nlp::AbstractNLPModel{T, VT}; options...) where {T, VT}\n\nInstantiate a new MadNLPSolver associated to the nonlinear program nlp::AbstractNLPModel. The options are passed as optional arguments.\n\nThe constructor allocates all the memory required in the interior-point algorithm, so the main algorithm remains allocation free.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/warmstart/#Warmstarting-MadNLP","page":"Warm-start","title":"Warmstarting MadNLP","text":"We use a parameterized version of the instance HS15 used in the introduction. This updated version of HS15Model stores the parameters of the model in an attribute nlp.params:\n\n\nnlp = HS15Model()\nprintln(nlp.params)\n\nBy default the parameters are set to [100.0, 1.0]. In a first solve, we find a solution associated to these parameters. We want to warmstart MadNLP from the solution found in the first solve, after a small update in the problem's parameters.\n\ninfo: Info\nIt is known that the interior-point method has a poor support of warmstarting, on the contrary to active-set methods. However, if the parameter changes remain reasonable and do not lead to significant changes in the active set, warmstarting the interior-point algorithm can significantly reduces the total number of barrier iterations in the second solve.\n\nwarning: Warning\nThe warm-start described in this tutorial remains basic. Its main application is updating the solution of a parametric problem after an update in the parameters. The warm-start always assumes that the structure of the problem remains the same between two consecutive solves. MadNLP cannot be warm-started if variables or constraints are added to the problem.","category":"section"},{"location":"tutorials/warmstart/#Naive-solution:-starting-from-the-previous-solution","page":"Warm-start","title":"Naive solution: starting from the previous solution","text":"By default, MadNLP starts its interior-point algorithm at the primal variable stored in nlp.meta.x0. We can access this attribute using the function get_x0:\n\nx0 = NLPModels.get_x0(nlp)\n\n\nHere, we observe that the initial solution is [0, 0].\n\nWe solve the problem using the function madnlp:\n\nresults = madnlp(nlp)\nnothing\n\nMadNLP converges in 19 barrier iterations.  The solution is:\n\nprintln(\"Objective: \", results.objective)\nprintln(\"Solution:  \", results.solution)","category":"section"},{"location":"tutorials/warmstart/#Solution-1:-updating-the-starting-solution","page":"Warm-start","title":"Solution 1: updating the starting solution","text":"We have found a solution to the problem. Now, what happens if we update the parameters inside nlp?\n\nnlp.params .= [101.0, 1.1]\n\nAs MadNLP starts the algorithm at nlp.meta.x0, we pass the previous solution to the initial vector:\n\ncopyto!(NLPModels.get_x0(nlp), results.solution)\n\nSolving the problem again with MadNLP, we observe that MadNLP converges in only 6 iterations:\n\nresults_new = madnlp(nlp)\nnothing\n\n\nBy decreasing the initial barrier parameter, we can reduce the total number of iterations to 5:\n\nresults_new = madnlp(nlp; mu_init=1e-7)\nnothing\n\n\nThe final solution is slightly different from the previous one, as we have updated the parameters inside the model nlp:\n\nresults_new.solution\n\n\ninfo: Info\nSimilarly as with the primal solution, we can pass the initial dual solution to MadNLP using the function get_y0. We can overwrite the value of y0 in nlp using:copyto!(NLPModels.get_y0(nlp), results.multipliers)In our particular example, setting the dual multipliers has only a minor influence on the convergence of the algorithm.","category":"section"},{"location":"tutorials/warmstart/#Advanced-solution:-keeping-the-solver-in-memory","page":"Warm-start","title":"Advanced solution: keeping the solver in memory","text":"The previous solution works, but is wasteful in resource: each time we call the function madnlp we create a new instance of MadNLPSolver, leading to a significant number of memory allocations. A workaround is to keep the solver in memory to have more fine-grained control on the warm-start.\n\nWe start by creating a new model nlp and we instantiate a new instance of MadNLPSolver attached to this model:\n\nnlp = HS15Model()\nsolver = MadNLP.MadNLPSolver(nlp)\n\nNote that\n\nnlp === solver.nlp\n\nHence, updating the parameter values in nlp will automatically update the parameters in the solver.\n\nWe solve the problem using the function solve!:\n\nresults = MadNLP.solve!(solver)\n\nBefore warmstarting MadNLP, we proceed as before and update the parameters and the primal solution in nlp:\n\nnlp.params .= [101.0, 1.1]\ncopyto!(NLPModels.get_x0(nlp), results.solution)\n\nMadNLP stores in memory the dual solutions computed during the first solve. One can access to the (scaled) multipliers as\n\nsolver.y\n\nand to the multipliers of the bound constraints with\n\n[solver.zl.values solver.zu.values]\n\nwarning: Warning\nIf we call the function solve! a second-time, MadNLP will use the following rule:The initial primal solution is copied from NLPModels.get_x0(nlp)\nThe initial dual solution is directly taken from the values specified in solver.y, solver.zl and solver.zu. (MadNLP is not using the values stored in nlp.meta.y0 in the second solve).\n\nAs before, it is advised to decrease the initial barrier parameter: if the initial point is close enough to the solution, this reduces drastically the total number of iterations. We solve the problem again using:\n\nMadNLP.solve!(solver; mu_init=1e-7)\nnothing\n\nThree observations are in order:\n\nThe iteration count starts directly from the previous count (as stored in solver.cnt.k).\nMadNLP converges in only 4 iterations.\nThe factorization stored in solver is directly re-used, leading to significant savings. As a consequence, the warm-start does not work if the structure of the problem changes between the first and the second solve (e.g, if variables or constraints are added to the constraints).","category":"section"},{"location":"lib/barrier/#Barrier-strategies","page":"Barrier strategies","title":"Barrier strategies","text":"The structure AbstractBarrierUpdate encodes the barrier strategy currently used in MadNLP. The strategy is defined by the option barrier when initializing MadNLP.","category":"section"},{"location":"lib/barrier/#Monotone-strategy","page":"Barrier strategies","title":"Monotone strategy","text":"By default, MadNLP uses a monotone strategy, following the Fiacco-McCormick approach.","category":"section"},{"location":"lib/barrier/#Adaptive-strategies","page":"Barrier strategies","title":"Adaptive strategies","text":"As an alternative to the monotone strategy, MadNLP implements several adaptive rules, all described in this article.","category":"section"},{"location":"lib/barrier/#MadNLP.AbstractBarrierUpdate","page":"Barrier strategies","title":"MadNLP.AbstractBarrierUpdate","text":"AbstractBarrierUpdate{T}\n\nAbstraction used to implement the different rules to update the barrier parameter. The barrier is updated using either a monotone rule or an adaptive rule.\n\n\n\n\n\n","category":"type"},{"location":"lib/barrier/#MadNLP.update_barrier!","page":"Barrier strategies","title":"MadNLP.update_barrier!","text":"update_barrier!(barrier::AbstractBarrierUpdate{T}, solver::AbstractMadNLPSolver{T}, sc::T) where T\n\nUpdate barrier using the rule in barrier. Store the results in solver.mu.\n\n\n\n\n\n","category":"function"},{"location":"lib/barrier/#MadNLP.MonotoneUpdate","page":"Barrier strategies","title":"MadNLP.MonotoneUpdate","text":"MonotoneUpdate{T} <: AbstractBarrierUpdate{T}\n\nUpdate the barrier parameter using the classical Fiacco-McCormick monotone rule.\n\n\n\n\n\n","category":"type"},{"location":"lib/barrier/#MadNLP.AbstractAdaptiveUpdate","page":"Barrier strategies","title":"MadNLP.AbstractAdaptiveUpdate","text":"AbstractAdaptiveUpdate{T} <: AbstractBarrierUpdate{T}\n\nAbstraction used to implement the adaptive barrier updates described in [Nocedal2009].\n\nReferences\n\n[Nocedal2009] Nocedal, J., Wächter, A., & Waltz, R. A. (2009). Adaptive barrier update strategies for nonlinear interior methods. SIAM Journal on Optimization, 19(4), 1674-1693.\n\n\n\n\n\n","category":"type"},{"location":"lib/barrier/#MadNLP.QualityFunctionUpdate","page":"Barrier strategies","title":"MadNLP.QualityFunctionUpdate","text":"QualityFunctionUpdate{T} <: AbstractAdaptiveUpdate{T}\n\nFind the barrier parameter using a quality function encoding the ℓ1-norm of the KKT violations. At each IPM iteration, the minimum of the quality function is found using a Golden search algorithm.\n\nIf no sufficient progress is made, the barrier fallbacks to a monotone rule.\n\nReferences\n\nThe algorithm is described in [Nocedal2009, Section 4].\n\n\n\n\n\n","category":"type"},{"location":"lib/barrier/#MadNLP.LOQOUpdate","page":"Barrier strategies","title":"MadNLP.LOQOUpdate","text":"LOQOUpdate{T} <: AbstractAdaptiveUpdate{T}\n\nFind the barrier parameter using the rule used in the LOQO solver. The rule is explicited in [Nocedal2009, Eq (3.6)].\n\nIf no sufficient progress is made, the barrier fallbacks to a monotone rule.\n\nReferences\n\nThe algorithm is described in [Nocedal2009, Section 3].\n\n\n\n\n\n","category":"type"},{"location":"quickstart/#Quickstart","page":"Quickstart","title":"Quickstart","text":"This page presents a quickstart guide to solve a nonlinear problem with MadNLP.\n\nAs a demonstration, we show how to implement the HS15 nonlinear problem from the Hock & Schittkowski collection, first by using a nonlinear modeler and then by specifying the derivatives manually.\n\nThe HS15 problem is defined as:\n\nbeginaligned\nmin_x_1 x_2     100 times (x_2 - x_1^2)^2 +(1 - x_1)^2 \ntextsubject to quad   x_1  times x_2 geq 1 \n         x_1 + x_2^2 geq 0 \n         x_1 leq 05\nendaligned\n\n\nDespite its small dimension, its resolution remains challenging as the problem is nonlinear nonconvex. Note that HS15 encompasses one bound constraint (x_1 leq 05) and two generic constraints.","category":"section"},{"location":"quickstart/#Using-a-nonlinear-modeler:-JuMP.jl","page":"Quickstart","title":"Using a nonlinear modeler: JuMP.jl","text":"The easiest way to implement a nonlinear problem is to use a nonlinear modeler as JuMP. In JuMP, the user just has to pass the structure of the problem, the computation of the first- and second-order derivatives being handled automatically.\n\nUsing JuMP's syntax, the HS15 problem translates to\n\nusing JuMP\nmodel = Model()\n@variable(model, x1 <= 0.5)\n@variable(model, x2)\n\n@objective(model, Min, 100.0 * (x2 - x1^2)^2 + (1.0 - x1)^2)\n@constraint(model, x1 * x2 >= 1.0)\n@constraint(model, x1 + x2^2 >= 0.0)\n\nprintln(model)\n\n\nThen, solving HS15 with MadNLP directly translates to\n\nusing MadNLP\nJuMP.set_optimizer(model, MadNLP.Optimizer)\nJuMP.optimize!(model)\n\n\nUnder the hood, JuMP builds a nonlinear model with a sparse AD backend to evaluate the first and second-order derivatives of the objective and the constraints. Internally, MadNLP takes as input the callbacks generated by JuMP and wraps them inside a MadNLP.MOIModel.","category":"section"},{"location":"quickstart/#Specifying-the-derivatives-by-hand:-NLPModels.jl","page":"Quickstart","title":"Specifying the derivatives by hand: NLPModels.jl","text":"Alternatively, we can compute the derivatives manually and define directly a NLPModel associated to our problem. This second option, although more complicated, gives us more flexibility and comes without boilerplate.\n\nWe define a new NLPModel structure simply as:\n\nstruct HS15Model <: NLPModels.AbstractNLPModel{Float64,Vector{Float64}}\n    meta::NLPModels.NLPModelMeta{Float64, Vector{Float64}}\n    counters::NLPModels.Counters\nend\n\nfunction HS15Model(x0)\n    return HS15Model(\n        NLPModels.NLPModelMeta(\n            2,     #nvar\n            ncon = 2,\n            nnzj = 4,\n            nnzh = 3,\n            x0 = x0,\n            y0 = zeros(2),\n            lvar = [-Inf, -Inf],\n            uvar = [0.5, Inf],\n            lcon = [1.0, 0.0],\n            ucon = [Inf, Inf],\n            minimize = true\n        ),\n        NLPModels.Counters()\n    )\nend\n\nThis structure takes as input the initial position x0 and generates a AbstractNLPModel. NLPModelMeta stores the information about the structure of the problem (variables and constraints' lower and upper bounds, number of variables, number of constraints, ...). Counters is just a utility storing the number of time each callbacks is being called.\n\nThe objective function takes as input a HS15Model instance and a vector with dimension 2 storing the current values for x_1 and x_2:\n\nfunction NLPModels.obj(nlp::HS15Model, x::AbstractVector)\n    return 100.0 * (x[2] - x[1]^2)^2 + (1.0 - x[1])^2\nend\n\nThe corresponding gradient writes (note that we update the values of the gradient g inplace):\n\nfunction NLPModels.grad!(nlp::HS15Model, x::AbstractVector, g::AbstractVector)\n    z = x[2] - x[1]^2\n    g[1] = -400.0 * z * x[1] - 2.0 * (1.0 - x[1])\n    g[2] = 200.0 * z\n    return g\nend\n\nSimilarly, we define the constraints\n\nfunction NLPModels.cons!(nlp::HS15Model, x::AbstractVector, c::AbstractVector)\n    c[1] = x[1] * x[2]\n    c[2] = x[1] + x[2]^2\n    return c\nend\n\nand the associated Jacobian\n\nfunction NLPModels.jac_structure!(nlp::HS15Model, I::AbstractVector{T}, J::AbstractVector{T}) where T\n    copyto!(I, [1, 1, 2, 2])\n    copyto!(J, [1, 2, 1, 2])\nend\n\nfunction NLPModels.jac_coord!(nlp::HS15Model, x::AbstractVector, J::AbstractVector)\n    J[1] = x[2]    # (1, 1)\n    J[2] = x[1]    # (1, 2)\n    J[3] = 1.0     # (2, 1)\n    J[4] = 2*x[2]  # (2, 2)\n    return J\nend\n\nnote: Note\nAs the Jacobian is sparse, we have to provide its sparsity structure.\n\nIt remains to implement the Hessian of the Lagrangian for a HS15Model. The Lagrangian of the problem writes\n\nL(x_1 x_2 y_1 y_2) = 100 times (x_2 - x_1^2)^2 +(1 - x_1)^2\n+ y_1 times (x_1 times x_2) + y_2 times (x_1 + x_2^2)\n\nand we aim at evaluating its second-order derivative nabla^2_xxL(x_1 x_2 y_1 y_2).\n\nWe first have to define the sparsity structure of the Hessian, which is assumed to be sparse. The Hessian is a symmetric matrix, and by convention we pass only the lower-triangular part of the matrix to the solver. Hence, we define the sparsity structure as\n\nfunction NLPModels.hess_structure!(nlp::HS15Model, I::AbstractVector{T}, J::AbstractVector{T}) where T\n    copyto!(I, [1, 2, 2])\n    copyto!(J, [1, 1, 2])\nend\n\nNow that the sparsity structure is defined, the associated Hessian writes down:\n\nfunction NLPModels.hess_coord!(nlp::HS15Model, x, y, H::AbstractVector; obj_weight=1.0)\n    # Objective\n    H[1] = obj_weight * (-400.0 * x[2] + 1200.0 * x[1]^2 + 2.0)\n    H[2] = obj_weight * (-400.0 * x[1])\n    H[3] = obj_weight * 200.0\n    # First constraint\n    H[2] += y[1] * 1.0\n    # Second constraint\n    H[3] += y[2] * 2.0\n    return H\nend\n\n\nOnce the problem specified in NLPModels's syntax, we can create a new MadNLP instance and solve it:\n\nx0 = zeros(2) # initial position\nnlp = HS15Model(x0)\nsolver = MadNLP.MadNLPSolver(nlp; print_level=MadNLP.INFO)\nresults = MadNLP.solve!(solver)\n\nMadNLP converges in 19 iterations to a (local) optimal solution. MadNLP returns a MadNLPExecutionStats storing all the results. We can query the primal and the dual solutions respectively by\n\nresults.solution\n\nand\n\nresults.multipliers","category":"section"},{"location":"man/linear_solvers/#Linear-solvers","page":"Linear Solvers","title":"Linear solvers","text":"We suppose that the KKT system has been assembled previously into a given AbstractKKTSystem. Then, it remains to compute the Newton step by solving the KKT system for a given right-hand-side (given as a AbstractKKTVector). That's exactly the role of the linear solver.\n\nIf we do not assume any structure, the KKT system writes in generic form\n\nK x = b\n\nwith K the KKT matrix and b the current right-hand-side. MadNLP provides a suite of specialized linear solvers to solve the linear system.","category":"section"},{"location":"man/linear_solvers/#Inertia-detection","page":"Linear Solvers","title":"Inertia detection","text":"If the matrix K has negative eigenvalues, we have no guarantee that the solution of the KKT system is a descent direction with regards to the original nonlinear problem. That's the reason why most of the linear solvers compute the inertia of the linear system when factorizing the matrix K. The inertia counts the number of positive, negative and zero eigenvalues in the matrix. If the inertia does not meet a given criteria, then the matrix K is regularized by adding a multiple of the identity to it: K_r = K + alpha I.\n\nnote: Note\nWe recall that the inertia of a matrix K is given as a triplet (nmp), with n the number of positive eigenvalues, m the number of negative eigenvalues and p the number of zero eigenvalues.","category":"section"},{"location":"man/linear_solvers/#Factorization-algorithm","page":"Linear Solvers","title":"Factorization algorithm","text":"In nonlinear programming, it is common to employ a LBL factorization to decompose the symmetric indefinite matrix K, as this algorithm returns the inertia of the matrix directly as a result of the factorization.\n\nnote: Note\nWhen MadNLP runs in inertia-free mode, the algorithm does not require to compute the inertia when factorizing the matrix K. In that case, MadNLP can use a classical LU or QR factorization to solve the linear system Kx = b.","category":"section"},{"location":"man/linear_solvers/#Solving-a-KKT-system-with-MadNLP","page":"Linear Solvers","title":"Solving a KKT system with MadNLP","text":"We suppose available a AbstractKKTSystem kkt, properly assembled following the procedure presented previously. We can query the assembled matrix K as\n\nK = MadNLP.get_kkt(kkt)\n\n\nThen, if we want to pass the KKT matrix K to Lapack, this translates to\n\nlinear_solver = LapackCPUSolver(K)\n\n\nThe instance linear_solver does not copy the matrix K and instead keep a reference to it.\n\nlinear_solver.A === K\n\nThat way every time we re-assemble the matrix K in kkt, the values are directly updated inside linear_solver.\n\nTo compute the factorization inside linear_solver, one simply as to call:\n\nMadNLP.factorize!(linear_solver)\n\n\nOnce the factorization computed, computing the backsolve for a right-hand-side b amounts to\n\nnk = size(kkt, 1)\nb = rand(nk)\nMadNLP.solve!(linear_solver, b)\n\nThe values of b being modified inplace to store the solution x of the linear system Kx =b.","category":"section"},{"location":"options/#MadNLP-Options","page":"Options","title":"MadNLP Options","text":"Pages = [\n    \"options.md\",\n]\nDepth=3\n\n","category":"section"},{"location":"options/#Primary-options","page":"Options","title":"Primary options","text":"These options are used to set the values for other options. The default values are inferred from the NLP model.\n\ntol::Float64\n  Termination tolerance. The default value is 1e-8 for double precision. The solver terminates if the scaled primal, dual, complementary infeasibility is less than tol. Valid range is (0infty).\ncallback::Type \t Valid values are: MadNLP.{DenseCallback,SparseCallback}.\nkkt_system::Type  The type of KKT system. Valid values are MadNLP.{SparseKKTSystem,ScaledSparseKKTSystem,SparseUnreducedKKTSystem,SparseCondensedKKTSystem, DenseKKTSystem,DenseCondensedKKTSystem}.\nlinear_solver::Type\nLinear solver used for solving primal-dual system. Valid values are: {MadNLP.MumpsSolver, MadNLP.UmfpackSolver, MadNLP.LDLSolver, MadNLP.CHOLMODSolver, MadNLPPardiso.PardisoSolver, MadNLPPardiso.PardisoMKLSolver, MadNLPHSL.Ma27Solver, MadNLPHSL.Ma57Solver, MadNLPHSL.Ma77Solver, MadNLPHSL.Ma86Solver, MadNLPHSL.Ma97Solver, MadNLP.LapackCPUSolver, MadNLPGPU.LapackGPUSolver,MadNLPGPU.CUDSSSolver, MadNLPGPU.LapackROCSolver} (some may require using extension packages). The selected solver should be properly built in the build procedure. See README.md file.","category":"section"},{"location":"options/#General-options","page":"Options","title":"General options","text":"iterator::Type = RichardsonIterator\n  Iterator used for iterative refinement. Valid values are: {MadNLPRichardson,MadNLPKrylov}.\nRichardson uses Richardson iteration\nKrylov uses restarted Generalized Minimal Residual method implemented in IterativeSolvers.jl.\nblas_num_threads::Int = 1\n  Number of threads used for BLAS routines. Valid range is 1infty).\ndisable_garbage_collector::Bool = false\n  If true, Julia garbage collector is temporarily disabled while solving the problem, and then enabled back once the solution is complete.\nrethrow_error::Bool = true\n  If false, any internal error thrown by MadNLP and interruption exception (triggered by the user via ^C) is caught, and not rethrown. If an error is caught, the solver terminates with an error message.\n\n","category":"section"},{"location":"options/#Output-options","page":"Options","title":"Output options","text":"print_level::LogLevels = INFO\n  stdout print level. Any message with level less than print_level is not printed on stdout. Valid values are: MadNLP.{TRACE, DEBUG, INFO, NOTICE, WARN, ERROR}.\noutput_file::String = INFO\n  If not \"\", the output log is teed to the file at the path specified in output_file.\nfile_print_level::LogLevels = TRACE\n  File print level; any message with level less than file_print_level is not printed on the file specified in output_file. Valid values are: MadNLP.{TRACE, DEBUG, INFO, NOTICE, WARN, ERROR}.\n\n","category":"section"},{"location":"options/#Termination-options","page":"Options","title":"Termination options","text":"max_iter::Int = 3000\n  Maximum number of interior point iterations. The solver terminates with exit symbol :Maximum_Iterations_Exceeded if the interior point iteration count exceeds max_iter.\nacceptable_tol::Float64 = 1e-6\n  Acceptable tolerance. The solver terminates if the scaled primal, dual, complementary infeasibility is less than acceptable_tol, for acceptable_iter consecutive interior point iteration steps.\nacceptable_iter::Int = 15\n  Acceptable iteration tolerance. Valid rage is 1infty).\ndiverging_iterates_tol::Float64 = 1e20\n  Diverging iteration tolerance. The solver terminates with exit symbol :Diverging_Iterates if the NLP error is greater than diverging_iterates_tol.\nmax_wall_time::Float64 = 1e6\n  Maximum wall time for interior point solver. The solver terminates with exit symbol :Maximum_WallTime_Exceeded if the total solver wall time exceeds max_wall_time.\ns_max::Float64 = 100.\n\n","category":"section"},{"location":"options/#NLP-options","page":"Options","title":"NLP options","text":"kappa_d::Float64 = 1e-5\nfixed_variable_treatment::FixedVariableTreatments = MakeParameter\n  Valid values are: MadNLP.{RelaxBound,MakeParameter}.\nequality_treatment::FixedVariableTreatments = MakeParameter\n  Valid values are: MadNLP.{RelaxEquality,EnforceEquality}.\njacobian_constant::Bool = false\n  If true, constraint Jacobian is only evaluated once and reused.\nhessian_constant::Bool = false\n  If true, Lagrangian Hessian is only evaluated once and reused.\nbound_push::Float64 = 1e-2\nbound_fac::Float64 = 1e-2\nhessian_approximation::Type = ExactHessian\nquasi_newton_options::QuasiNewtonOptions = QuasiNewtonOptions()\ninertia_correction_method::InertiaCorrectionMethods = INERTIA_AUTO\n  Valid values are: MadNLP.{INERTIA_AUTO,INERTIA_BASED, INERTIA_FREE}.\nINERTIA_BASED uses the strategy in Ipopt.\nINERTIA_FREE uses the strategy in Chiang (2016).\nINERTIA_AUTO uses INERTIA_BASED if inertia information is available and uses INERTIA_FREE otherwise.\ninertia_free_tol::Float64 = 0.\n\n","category":"section"},{"location":"options/#Initialization-Options","page":"Options","title":"Initialization Options","text":"dual_initialized::Bool = false\ndual_initialization_method::Type = kkt_system <: MadNLP.SparseCondensedKKTSystem ? DualInitializeSetZero : DualInitializeLeastSquares\n\nconstr_mult_init_max::Float64 = 1e3\nnlp_scaling::Bool = true: \n  If true, MadNLP scales the nonlinear problem during the resolution.\nnlp_scaling_max_gradient::Float64 = 100.\n\n","category":"section"},{"location":"options/#Hessian-perturbation-options","page":"Options","title":"Hessian perturbation options","text":"min_hessian_perturbation::Float64 = 1e-20\nfirst_hessian_perturbation::Float64 = 1e-4\nmax_hessian_perturbation::Float64 = 1e20\nperturb_inc_fact_first::Float64 = 1e2\nperturb_inc_fact::Float64 = 8.\nperturb_dec_fact::Float64 = 1/3\njacobian_regularization_exponent::Float64 = 1/4\njacobian_regularization_value::Float64 = 1e-8\n\n","category":"section"},{"location":"options/#Restoration-options","page":"Options","title":"Restoration options","text":"soft_resto_pderror_reduction_factor::Float64 = 0.9999\nrequired_infeasibility_reduction::Float64 = 0.9\n\n","category":"section"},{"location":"options/#Line-search-options","page":"Options","title":"Line-search options","text":"obj_max_inc::Float64 = 5.\nkappha_soc::Float64 = 0.99\nmax_soc::Int = 4\nalpha_min_frac::Float64 = 0.05\ns_theta::Float64 = 1.1\ns_phi::Float64 = 2.3\neta_phi::Float64 = 1e-4\nkappa_soc::Float64 = 0.99\ngamma_theta::Float64 = 1e-5\ngamma_phi::Float64 = 1e-5\ndelta::Float64 = 1\nkappa_sigma::Float64 = 1e10\nbarrier_tol_factor::Float64 = 10.\nrho::Float64 = 1000.\n\n","category":"section"},{"location":"options/#Barrier-options","page":"Options","title":"Barrier options","text":"mu_init::Float64 = 1e-1\nmu_min::Float64 = 1e-9\nmu_superlinear_decrease_power::Float64 = 1.5\ntau_min::Float64 = 0.99\nmu_linear_decrease_factor::Float64 = .2\n\n","category":"section"},{"location":"options/#Linear-Solver-Options","page":"Options","title":"Linear Solver Options","text":"Linear solver options are specific to the linear solver chosen at linear_solver option. Irrelevant options are ignored and a warning message is printed.","category":"section"},{"location":"options/#Mumps-(default-–-available-with-MadNLP)","page":"Options","title":"Mumps (default – available with MadNLP)","text":"mumps_dep_tol::Float64 = 0.\nmumps_mem_percent::Int = 1000\nmumps_permuting_scaling::Int = 7\nmumps_pivot_order::Int = 7\nmumps_pivtol::Float64 = 1e-6\nmumps_pivtolmax::Float64 = .1\nmumps_scaling::Int = 77","category":"section"},{"location":"options/#Umfpack-(available-with-MadNLP)","page":"Options","title":"Umfpack (available with MadNLP)","text":"umfpack_pivtol::Float64 = 1e-4\numfpack_pivtolmax::Float64 = 1e-1\numfpack_sym_pivtol::Float64 = 1e-3\numfpack_block_size::Float64 = 16\numfpack_strategy::Float64 = 2.","category":"section"},{"location":"options/#Ma27-(requires-MadNLPHSL)","page":"Options","title":"Ma27 (requires MadNLPHSL)","text":"ma27_pivtol::Float64 = 1e-8\nma27_pivtolmax::Float64 = 1e-4\nma27_liw_init_factor::Float64 = 5.\nma27_la_init_factor::Float64 = 5.\nma27_meminc_factor::Float64 = 2.","category":"section"},{"location":"options/#Ma57-(requires-MadNLPHSL)","page":"Options","title":"Ma57 (requires MadNLPHSL)","text":"ma57_pivtol::Float64 = 1e-8\nma57_pivtolmax::Float64 = 1e-4\nma57_pre_alloc::Float64 = 1.05\nma57_pivot_order::Int = 5\nma57_automatic_scaling::Bool = false\nma57_block_size::Int = 16\nma57_node_amalgamation::Int = 16\nma57_small_pivot_flag::Int = 0","category":"section"},{"location":"options/#Ma77-(requires-MadNLPHSL)","page":"Options","title":"Ma77 (requires MadNLPHSL)","text":"ma77_buffer_lpage::Int = 4096\nma77_buffer_npage::Int = 1600\nma77_file_size::Int = 2097152\nma77_maxstore::Int = 0\nma77_nemin::Int = 8\nma77_order::Ma77.Ordering = Ma77.METIS\nma77_print_level::Int = -1\nma77_small::Float64 = 1e-20\nma77_static::Float64 = 0.\nma77_u::Float64 = 1e-8\nma77_umax::Float64 = 1e-4","category":"section"},{"location":"options/#Ma86-(requires-MadNLPHSL)","page":"Options","title":"Ma86 (requires MadNLPHSL)","text":"ma86_num_threads::Int = 1\nma86_print_level::Float64 = -1\nma86_nemin::Int = 32\nma86_order::Ma86.Ordering = Ma86.METIS\nma86_scaling::Ma86.Scaling = Ma86.SCALING_NONE\nma86_small::Float64 = 1e-20\nma86_static::Float64 = 0.\nma86_u::Float64 = 1e-8\nma86_umax::Float64 = 1e-4","category":"section"},{"location":"options/#Ma97-(requires-MadNLPHSL)","page":"Options","title":"Ma97 (requires MadNLPHSL)","text":"ma97_num_threads::Int = 1\nma97_print_level::Int = -1\nma97_nemin::Int = 8\nma97_order::Ma97.Ordering = Ma97.METIS\nma97_scaling::Ma97.Scaling = Ma97.SCALING_NONE\nma97_small::Float64 = 1e-20\nma97_u::Float64 = 1e-8\nma97_umax::Float64 = 1e-4","category":"section"},{"location":"options/#Pardiso-(requires-MadNLPPardiso)","page":"Options","title":"Pardiso (requires MadNLPPardiso)","text":"pardiso_matching_strategy::Pardiso.MatchingStrategy = COMPLETE2x2\npardiso_max_inner_refinement_steps::Int = 1\npardiso_msglvl::Int = 0\npardiso_order::Int = 2","category":"section"},{"location":"options/#PardisoMKL-(requires-MadNLPPardiso)","page":"Options","title":"PardisoMKL (requires MadNLPPardiso)","text":"pardisomkl_num_threads::Int = 1\npardiso_matching_strategy::PardisoMKL.MatchingStrategy = COMPLETE2x2\npardisomkl_max_iterative_refinement_steps::Int = 1\npardisomkl_msglvl::Int = 0\npardisomkl_order::Int = 2","category":"section"},{"location":"options/#LapackGPUSolver-(requires-MadNLPGPU)","page":"Options","title":"LapackGPUSolver (requires MadNLPGPU)","text":"lapack_algorithm::LapackGPU.Algorithms = MadNLP.BUNCHKAUFMAN","category":"section"},{"location":"options/#LapackROCSolver-(requires-MadNLPGPU)","page":"Options","title":"LapackROCSolver (requires MadNLPGPU)","text":"lapack_algorithm::LapackGPU.Algorithms = MadNLP.EVD","category":"section"},{"location":"options/#CUDSSSolver-(requires-MadNLPGPU)","page":"Options","title":"CUDSSSolver (requires MadNLPGPU)","text":"cudss_algorithm::MadNLP.LinearFactorization = MadNLP.LDL\ncudss_ordering::ORDERING = DEFAULT_ORDERING\ncudss_perm::Vector{Cint} = Cint[]\ncudss_ir::Int = 0\ncudss_ir_tol::Float64 = 1e-8\ncudss_pivot_threshold::Float64 = 0.0\ncudss_pivot_epsilon::Float64 = 0.0\ncudss_matching_alg::String = \"default\"\ncudss_reordering_alg::String = \"default\"\ncudss_factorization_alg::String = \"default\"\ncudss_solve_alg::String = \"default\"\ncudss_matching::Bool = false\ncudss_pivoting::Bool = true\ncudss_hybrid_execute::Bool = false\ncudss_hybrid_memory::Bool = false\ncudss_hybrid_memory_limit::Int = 0\ncudss_superpanels::Bool = true\ncudss_schur::Bool = false\ncudss_deterministic::Bool = false\ncudss_device_indices::Vector{Cint} = Cint[]","category":"section"},{"location":"options/#Iterator-Options","page":"Options","title":"Iterator Options","text":"","category":"section"},{"location":"options/#Richardson","page":"Options","title":"Richardson","text":"richardson_max_iter::Int = 10 \n  Maximum number of Richardson iteration steps. Valid range is 1infty).\nrichardson_tol::Float64 = 1e-10 \n  Convergence tolerance of Richardson iteration. Valid range is (0infty).\nrichardson_acceptable_tol::Float64 = 1e-5 \n  Acceptable convergence tolerance of Richardson iteration. If the Richardson iteration counter exceeds richardson_max_iter without satisfying the convergence criteria set with richardson_tol, the Richardson solver checks whether the acceptable convergence criteria set with richardson_acceptable_tol is satisfied; if the acceptable convergence criteria is satisfied, the computed step is used; otherwise, the augmented system is treated to be singular. Valid range is (0infty).","category":"section"},{"location":"options/#Krylov-(requires-MadNLPIterative)","page":"Options","title":"Krylov (requires MadNLPIterative)","text":"krylov_max_iter::Int = 10 \n  Maximum number of Krylov iteration steps. Valid range is 1infty).\nkrylov_tol::Float64 = 1e-10 \n  Convergence tolerance of Krylov iteration. Valid range is (0infty).\nkrylov_acceptable_tol::Float64 = 1e-5 \n  Acceptable convergence tolerance of Krylov iteration. If the Krylov iteration counter exceeds krylov_max_iter without satisfying the convergence criteria set with krylov_tol, the Krylov solver checks whether the acceptable convergence criteria set with krylov_acceptable_tol is satisfied; if the acceptable convergence criteria is satisfied, the computed step is used; otherwise, the augmented system is treated to be singular. Valid range is (0infty).\nkrylov_restart::Int = 5 \n  Maximum Krylov iteration before restarting. Valid range is 1infty).","category":"section"},{"location":"options/#Reference","page":"Options","title":"Reference","text":"[Bunch, 1977]: J R Bunch and L Kaufman, Some stable methods for calculating inertia and solving symmetric linear systems, Mathematics of Computation 31:137 (1977), 163-179.\n\n[Greif, 2014]: Greif, Chen, Erin Moulding, and Dominique Orban. \"Bounds on eigenvalues of matrices arising from interior-point methods.\" SIAM Journal on Optimization 24.1 (2014): 49-83.\n\n[Wächter, 2006]: Wächter, Andreas, and Lorenz T. Biegler. \"On the implementation of an interior-point filter line-search algorithm for large-scale nonlinear programming.\" Mathematical programming 106.1 (2006): 25-57.\n\n[Chiang, 2016]: Chiang, Nai-Yuan, and Victor M. Zavala. \"An inertia-free filter line-search algorithm for large-scale nonlinear programming.\" Computational Optimization and Applications 64.2 (2016): 327-354.","category":"section"},{"location":"lib/callbacks/#Callbacks","page":"Callback wrappers","title":"Callbacks","text":"In MadNLP, a nonlinear program is implemented with a given AbstractNLPModel. The model may have a form unsuitable for the interior-point algorithm. For that reason, MadNLP wraps the AbstractNLPModel internally using custom data structures, encoded as a AbstractCallback. Depending on the setting, choose to wrap the AbstractNLPModel as a DenseCallback or alternatively, as a SparseCallback.\n\nThe function create_callback allows to instantiate a AbstractCallback from a given NLPModel:\n\nInternally, a AbstractCallback reformulates the inequality constraints as equality constraints by introducing additional slack variables. The fixed variables are reformulated as parameters (using MakeParameter) or are relaxed (using RelaxBound). The equality constraints can be keep as is with EnforceEquality (default option) or relaxed as inequality constraints with RelaxEquality. In that later case, MadNLP solves a relaxation of the original problem.\n\nMadNLP has to keep in memory all the indexes associated to the equality and inequality constraints, as well as the indexes of the bounded variables and the fixed variables. The indexes are stored explicitly as a Vector{Int} in the AbstractCallback structure used by MadNLP.","category":"section"},{"location":"lib/callbacks/#MadNLP.AbstractCallback","page":"Callback wrappers","title":"MadNLP.AbstractCallback","text":"AbstractCallback{T, VT}\n\nWrap the AbstractNLPModel passed by the user in a form amenable to MadNLP.\n\nAn AbstractCallback handles the scaling of the problem and the reformulations of the equality constraints and fixed variables.\n\n\n\n\n\n","category":"type"},{"location":"lib/callbacks/#MadNLP.DenseCallback","page":"Callback wrappers","title":"MadNLP.DenseCallback","text":"DenseCallback{T, VT} < AbstractCallback{T, VT}\n\nWrap an AbstractNLPModel using dense structures.\n\n\n\n\n\n","category":"type"},{"location":"lib/callbacks/#MadNLP.SparseCallback","page":"Callback wrappers","title":"MadNLP.SparseCallback","text":"SparseCallback{T, VT} < AbstractCallback{T, VT}\n\nWrap an AbstractNLPModel using sparse structures.\n\n\n\n\n\n","category":"type"},{"location":"lib/callbacks/#MadNLP.create_callback","page":"Callback wrappers","title":"MadNLP.create_callback","text":"create_callback(\n    ::Type{Callback},\n    nlp::AbstractNLPModel{T, VT};\n    fixed_variable_treatment=MakeParameter,\n    equality_treatment=EnforceEquality,\n) where {T, VT}\n\nWrap the nonlinear program nlp using the callback wrapper with type Callback. The option fixed_variable_treatment decides if the fixed variables are relaxed (RelaxBound) or removed (MakeParameter). The option equality_treatment decides if the the equality constraints are keep as is (EnforceEquality) or relaxed (RelaxEquality).\n\n\n\n\n\n","category":"function"},{"location":"lib/callbacks/#MadNLP.AbstractFixedVariableTreatment","page":"Callback wrappers","title":"MadNLP.AbstractFixedVariableTreatment","text":"AbstractFixedVariableTreatment\n\nAbstract type to define the reformulation of the fixed variables inside MadNLP.\n\n\n\n\n\n","category":"type"},{"location":"lib/callbacks/#MadNLP.MakeParameter","page":"Callback wrappers","title":"MadNLP.MakeParameter","text":"MakeParameter{VT, VI} <: AbstractFixedVariableTreatment\n\nRemove the fixed variables from the optimization variables and define them as problem's parameters.\n\n\n\n\n\n","category":"type"},{"location":"lib/callbacks/#MadNLP.RelaxBound","page":"Callback wrappers","title":"MadNLP.RelaxBound","text":"RelaxBound <: AbstractFixedVariableTreatment\n\nRelax the fixed variables x = x_fixed as bounded variables x_fixed - ϵ  x  x_fixed + ϵ, with ϵ a small-enough parameter.\n\n\n\n\n\n","category":"type"},{"location":"lib/callbacks/#MadNLP.AbstractEqualityTreatment","page":"Callback wrappers","title":"MadNLP.AbstractEqualityTreatment","text":"AbstractEqualityTreatment\n\nAbstract type to define the reformulation of the equality constraints inside MadNLP.\n\n\n\n\n\n","category":"type"},{"location":"lib/callbacks/#MadNLP.EnforceEquality","page":"Callback wrappers","title":"MadNLP.EnforceEquality","text":"EnforceEquality <: AbstractEqualityTreatment\n\nKeep the equality constraints intact.\n\nThe solution returned by MadNLP will respect the equality constraints.\n\n\n\n\n\n","category":"type"},{"location":"lib/callbacks/#MadNLP.RelaxEquality","page":"Callback wrappers","title":"MadNLP.RelaxEquality","text":"RelaxEquality <: AbstractEqualityTreatment\n\nRelax the equality constraints g(x) = 0 with two inequality constraints, as -ϵ  g(x)  ϵ. The parameter ϵ is usually small.\n\nThe solution returned by MadNLP will satisfy the equality constraints only up to a tolerance ϵ.\n\n\n\n\n\n","category":"type"},{"location":"","page":"Home","title":"Home","text":"(Image: Logo)\n\nMadNLP is an open-source nonlinear programming solver, purely implemented in Julia. MadNLP implements a filter line-search interior-point algorithm, as used in Ipopt. MadNLP seeks to streamline the development of modeling and algorithmic paradigms in order to exploit structures and to make efficient use of high-performance computers.","category":"section"},{"location":"#Design","page":"Home","title":"Design","text":"","category":"section"},{"location":"#MadNLP's-problem-structure","page":"Home","title":"MadNLP's problem structure","text":"MadNLP targets the resolution of constrained nonlinear problems, formulating as\n\n  beginaligned\n    min_x   f(x) \n    textsubject to quad  g_ell leq g(x) leq g_u \n                             x_ell leq x leq x_u\n  endaligned\n\nwhere x in mathbbR^n is the decision variable, f mathbbR^n to mathbbR and g mathbbR^n to mathbbR^m two nonlinear functions. MadNLP makes the distinction between the bound constraints x_ell leq x leq x_u and the generic constraints g_ell leq g(x) leq g_u. No other structure is assumed a priori.\n\nMadNLP is built on top of NLPModels.jl, a generic package to represent optimization models in Julia. In addition, MadNLP is interfaced with JuMP and Plasmo.\n\nnote: Note\nMadNLP requires the evaluation of both the first-order and the second-order derivatives of the nonlinear problem.","category":"section"},{"location":"#MadNLP's-algorithm","page":"Home","title":"MadNLP's algorithm","text":"MadNLP implements a primal-dual filter line-search interior-point algorithm, closely related to Ipopt. The nonlinear problem is reformulated in standard form by introducing a slack variables s in mathbbR^m to rewrite all the inequality constraints as equality constraints:\n\n  beginaligned\n    min_x s   f(x) \n    textsubject to quad  g(x) - s = 0  \n                             g_ell leq s leq g_u \n                             x_ell leq x leq x_u\n  endaligned\n\nThe algorithm starts from an initial primal-dual iterate (x_0 s_0 y_0). Then, at each iteration the current iterate is updated by solving the KKT linear system:\n\nbeginbmatrix\n    W_k + Sigma_x  0  A_k^top \n    0  Sigma_s  - I \n    A_k  -I  0\nendbmatrix\nbeginbmatrix\n    Delta x  Delta s  Delta y\nendbmatrix\n=\n-\nbeginbmatrix\n    nabla f(x_k) + A_k^top y_k - mu U^-1 e_n \n    y_k - mu S^-1 e_n \n    g(x_k) - s_k\nendbmatrix\n\n\nwith mu being the current barrier parameter.\n\nWe call the linear system the augmented KKT system at iteration k.","category":"section"},{"location":"#MadNLP's-performance","page":"Home","title":"MadNLP's performance","text":"In any interior-point algorithm, the two computational bottlenecks are\n\nThe evaluation of the first- and second-order derivatives.\nThe factorization of the augmented KKT system.\n\nThe first point is problem-dependent and is often related to the automatic differentiation backend being employed. The second point is more problematic, as the augmented KKT system is usually symmetric indefinite and ill-conditioned. For this reason, we recommend using efficient sparse linear solvers that are not publicly available, such as HSL or PARDISO, if they are available to the user.","category":"section"},{"location":"#Citing-MadNLP.jl","page":"Home","title":"Citing MadNLP.jl","text":"If you use MadNLP.jl in your research, we would greatly appreciate your citing it.\n\n@article{shin2024accelerating,\n  title     = {Accelerating optimal power flow with {GPU}s: {SIMD} abstraction of nonlinear programs and condensed-space interior-point methods},\n  author    = {Shin, Sungho and Anitescu, Mihai and Pacaud, Fran{\\c{c}}ois},\n  journal   = {Electric Power Systems Research},\n  volume    = {236},\n  pages     = {110651},\n  year      = {2024},\n  publisher = {Elsevier}\n}\n\n@article{shin2021graph,\n  title     = {Graph-based modeling and decomposition of energy infrastructures},\n  author    = {Shin, Sungho and Coffrin, Carleton and Sundar, Kaarthik and Zavala, Victor M},\n  journal   = {IFAC-PapersOnLine},\n  volume    = {54},\n  number    = {3},\n  pages     = {693--698},\n  year      = {2021},\n  publisher = {Elsevier}\n}","category":"section"}]
}
