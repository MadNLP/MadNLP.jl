<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Algorithm · MadNLP.jl</title><meta name="title" content="Algorithm · MadNLP.jl"/><meta property="og:title" content="Algorithm · MadNLP.jl"/><meta property="twitter:title" content="Algorithm · MadNLP.jl"/><meta name="description" content="Documentation for MadNLP.jl."/><meta property="og:description" content="Documentation for MadNLP.jl."/><meta property="twitter:description" content="Documentation for MadNLP.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="MadNLP.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">MadNLP.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../installation/">Installation</a></li><li><a class="tocitem" href="../quickstart/">Quickstart</a></li><li class="is-active"><a class="tocitem" href>Algorithm</a><ul class="internal"><li><a class="tocitem" href="#Pre-processing"><span>Pre-processing</span></a></li><li><a class="tocitem" href="#Interior-point-iterations"><span>Interior-point iterations</span></a></li></ul></li><li><a class="tocitem" href="../options/">Options</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../tutorials/gpu/">GPU acceleration</a></li><li><a class="tocitem" href="../tutorials/multiprecision/">Multi-precision</a></li><li><a class="tocitem" href="../tutorials/warmstart/">Warm-start</a></li><li><a class="tocitem" href="../tutorials/quasi_newton/">Quasi-Newton</a></li><li><a class="tocitem" href="../tutorials/kktsystem/">Custom KKT system</a></li></ul></li><li><span class="tocitem">Manual</span><ul><li><a class="tocitem" href="../man/kkt/">KKT systems</a></li><li><a class="tocitem" href="../man/linear_solvers/">Linear Solvers</a></li></ul></li><li><span class="tocitem">API Reference</span><ul><li><a class="tocitem" href="../lib/ipm/">IPM solver</a></li><li><a class="tocitem" href="../lib/barrier/">Barrier strategies</a></li><li><a class="tocitem" href="../lib/callbacks/">Callback wrappers</a></li><li><a class="tocitem" href="../lib/kkt/">KKT systems</a></li><li><a class="tocitem" href="../lib/linear_solvers/">Linear Solvers</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Algorithm</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Algorithm</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/MadNLP/MadNLP.jl/blob/master/docs/src/algorithm.md#" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Interior-point-algorithm"><a class="docs-heading-anchor" href="#Interior-point-algorithm">Interior-point algorithm</a><a id="Interior-point-algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Interior-point-algorithm" title="Permalink"></a></h1><p>We give a brief description of the interior-point algorithm used in MadNLP, together with <a href="../options/">the principal options</a> impacting MadNLP&#39;s behavior. The algorithm is described in more length in the <a href="https://link.springer.com/article/10.1007/S10107-004-0559-Y">Ipopt paper</a>.</p><p>MadNLP searches for a local solution of the nonlinear program:</p><p class="math-container">\[  \begin{aligned}
    \min_{x} \; &amp; f(x) \;, \\
    \text{subject to} \quad &amp; g_\ell \leq g(x) \leq g_u \; ,\\
                            &amp; x_\ell \leq x \leq x_u \; ,
  \end{aligned}\]</p><p>where <span>$x \in \mathbb{R}^n$</span> is the decision variable, <span>$f: \mathbb{R}^n \to \mathbb{R}$</span> and <span>$g: \mathbb{R}^n \to \mathbb{R}^m$</span> two smooth nonlinear functions.</p><h2 id="Pre-processing"><a class="docs-heading-anchor" href="#Pre-processing">Pre-processing</a><a id="Pre-processing-1"></a><a class="docs-heading-anchor-permalink" href="#Pre-processing" title="Permalink"></a></h2><p>Before running the interior-point method, MadNLP applies a pre-processing to the problem to improve the numerical performance. The pre-processing operations are described below.</p><h3 id="Problem&#39;s-reformulation"><a class="docs-heading-anchor" href="#Problem&#39;s-reformulation">Problem&#39;s reformulation</a><a id="Problem&#39;s-reformulation-1"></a><a class="docs-heading-anchor-permalink" href="#Problem&#39;s-reformulation" title="Permalink"></a></h3><h4 id="Slack-variables"><a class="docs-heading-anchor" href="#Slack-variables">Slack variables</a><a id="Slack-variables-1"></a><a class="docs-heading-anchor-permalink" href="#Slack-variables" title="Permalink"></a></h4><p>First, MadNLP splits the equality from the inequality constraints in the definition of the feasible set:</p><p class="math-container">\[g_\ell \leq g(x) \leq g_u \; .\]</p><p>If <span>$g_{\ell,i} &lt; g_{u,i}$</span>, the constraint is reformulated as</p><p class="math-container">\[g_i(x) - s_i = 0  \; , \quad g_{\ell, i} \leq s_i \leq g_{u,i} \; ,\]</p><p>with <span>$s_i$</span> an additional slack variable. By doing so, all the inequality constraints are moved into the bound constraints. This benefits directly to the interior-point algorithm, as it becomes much easier to compute a strictly feasible initial point.</p><div class="admonition is-info" id="Info-51a88b4de5b9bc38"><header class="admonition-header">Info<a class="admonition-anchor" href="#Info-51a88b4de5b9bc38" title="Permalink"></a></header><div class="admonition-body"><p>MadNLP only reformulates with a slack variable the inequality constraints, the equality constraints are left untouched.</p></div></div><p>As a result, we obtain an equivalent problem with the following structure:</p><p class="math-container">\[  \begin{aligned}
    \min_{w} \; &amp; f(w) \; , \\
    \text{subject to} \quad &amp; c(w) = 0 \; , \quad w \geq 0 \; .
  \end{aligned}\]</p><p>with <span>$w = (x, s)$</span> and <span>$c(w) = (g_I(x) - s, g_E(x))$</span>, with <span>$I$</span> the index set for the inequality constraints and <span>$E$</span> the index set for equality constraints.</p><div class="admonition is-info" id="Info-d7d2af2ada8ff2af"><header class="admonition-header">Info<a class="admonition-anchor" href="#Info-d7d2af2ada8ff2af" title="Permalink"></a></header><div class="admonition-body"><p>To simplify the exposition, we have assumed that the variables have all their lower bound sets to zero, and no upper bound.</p></div></div><p>The KKT stationary conditions associated to the reformulated problem are:</p><p class="math-container">\[  \begin{aligned}
    &amp; \nabla f(w) + \nabla c(w)^\top y - z = 0 \; , \\
    &amp; c(w) = 0 \; , \\
    &amp; 0 \leq w \perp z \geq 0 \; .
  \end{aligned}\]</p><p>MadNLP looks for a primal-dual solution <span>$(w, y, z)$</span> satisfying the KKT conditions.</p><p>The Lagrangian of the problem is:</p><p class="math-container">\[L(w, y, z) = f(w) + c(w)^\top y - z^\top w \; .\]</p><h4 id="Fixed-variables"><a class="docs-heading-anchor" href="#Fixed-variables">Fixed variables</a><a id="Fixed-variables-1"></a><a class="docs-heading-anchor-permalink" href="#Fixed-variables" title="Permalink"></a></h4><p>If for some <span>$i=1, \cdots,n$</span> we have <span>$x_{\ell,i} = x_{u,i}$</span>, the variable&#39;s lower bound is equal to its upper bound, meaning the variable is fixed. By default, MadNLP removes all the fixed variables in the problem.</p><p>Alternatively, MadNLP can relax all the fixed variables with a small parameter epsilon as</p><p class="math-container">\[x_{\ell,i} - \epsilon \leq x_i \leq x_{u,i} + \epsilon \; .\]</p><p>The behavior is specified by the option <code>fixed_variable_treatment</code>: if set to <a href="../lib/callbacks/#MadNLP.MakeParameter"><code>MakeParameter</code></a>, MadNLP removes the fixed variables, and if it is set to <a href="../lib/callbacks/#MadNLP.RelaxBound"><code>RelaxBound</code></a> MadNLP relaxes the bounds.</p><h4 id="Equality-constraints"><a class="docs-heading-anchor" href="#Equality-constraints">Equality constraints</a><a id="Equality-constraints-1"></a><a class="docs-heading-anchor-permalink" href="#Equality-constraints" title="Permalink"></a></h4><p>As discussed before, MadNLP keeps the equality constraints <span>$g_E(x) = 0$</span> untouched in the problem. However, it is sometimes appropriate to relax the equality constraints by converting them to (tight) inequality constraints:</p><p class="math-container">\[-\tau \leq g_E(x) \leq \tau \; .\]</p><p>That behavior is determined by the option <code>equality_treatment</code>. It is set to <a href="../lib/callbacks/#MadNLP.EnforceEquality"><code>EnforceEquality</code></a> by default. If otherwise it is set to <a href="../lib/callbacks/#MadNLP.RelaxEquality"><code>RelaxEquality</code></a>, the equality constraints are relaxed as inequality constraints.</p><h3 id="Scaling"><a class="docs-heading-anchor" href="#Scaling">Scaling</a><a id="Scaling-1"></a><a class="docs-heading-anchor-permalink" href="#Scaling" title="Permalink"></a></h3><p>Once the problem has been reformulated, MadNLP scales the objective and the constraints to ensure that the gradient and the rows of the Jacobian have all a norm being less than <code>nlp_scaling_max_gradient</code> (by default equal to <code>100.0</code>). The lower the value of <code>nlp_scaling_max_gradient</code>, the more aggressive the scaling gets.</p><p>The scaling can be deactivated by setting <code>nlp_scaling=false</code>.</p><h3 id="Computing-the-initial-primal-dual-iterate"><a class="docs-heading-anchor" href="#Computing-the-initial-primal-dual-iterate">Computing the initial primal-dual iterate</a><a id="Computing-the-initial-primal-dual-iterate-1"></a><a class="docs-heading-anchor-permalink" href="#Computing-the-initial-primal-dual-iterate" title="Permalink"></a></h3><p>The user can pass to MadNLP an initial primal point <span>$x_0$</span>. MadNLP modifies it to ensure it is strictly feasible by applying the operation</p><p class="math-container">\[x_{0,j} = \max(x_{0,j}, \kappa_1) \; ,\]</p><p>with <span>$\kappa_1$</span> the parameter specified in the option <code>bound_push</code>.</p><p>If <code>dual_initialized=false</code> (default), MadNLP computes the initial dual multiplier <span>$y_0$</span> as solution of the least-square problem</p><p class="math-container">\[\begin{bmatrix}
I &amp; J_0^\top \\
J_0 &amp; 0
\end{bmatrix}
\begin{bmatrix}
w \\ y_0
\end{bmatrix}
=
-
\begin{bmatrix}
 \nabla f(x_0) - z_0  \\ 0
\end{bmatrix} \; .\]</p><p>Otherwise, if <code>dual_initialized=true</code>, MadNLP uses the values passed by the user.</p><div class="admonition is-info" id="Info-7047e08ed4d1d19d"><header class="admonition-header">Info<a class="admonition-anchor" href="#Info-7047e08ed4d1d19d" title="Permalink"></a></header><div class="admonition-body"><p>The user cannot pass the initial value of the bound multiplier. MadNLP automatically sets <span>$z_0 = 1$</span>.</p></div></div><h2 id="Interior-point-iterations"><a class="docs-heading-anchor" href="#Interior-point-iterations">Interior-point iterations</a><a id="Interior-point-iterations-1"></a><a class="docs-heading-anchor-permalink" href="#Interior-point-iterations" title="Permalink"></a></h2><p>MadNLP solves the KKT conditions iteratively using a globalized Newton algorithm. The interior-point method reformulates the KKT conditions using a homotopy method, with, for a positive barrier parameter <span>$\mu &gt; 0$</span>:</p><p class="math-container">\[\begin{aligned}
&amp; \nabla f(w) + \nabla c(w)^\top y - z = 0 \; , \\
&amp; c(w) = 0 \; , \\
&amp; WZe = \mu e \;, \; (w, z) &gt; 0 \; .
\end{aligned}\]</p><p>The algorithm stops as soon as <code>max(inf_pr, inf_du, inf_compl) &lt; tol</code>, with</p><p class="math-container">\[\begin{aligned}
&amp; \texttt{inf\_pr} = \| \nabla f(w) + \nabla c(w)^\top y - z \|_\infty \; , \\
&amp; \texttt{inf\_du} = \| c(w) \|_\infty \; , \\
&amp; \texttt{inf\_compl} = \| WZe \|_\infty \; .
\end{aligned}\]</p><p>The tolerance <code>tol</code> is set to <code>1e-8</code> by default.</p><p>If the stopping criterion is not satisfied, MadNLP moves to the next iteration (here denoted with an index <span>$k$</span>). The iteration proceeds in four steps.</p><h3 id="Step-1:-Computing-the-first-and-second-order-sensitivities"><a class="docs-heading-anchor" href="#Step-1:-Computing-the-first-and-second-order-sensitivities">Step 1: Computing the first and second-order sensitivities</a><a id="Step-1:-Computing-the-first-and-second-order-sensitivities-1"></a><a class="docs-heading-anchor-permalink" href="#Step-1:-Computing-the-first-and-second-order-sensitivities" title="Permalink"></a></h3><p>First, MadNLP evaluates the Jacobian of the constraints <span>$J_k = \nabla c(w_k)^\top$</span> and the Hessian of the Lagrangian <span>$H_k = \nabla_{xx}^2 L(w_k, y_k, z_k)$</span>.</p><p>MadNLP evaluates the sensitivities inside an <a href="../lib/callbacks/#MadNLP.AbstractCallback"><code>AbstractCallback</code></a>, which acts as a buffer between the solver and the model implemented with <code>NLPModels</code>. Compared to the original model, the <code>AbstractCallback</code> adds the slack variable and scales the problem appropriately.</p><p>By default the two matrices <span>$J_k$</span> and <span>$H_k$</span> are assumed sparse, with only the non-zero entries being stored. If convenient, the user can evaluate them in dense format by switching to a <a href="../lib/callbacks/#MadNLP.DenseCallback"><code>DenseCallback</code></a> by setting the option:</p><pre><code class="nohighlight hljs">callback=DenseCallback</code></pre><h3 id="Step-2:-Updating-the-barrier-parameter"><a class="docs-heading-anchor" href="#Step-2:-Updating-the-barrier-parameter">Step 2: Updating the barrier parameter</a><a id="Step-2:-Updating-the-barrier-parameter-1"></a><a class="docs-heading-anchor-permalink" href="#Step-2:-Updating-the-barrier-parameter" title="Permalink"></a></h3><p>Once the sensitivities are evaluated, MadNLP updates the barrier parameter <span>$\mu$</span>. By default, MadNLP uses the monotone update rule inspired by the Fiacco-McCormick rule: <a href="../lib/barrier/#MadNLP.MonotoneUpdate"><code>MonotoneUpdate</code></a>. Alternatively, MadNLP provides two adaptive update rules, implemented in the solver respectively as <a href="../lib/barrier/#MadNLP.QualityFunctionUpdate"><code>QualityFunctionUpdate</code></a> and <a href="../lib/barrier/#MadNLP.LOQOUpdate"><code>LOQOUpdate</code></a>. The user can change the barrier update, e.g. by setting the option</p><pre><code class="nohighlight hljs">barrier=QualityFunctionUpdate()</code></pre><div class="admonition is-info" id="Info-f1ee547a82c2df0b"><header class="admonition-header">Info<a class="admonition-anchor" href="#Info-f1ee547a82c2df0b" title="Permalink"></a></header><div class="admonition-body"><p>We recommend using an adaptive barrier update for difficult problems, in particular if we have a poor initial iterate <span>$x_0$</span>.</p></div></div><h3 id="Step-3:-Solving-the-primal-dual-KKT-system"><a class="docs-heading-anchor" href="#Step-3:-Solving-the-primal-dual-KKT-system">Step 3: Solving the primal-dual KKT system</a><a id="Step-3:-Solving-the-primal-dual-KKT-system-1"></a><a class="docs-heading-anchor-permalink" href="#Step-3:-Solving-the-primal-dual-KKT-system" title="Permalink"></a></h3><p>With the new barrier parameter, we can compute a new KKT residual. MadNLP aims at decreasing this residual by computing a Newton step <span>$(\Delta w, \Delta y, \Delta z)$</span> solution of the primal-dual KKT system:</p><p class="math-container">\[\begin{bmatrix}
H_k +\delta_x I &amp; J_k^\top &amp; -I \\
J_k &amp; -\delta_y &amp; 0 \\
Z_k &amp; 0 &amp; W_k
\end{bmatrix}
\begin{bmatrix}
\Delta w \\ \Delta y \\ \Delta z
\end{bmatrix}
= -
\begin{bmatrix}
 \nabla f(w_k) + \nabla c(w_k)^\top y_k - z_k \\
 c(w_k)  \\
 W_k Z_k e - \mu e
\end{bmatrix} \; ,\]</p><p>with <span>$\delta_x$</span> and <span>$\delta_y$</span> appropriate primal-dual regularization terms whose exact roles are detailed hereafter.</p><h4 id="Solution-of-the-primal-dual-KKT-system"><a class="docs-heading-anchor" href="#Solution-of-the-primal-dual-KKT-system">Solution of the primal-dual KKT system</a><a id="Solution-of-the-primal-dual-KKT-system-1"></a><a class="docs-heading-anchor-permalink" href="#Solution-of-the-primal-dual-KKT-system" title="Permalink"></a></h4><p>The linear system is called a <em>primal-dual KKT system</em>. MadNLP can solve a symmetrized version of the primal-dual KKT system, known as an <a href="../lib/kkt/#MadNLP.AbstractUnreducedKKTSystem"><code>AbstractUnreducedKKTSystem</code></a>. However, it is beneficial to remove the blocks associated to <span>$\Delta z$</span>  and solve the smaller <a href="../lib/kkt/#MadNLP.AbstractReducedKKTSystem"><code>AbstractReducedKKTSystem</code></a>:</p><p class="math-container">\[\begin{bmatrix}
H_k + \Sigma_k + \delta_x I &amp; J_k^\top \\
J_k &amp; -\delta_y
\end{bmatrix}
\begin{bmatrix}
\Delta w \\ \Delta y
\end{bmatrix}
= -
\begin{bmatrix}
 \nabla f(w_k) + \nabla c(w_k)^\top y_k - \mu_k W_k^{-1} e \\
 c(w_k)
\end{bmatrix}\]</p><p>with the diagonal matrix <span>$\Sigma_k = W_k^{-1} Z_k$</span>. The default implementation of the <code>AbstractReducedKKTSystem</code> is provided in <a href="../lib/kkt/#MadNLP.SparseKKTSystem"><code>SparseKKTSystem</code></a>. If the problem exhibits significant ill-conditioning, the user can also use a <a href="../lib/kkt/#MadNLP.ScaledSparseKKTSystem"><code>ScaledSparseKKTSystem</code></a>, which is more numerically stable than the <a href="../lib/kkt/#MadNLP.SparseKKTSystem"><code>SparseKKTSystem</code></a>.</p><p>There exists a smaller system <a href="../lib/kkt/#MadNLP.AbstractCondensedKKTSystem"><code>AbstractCondensedKKTSystem</code></a> that also removes the blocks associated to the inequality constraints. This system is useful if the problem has many inequality constraints, or more importantly, <a href="../tutorials/gpu/">to run MadNLP on the GPU</a>. The user can switch the KKT system by using the option</p><pre><code class="language-julia hljs">kkt_system=SparseUnreducedKKTSystem</code></pre><p>For certain highly structured problem, it may be beneficial to implement a <a href="../tutorials/kktsystem/">custom KKT system</a> to exploit the problem&#39;s structure.</p><h4 id="Inertia-correction"><a class="docs-heading-anchor" href="#Inertia-correction">Inertia-correction</a><a id="Inertia-correction-1"></a><a class="docs-heading-anchor-permalink" href="#Inertia-correction" title="Permalink"></a></h4><p>The vector <span>$(\Delta w, \Delta y, \Delta z)$</span> is a descent direction if the reduced Hessian (the Hessian <span>$H_k$</span> projected on the null-space of the Jacobian <span>$J_k$</span>) of the primal-dual KKT system is <strong>positive definite</strong>. The reduced Hessian is positive definite if and only if the inertia of the reduced primal-dual KKT system (the number of positive, zero and negative eigenvalues) should exactly be equal to <span>$(n, 0, m)$</span>.</p><p>MadNLP uses an inertia correction mechanism that increases the values of the primal and dual regularizations <span>$(\delta_x, \delta_y)$</span> in the KKT system until the inertia of the system is exactly <span>$(n, 0, m)$</span>. This procedure requires using an <em>inertia-revealing</em> inertia solver that returns explicitly the inertia of the system as an output of the factorization. This procedure can be costly, as it involves re-factorizing the linear system each time a new regularization is tried.</p><p>If the linear solver does not compute the inertia of the KKT system, MadNLP uses the inertia-free algorithm described <a href="https://link.springer.com/article/10.1007/s10589-015-9820-y">in this article</a>. This procedure is less stringent than the classical inertia-based correction, in the sense that it doesn&#39;t require the reduced Hessian to be positive definite. The inertia-free correction can be activated explicitly by using the option <code>inertia_correction_method=InertiaFree</code>.</p><h3 id="Step-4:-Finding-the-next-iterate-using-the-filter-line-search"><a class="docs-heading-anchor" href="#Step-4:-Finding-the-next-iterate-using-the-filter-line-search">Step 4: Finding the next iterate using the filter line-search</a><a id="Step-4:-Finding-the-next-iterate-using-the-filter-line-search-1"></a><a class="docs-heading-anchor-permalink" href="#Step-4:-Finding-the-next-iterate-using-the-filter-line-search" title="Permalink"></a></h3><p>MadNLP uses the descent direction <span>$(\Delta w, \Delta y, \Delta z)$</span> to compute the next iterate as</p><p class="math-container">\[w_{k+1} = w_k + \alpha_k^p \Delta w \; , \quad
y_{k+1} = y_k + \alpha_k^d \Delta y \; , \quad
z_{k+1} = z_k + \alpha_k^d \Delta z \; .\]</p><p>First, the algorithm computes the maximum primal-dual steps <span>$(\alpha_k^{p,max}, \alpha_k^{d,max})$</span> satisfying the <em>fraction-to-boundary</em> rule:</p><p class="math-container">\[w_k + \alpha_k^{p,max} \Delta w \geq (1-\tau) w_k  \; , \quad
z_k + \alpha_k^{d,max} \Delta z \geq (1-\tau) z_k  \; .\]</p><p>This ensures that the iterates remain positive: <span>$(w_{k+1}, z_{k+1}) &gt; 0$</span>. In MadNLP, <span>$\tau = \max(\tau_{min}, 1-\mu)$</span>, with <span>$\tau_{min}$</span> a parameter defined by the option <code>tau_min</code>.</p><p>Once the maximum steps computed with the fraction-to-boundary rule, MadNLP finds the new iterate by using a backtracking line-search by testing different trial values for <span>$\alpha_{k,l} = \frac{1}{2^l} \alpha_k^{p,max}$</span>. Once a new trial iterate <span>$w_{k,l} = w_k + \alpha_{k,l} \Delta w$</span> achieves sufficient progress, the line-search stops and MadNLP proceeds to the next iteration. The progress is measured by a <a href="https://link.springer.com/article/10.1007/s101070100244">filter</a>, which accepts a new point either if it reduces the value of the barrier function <span>$\phi_\mu(w) = f(w) - \mu \sum_{i=1}^n \log(w_i)$</span> or it reduces the constraint violation <span>$\|c(w)\|_1$</span>, by comparing these two values with those of a list of past iterates.</p><h4 id="Feasility-restoration-phase"><a class="docs-heading-anchor" href="#Feasility-restoration-phase">Feasility restoration phase</a><a id="Feasility-restoration-phase-1"></a><a class="docs-heading-anchor-permalink" href="#Feasility-restoration-phase" title="Permalink"></a></h4><p>If no acceptable step is found by the line search, MadNLP switches to a restoration phase that attempts at projecting the current iterate onto the feasible set. The problem solved by the restoration phase is:</p><p class="math-container">\[\begin{aligned}
\min_{w} \;&amp; \| c(w) \|_1 \\
\text{subject to} \quad &amp; x \geq 0 \; .
\end{aligned}\]</p><p>Once the constraint violation has been significantly reduced, MadNLP returns to the classical algorithm. If the feasibility restoration phase converges to a solution of the feasibility restoration problem with a positive objective, the problem is detected as being locally infeasible.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../quickstart/">« Quickstart</a><a class="docs-footer-nextpage" href="../options/">Options »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Monday 16 February 2026 13:05">Monday 16 February 2026</span>. Using Julia version 1.12.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
